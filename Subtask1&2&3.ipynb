{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import smart_open\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import math\n",
    "from math import log\n",
    "from numpy.random import default_rng, rand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 96\n"
     ]
    }
   ],
   "source": [
    "f = open(\"../dataset/passage_collection_new.txt\", 'r', encoding = 'utf-8') \n",
    "document = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_list = [\"qid\", \"pid\", \"query\", \"passage\"]\n",
    "candidate_passages_top1000 = pd.read_csv(\"../dataset/candidate_passages_top1000.tsv\", sep='\\t', names=header_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>query</th>\n",
       "      <th>passage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494835</td>\n",
       "      <td>7130104</td>\n",
       "      <td>sensibilities, definition</td>\n",
       "      <td>This is the definition of RNA along with examp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1128373</td>\n",
       "      <td>7130104</td>\n",
       "      <td>iur definition</td>\n",
       "      <td>This is the definition of RNA along with examp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131843</td>\n",
       "      <td>7130104</td>\n",
       "      <td>definition of a sigmet</td>\n",
       "      <td>This is the definition of RNA along with examp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20455</td>\n",
       "      <td>7130335</td>\n",
       "      <td>ar glasses definition</td>\n",
       "      <td>Best Answer: The AR designation comes from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>719381</td>\n",
       "      <td>7130335</td>\n",
       "      <td>what is ar balance</td>\n",
       "      <td>Best Answer: The AR designation comes from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189872</th>\n",
       "      <td>1056204</td>\n",
       "      <td>79980</td>\n",
       "      <td>who was the first steam boat operator</td>\n",
       "      <td>Other operators with special formats accept mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189873</th>\n",
       "      <td>1132213</td>\n",
       "      <td>7998257</td>\n",
       "      <td>how long to hold bow in yoga</td>\n",
       "      <td>You may be surprised that to learn that yoga t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189874</th>\n",
       "      <td>324211</td>\n",
       "      <td>7998651</td>\n",
       "      <td>how much money a united airline get as a capta...</td>\n",
       "      <td>Find cheap airline tickets &amp; deals on flights ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189875</th>\n",
       "      <td>1116341</td>\n",
       "      <td>7998709</td>\n",
       "      <td>closed ended mortgage definition</td>\n",
       "      <td>What is a wrap-around mortgage, and who is it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189876</th>\n",
       "      <td>1124145</td>\n",
       "      <td>7998901</td>\n",
       "      <td>truncating meaning</td>\n",
       "      <td>Katie The name Katie is a baby girl name. Mean...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189877 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            qid      pid                                              query  \\\n",
       "0        494835  7130104                          sensibilities, definition   \n",
       "1       1128373  7130104                                     iur definition   \n",
       "2        131843  7130104                             definition of a sigmet   \n",
       "3         20455  7130335                              ar glasses definition   \n",
       "4        719381  7130335                                 what is ar balance   \n",
       "...         ...      ...                                                ...   \n",
       "189872  1056204    79980              who was the first steam boat operator   \n",
       "189873  1132213  7998257                       how long to hold bow in yoga   \n",
       "189874   324211  7998651  how much money a united airline get as a capta...   \n",
       "189875  1116341  7998709                   closed ended mortgage definition   \n",
       "189876  1124145  7998901                                 truncating meaning   \n",
       "\n",
       "                                                  passage  \n",
       "0       This is the definition of RNA along with examp...  \n",
       "1       This is the definition of RNA along with examp...  \n",
       "2       This is the definition of RNA along with examp...  \n",
       "3       Best Answer: The AR designation comes from the...  \n",
       "4       Best Answer: The AR designation comes from the...  \n",
       "...                                                   ...  \n",
       "189872  Other operators with special formats accept mo...  \n",
       "189873  You may be surprised that to learn that yoga t...  \n",
       "189874  Find cheap airline tickets & deals on flights ...  \n",
       "189875  What is a wrap-around mortgage, and who is it ...  \n",
       "189876  Katie The name Katie is a baby girl name. Mean...  \n",
       "\n",
       "[189877 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_passages_top1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_list = [\"qid\", \"query\"]\n",
    "test_queries = pd.read_csv(\"../dataset/test-queries.tsv\", sep='\\t', names=header_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../part2/train_data.tsv\", sep='\\t')\n",
    "validation_data = pd.read_csv(\"../part2/validation_data.tsv\", sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>queries</th>\n",
       "      <th>passage</th>\n",
       "      <th>relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188714</td>\n",
       "      <td>1000052</td>\n",
       "      <td>foods and supplements to lower blood sugar</td>\n",
       "      <td>Watch portion sizes: ■ Even healthy foods will...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>995526</td>\n",
       "      <td>1000094</td>\n",
       "      <td>where is the federal penitentiary in ind</td>\n",
       "      <td>It takes THOUSANDS of Macy's associates to bri...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660957</td>\n",
       "      <td>1000115</td>\n",
       "      <td>what foods are good if you have gout?</td>\n",
       "      <td>The good news is that you will discover what g...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>837202</td>\n",
       "      <td>1000252</td>\n",
       "      <td>what is the nutritional value of oatmeal</td>\n",
       "      <td>Oats make an easy, balanced breakfast. One cup...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130825</td>\n",
       "      <td>1000268</td>\n",
       "      <td>definition for daring</td>\n",
       "      <td>Such a requirement would have three desirable ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>408149</td>\n",
       "      <td>1000288</td>\n",
       "      <td>is dhgate a scam</td>\n",
       "      <td>If you think you ve been targeted by a counter...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1019649</td>\n",
       "      <td>1000419</td>\n",
       "      <td>what study for mets to brain</td>\n",
       "      <td>Sorry he's having so much pain. The reason tha...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1099065</td>\n",
       "      <td>1000436</td>\n",
       "      <td>how far deep to plant beet early wonder</td>\n",
       "      <td>The simplest way, and my preference, is to roa...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1084910</td>\n",
       "      <td>1000466</td>\n",
       "      <td>what disease do roof rats cause</td>\n",
       "      <td>1 A cage trap baited with peanut butter or a s...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>959083</td>\n",
       "      <td>1000479</td>\n",
       "      <td>when was niagara falls created</td>\n",
       "      <td>Bulbar Onset – ALS. ALS is like Niagara Falls,...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid      pid                                     queries  \\\n",
       "0   188714  1000052  foods and supplements to lower blood sugar   \n",
       "1   995526  1000094    where is the federal penitentiary in ind   \n",
       "2   660957  1000115       what foods are good if you have gout?   \n",
       "3   837202  1000252    what is the nutritional value of oatmeal   \n",
       "4   130825  1000268                       definition for daring   \n",
       "5   408149  1000288                            is dhgate a scam   \n",
       "6  1019649  1000419                what study for mets to brain   \n",
       "7  1099065  1000436     how far deep to plant beet early wonder   \n",
       "8  1084910  1000466             what disease do roof rats cause   \n",
       "9   959083  1000479              when was niagara falls created   \n",
       "\n",
       "                                             passage  relevancy  \n",
       "0  Watch portion sizes: ■ Even healthy foods will...        0.0  \n",
       "1  It takes THOUSANDS of Macy's associates to bri...        0.0  \n",
       "2  The good news is that you will discover what g...        0.0  \n",
       "3  Oats make an easy, balanced breakfast. One cup...        0.0  \n",
       "4  Such a requirement would have three desirable ...        0.0  \n",
       "5  If you think you ve been targeted by a counter...        0.0  \n",
       "6  Sorry he's having so much pain. The reason tha...        0.0  \n",
       "7  The simplest way, and my preference, is to roa...        0.0  \n",
       "8  1 A cage trap baited with peanut butter or a s...        0.0  \n",
       "9  Bulbar Onset – ALS. ALS is like Niagara Falls,...        0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4364339, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>queries</th>\n",
       "      <th>passage</th>\n",
       "      <th>relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1082792</td>\n",
       "      <td>1000084</td>\n",
       "      <td>what does the golgi apparatus do to the protei...</td>\n",
       "      <td>Start studying Bonding, Carbs, Proteins, Lipid...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>995825</td>\n",
       "      <td>1000492</td>\n",
       "      <td>where is the graphic card located in the cpu</td>\n",
       "      <td>For example, a “PC Expansion Card” maybe the j...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>995825</td>\n",
       "      <td>1000494</td>\n",
       "      <td>where is the graphic card located in the cpu</td>\n",
       "      <td>The Common Cards &amp; Buses. The most common type...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1091246</td>\n",
       "      <td>1000522</td>\n",
       "      <td>property premises meaning</td>\n",
       "      <td>The occurrence of since tells us that the firs...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1047854</td>\n",
       "      <td>1000585</td>\n",
       "      <td>what is printing mechanism</td>\n",
       "      <td>Windows desktop applications Develop Desktop t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>991832</td>\n",
       "      <td>1000599</td>\n",
       "      <td>who discovered the element carbon</td>\n",
       "      <td>1. 1  a nonmetallic element existing in the th...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>185299</td>\n",
       "      <td>1000647</td>\n",
       "      <td>fastest cell phone processor</td>\n",
       "      <td>Tips for calling a cell phone in Greece: To ca...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>574730</td>\n",
       "      <td>1000663</td>\n",
       "      <td>what are the three monetary policy tools of th...</td>\n",
       "      <td>Federal Reserve updates including rates, news ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1085008</td>\n",
       "      <td>1000675</td>\n",
       "      <td>what did maria theresa do for the serfs</td>\n",
       "      <td>In this feudal system, the king awarded land g...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>609628</td>\n",
       "      <td>1000771</td>\n",
       "      <td>what county is mitchell south dakota in</td>\n",
       "      <td>South Dakota: According to our research of Sou...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid      pid                                            queries  \\\n",
       "0  1082792  1000084  what does the golgi apparatus do to the protei...   \n",
       "1   995825  1000492       where is the graphic card located in the cpu   \n",
       "2   995825  1000494       where is the graphic card located in the cpu   \n",
       "3  1091246  1000522                          property premises meaning   \n",
       "4  1047854  1000585                         what is printing mechanism   \n",
       "5   991832  1000599                  who discovered the element carbon   \n",
       "6   185299  1000647                       fastest cell phone processor   \n",
       "7   574730  1000663  what are the three monetary policy tools of th...   \n",
       "8  1085008  1000675            what did maria theresa do for the serfs   \n",
       "9   609628  1000771            what county is mitchell south dakota in   \n",
       "\n",
       "                                             passage  relevancy  \n",
       "0  Start studying Bonding, Carbs, Proteins, Lipid...        0.0  \n",
       "1  For example, a “PC Expansion Card” maybe the j...        0.0  \n",
       "2  The Common Cards & Buses. The most common type...        0.0  \n",
       "3  The occurrence of since tells us that the firs...        0.0  \n",
       "4  Windows desktop applications Develop Desktop t...        0.0  \n",
       "5  1. 1  a nonmetallic element existing in the th...        0.0  \n",
       "6  Tips for calling a cell phone in Greece: To ca...        0.0  \n",
       "7  Federal Reserve updates including rates, news ...        0.0  \n",
       "8  In this feudal system, the king awarded land g...        0.0  \n",
       "9  South Dakota: According to our research of Sou...        0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4364339, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1103039, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape\n",
    "validation_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(tokens):\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        new_token = re.sub(r'[^\\w\\s]', '', token)\n",
    "        if new_token != '':\n",
    "            new_tokens.append(new_token)\n",
    "    return new_tokens\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    new_tokens = []\n",
    "    stopword_set = set(stopwords.words('english'))\n",
    "    for token in tokens:\n",
    "        if token not in stopword_set:\n",
    "            new_tokens.append(token)\n",
    "    return new_tokens\n",
    "\n",
    "def lemmatize_verbs(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    root_words = []\n",
    "    for token in tokens:\n",
    "        root_word = lemmatizer.lemmatize(token, pos='v')\n",
    "#         root_word = lemmatizer.lemmatize(token, pos='n')\n",
    "#         root_word = lemmatizer.lemmatize(token, pos='a')\n",
    "        root_words.append(root_word)\n",
    "    return root_words\n",
    "\n",
    "def remove_numbers(tokens):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.isdigit():\n",
    "            pass\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "    return new_tokens\n",
    "\n",
    "def preprocessing(passage):\n",
    "    passage = passage.lower()\n",
    "    tokens = nltk.word_tokenize(passage)\n",
    "    tokens = remove_punctuation(tokens)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = lemmatize_verbs(tokens)\n",
    "    tokens = remove_numbers(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtask 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_no_dup_passages = validation_data.drop_duplicates(subset=['pid'], inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_data_no_dup_passages.head(20)\n",
    "validation_data_no_dup_passages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_passage_average_length_and_total_word_occurences_corpus():\n",
    "    number_of_passages = len(validation_data_no_dup_passages)\n",
    "    count_total_length = 0\n",
    "    for idx, row in validation_data_no_dup_passages.iterrows():\n",
    "        count_total_length += len(preprocessing(row['passage']))\n",
    "    return count_total_length, count_total_length/number_of_passages \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_word_occurences, avdl = get_passage_average_length_and_total_word_occurences_corpus()\n",
    "total_word_occurences = 30757932 # for validation data\n",
    "avdl = 32.200144261320276 # for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_word_occurences\n",
    "# avdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "k1 = 1.2\n",
    "k2 = 100\n",
    "b = 0.75\n",
    "R = 0\n",
    "r = 0\n",
    "N = len(validation_data_no_dup_passages)\n",
    "\n",
    "def K_cal(dl):\n",
    "    return k1 * ((1-b) + b * (float(dl)/float(avdl)) )\n",
    "\n",
    "\n",
    "def BM25_cal(query, passage):\n",
    "    query_tokens = preprocessing(query)\n",
    "    passage_tokens = preprocessing(passage)\n",
    "    query_length = len(query_tokens)\n",
    "    query_token_freq_dict = nltk.FreqDist(query_tokens)\n",
    "    passage_token_freq_dict = nltk.FreqDist(passage_tokens)\n",
    "    dl = len(passage_tokens)\n",
    "    K = K_cal(dl)\n",
    "    score = 0\n",
    "    for token in query_tokens:\n",
    "        try:\n",
    "            n = len(inverted_index[token])\n",
    "        except:\n",
    "            n = 0\n",
    "        f = passage_token_freq_dict[token]\n",
    "        qf = query_token_freq_dict[token]\n",
    "        first_term = log( ( (r + 0.5) / (R - r + 0.5) ) / ( (n - r + 0.5) / (N - n - R + r + 0.5)) )\n",
    "        second_term = ((k1 + 1) * f) / (K + f)\n",
    "        third_term = ((k2+1) * qf) / (k2 + qf)\n",
    "        score += first_term * second_term * third_term\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_rankings = []\n",
    "for idx, row in validation_data.iterrows():\n",
    "#     print('count:', idx+1)\n",
    "    query = row['queries']\n",
    "    passage = row['passage']\n",
    "    bm25_rankings.append(BM25_cal(query, passage))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 218534, 1076853,   95986,   95987,  831065,  725920,  504442,\n",
       "        654647,  341262,  281695,  950916,  586050,  871938,  793356,\n",
       "        944762,  714508,  900185,  206973,  629358,  701682,  248618,\n",
       "        259324,  702792,  286327,  205094,   98221,  519566,  242694,\n",
       "        705667,  922032,  197484,  431189,  419681,  237993,  502543,\n",
       "        793493,  646679,  559150,  303774,  638714,  778563,  155707,\n",
       "         45281,  132857,  213755,  299424,  252540,    9338, 1075869,\n",
       "        139233,  733889,  401291,  373812,  784890, 1008311,  911301,\n",
       "        909449,  558879,  857483,  602553,  171827,  217514,  857646,\n",
       "        539660,  195886, 1078616,  554316,  872234,  346989,   28250,\n",
       "       1026344,  330458, 1026153,  612051,  395691,  530538,  313678,\n",
       "        693646,  175037,  751525,  216400,  663797,   99840,  928996,\n",
       "          5558,  428329,  546185,   46827,  112665,  118094,  752555,\n",
       "       1017110,  455444,  910607,  648719,  793681, 1063650,  212436,\n",
       "        415429,  654277], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_k = 100\n",
    "results_bm25 = np.array(bm25_rankings).argsort()[-ranking_k:][::-1]\n",
    "results_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>queries</th>\n",
       "      <th>passage</th>\n",
       "      <th>relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>218534</th>\n",
       "      <td>1007691</td>\n",
       "      <td>7251254</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>Direct method allocates each service departmen...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076853</th>\n",
       "      <td>1007691</td>\n",
       "      <td>7251259</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>The direct method is the most widely-used meth...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95986</th>\n",
       "      <td>1007691</td>\n",
       "      <td>7251251</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>service department provides a large amount of ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95987</th>\n",
       "      <td>1007691</td>\n",
       "      <td>7251253</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>The rows sum to 100%, so that all services pro...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831065</th>\n",
       "      <td>1007691</td>\n",
       "      <td>7251255</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>The most defensible sequence is to start with ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793681</th>\n",
       "      <td>1007691</td>\n",
       "      <td>4814576</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>Service Members | Veterans | Both. Military On...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063650</th>\n",
       "      <td>1007691</td>\n",
       "      <td>6395207</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>that hospital emergency department services ar...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212436</th>\n",
       "      <td>1007691</td>\n",
       "      <td>6872353</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>Yelp Customer Service customer service phone n...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415429</th>\n",
       "      <td>1007691</td>\n",
       "      <td>4114248</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>A service fee, service charge, or surcharge is...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654277</th>\n",
       "      <td>1007691</td>\n",
       "      <td>3743546</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>A Sub-contractor is liable to pay Service Tax ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             qid      pid                                            queries  \\\n",
       "218534   1007691  7251254  when allocating service department costs, the ...   \n",
       "1076853  1007691  7251259  when allocating service department costs, the ...   \n",
       "95986    1007691  7251251  when allocating service department costs, the ...   \n",
       "95987    1007691  7251253  when allocating service department costs, the ...   \n",
       "831065   1007691  7251255  when allocating service department costs, the ...   \n",
       "...          ...      ...                                                ...   \n",
       "793681   1007691  4814576  when allocating service department costs, the ...   \n",
       "1063650  1007691  6395207  when allocating service department costs, the ...   \n",
       "212436   1007691  6872353  when allocating service department costs, the ...   \n",
       "415429   1007691  4114248  when allocating service department costs, the ...   \n",
       "654277   1007691  3743546  when allocating service department costs, the ...   \n",
       "\n",
       "                                                   passage  relevancy  \n",
       "218534   Direct method allocates each service departmen...        1.0  \n",
       "1076853  The direct method is the most widely-used meth...        0.0  \n",
       "95986    service department provides a large amount of ...        0.0  \n",
       "95987    The rows sum to 100%, so that all services pro...        0.0  \n",
       "831065   The most defensible sequence is to start with ...        0.0  \n",
       "...                                                    ...        ...  \n",
       "793681   Service Members | Veterans | Both. Military On...        0.0  \n",
       "1063650  that hospital emergency department services ar...        0.0  \n",
       "212436   Yelp Customer Service customer service phone n...        0.0  \n",
       "415429   A service fee, service charge, or surcharge is...        0.0  \n",
       "654277   A Sub-contractor is liable to pay Service Tax ...        0.0  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_list_df = validation_data.loc[results_bm25]\n",
    "ranking_list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>queries</th>\n",
       "      <th>passage</th>\n",
       "      <th>relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [qid, pid, queries, passage, relevancy]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data[(validation_data['relevancy'] < 1.0) & (validation_data['relevancy'] > 0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision_cal(ranking_list_df):\n",
    "    ranking_list_df = ranking_list_df.reset_index(drop=True, inplace=False)\n",
    "    total_relevant_retrieved = 0\n",
    "    precision_sum = 0\n",
    "    for idx, row in ranking_list_df.iterrows():\n",
    "        relevancy = row['relevancy']\n",
    "        if (relevancy):\n",
    "#             isRelevant = True\n",
    "            total_relevant_retrieved += 1\n",
    "            precision = total_relevant_retrieved / (idx + 1)\n",
    "            precision_sum += precision\n",
    "    result = precision_sum / len(ranking_list_df)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07445786781310981"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision_cal(ranking_list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>queries</th>\n",
       "      <th>passage</th>\n",
       "      <th>relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>218534</th>\n",
       "      <td>1007691</td>\n",
       "      <td>7251254</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>Direct method allocates each service departmen...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950916</th>\n",
       "      <td>1089945</td>\n",
       "      <td>7079883</td>\n",
       "      <td>the __________ test is a quick and dirty test ...</td>\n",
       "      <td>• The Smell Test is familiar ground in most bu...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539660</th>\n",
       "      <td>1007691</td>\n",
       "      <td>423230</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>IT Service (ITILv3): A Service provided to one...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612051</th>\n",
       "      <td>1007691</td>\n",
       "      <td>994382</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>All UK telephone numbers beginning with the di...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026153</th>\n",
       "      <td>1007691</td>\n",
       "      <td>3941750</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>Respite (Out-of-Home) Services [edit]. Respite...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431189</th>\n",
       "      <td>1007691</td>\n",
       "      <td>5146501</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>The Department offers service coordination and...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197484</th>\n",
       "      <td>1007691</td>\n",
       "      <td>5904987</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>If you are a delinquent juror who has been ins...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922032</th>\n",
       "      <td>1007691</td>\n",
       "      <td>5220119</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>There are different types of customer service ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705667</th>\n",
       "      <td>1007691</td>\n",
       "      <td>7088169</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>Us Postal Service Customer Service Phone Numbe...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654277</th>\n",
       "      <td>1007691</td>\n",
       "      <td>3743546</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>A Sub-contractor is liable to pay Service Tax ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             qid      pid                                            queries  \\\n",
       "218534   1007691  7251254  when allocating service department costs, the ...   \n",
       "950916   1089945  7079883  the __________ test is a quick and dirty test ...   \n",
       "539660   1007691   423230  when allocating service department costs, the ...   \n",
       "612051   1007691   994382  when allocating service department costs, the ...   \n",
       "1026153  1007691  3941750  when allocating service department costs, the ...   \n",
       "...          ...      ...                                                ...   \n",
       "431189   1007691  5146501  when allocating service department costs, the ...   \n",
       "197484   1007691  5904987  when allocating service department costs, the ...   \n",
       "922032   1007691  5220119  when allocating service department costs, the ...   \n",
       "705667   1007691  7088169  when allocating service department costs, the ...   \n",
       "654277   1007691  3743546  when allocating service department costs, the ...   \n",
       "\n",
       "                                                   passage  relevancy  \n",
       "218534   Direct method allocates each service departmen...        1.0  \n",
       "950916   • The Smell Test is familiar ground in most bu...        1.0  \n",
       "539660   IT Service (ITILv3): A Service provided to one...        0.0  \n",
       "612051   All UK telephone numbers beginning with the di...        0.0  \n",
       "1026153  Respite (Out-of-Home) Services [edit]. Respite...        0.0  \n",
       "...                                                    ...        ...  \n",
       "431189   The Department offers service coordination and...        0.0  \n",
       "197484   If you are a delinquent juror who has been ins...        0.0  \n",
       "922032   There are different types of customer service ...        0.0  \n",
       "705667   Us Postal Service Customer Service Phone Numbe...        0.0  \n",
       "654277   A Sub-contractor is liable to pay Service Tax ...        0.0  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_list_df.sort_values(by=['relevancy'], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>queries</th>\n",
       "      <th>passage</th>\n",
       "      <th>relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>218534</th>\n",
       "      <td>1007691</td>\n",
       "      <td>7251254</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>Direct method allocates each service departmen...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076853</th>\n",
       "      <td>1007691</td>\n",
       "      <td>7251259</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>The direct method is the most widely-used meth...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95986</th>\n",
       "      <td>1007691</td>\n",
       "      <td>7251251</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>service department provides a large amount of ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95987</th>\n",
       "      <td>1007691</td>\n",
       "      <td>7251253</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>The rows sum to 100%, so that all services pro...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831065</th>\n",
       "      <td>1007691</td>\n",
       "      <td>7251255</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>The most defensible sequence is to start with ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793681</th>\n",
       "      <td>1007691</td>\n",
       "      <td>4814576</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>Service Members | Veterans | Both. Military On...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063650</th>\n",
       "      <td>1007691</td>\n",
       "      <td>6395207</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>that hospital emergency department services ar...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212436</th>\n",
       "      <td>1007691</td>\n",
       "      <td>6872353</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>Yelp Customer Service customer service phone n...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415429</th>\n",
       "      <td>1007691</td>\n",
       "      <td>4114248</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>A service fee, service charge, or surcharge is...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654277</th>\n",
       "      <td>1007691</td>\n",
       "      <td>3743546</td>\n",
       "      <td>when allocating service department costs, the ...</td>\n",
       "      <td>A Sub-contractor is liable to pay Service Tax ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             qid      pid                                            queries  \\\n",
       "218534   1007691  7251254  when allocating service department costs, the ...   \n",
       "1076853  1007691  7251259  when allocating service department costs, the ...   \n",
       "95986    1007691  7251251  when allocating service department costs, the ...   \n",
       "95987    1007691  7251253  when allocating service department costs, the ...   \n",
       "831065   1007691  7251255  when allocating service department costs, the ...   \n",
       "...          ...      ...                                                ...   \n",
       "793681   1007691  4814576  when allocating service department costs, the ...   \n",
       "1063650  1007691  6395207  when allocating service department costs, the ...   \n",
       "212436   1007691  6872353  when allocating service department costs, the ...   \n",
       "415429   1007691  4114248  when allocating service department costs, the ...   \n",
       "654277   1007691  3743546  when allocating service department costs, the ...   \n",
       "\n",
       "                                                   passage  relevancy  \n",
       "218534   Direct method allocates each service departmen...        1.0  \n",
       "1076853  The direct method is the most widely-used meth...        0.0  \n",
       "95986    service department provides a large amount of ...        0.0  \n",
       "95987    The rows sum to 100%, so that all services pro...        0.0  \n",
       "831065   The most defensible sequence is to start with ...        0.0  \n",
       "...                                                    ...        ...  \n",
       "793681   Service Members | Veterans | Both. Military On...        0.0  \n",
       "1063650  that hospital emergency department services ar...        0.0  \n",
       "212436   Yelp Customer Service customer service phone n...        0.0  \n",
       "415429   A service fee, service charge, or surcharge is...        0.0  \n",
       "654277   A Sub-contractor is liable to pay Service Tax ...        0.0  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IDCG(ranking_list_df):\n",
    "    ranking_list_df_sorted = ranking_list_df.sort_values(by=['relevancy'], ascending=False)\n",
    "#     ranking_list_df_sorted = ranking_list_df.reset_index(drop=True, inplace=False)\n",
    "    ranking_list_df_sorted = ranking_list_df_sorted.reset_index().reindex(ranking_list_df_sorted.columns, axis=1)\n",
    "    ideal_discounted_gain_sum = 0\n",
    "    for idx, row in ranking_list_df_sorted.iterrows():\n",
    "        index = idx + 1\n",
    "        relevance_score = row['relevancy']\n",
    "        gain = 2 ** relevance_score - 1\n",
    "        discounted_gain = gain / math.log2(index + 1)\n",
    "        ideal_discounted_gain_sum += discounted_gain\n",
    "    return ideal_discounted_gain_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NDCG(ranking_list_df):\n",
    "    ranking_list_df = ranking_list_df.reset_index(drop=True, inplace=False)\n",
    "    discounted_gain_sum = 0\n",
    "    for idx, row in ranking_list_df.iterrows():\n",
    "        index = idx + 1\n",
    "        relevance_score = row['relevancy']\n",
    "        gain = 2 ** relevance_score - 1\n",
    "        discounted_gain = gain / math.log2(index + 1)\n",
    "        discounted_gain_sum += discounted_gain\n",
    "    IDCG = get_IDCG(ranking_list_df)\n",
    "    \n",
    "    nDCG = discounted_gain_sum / IDCG\n",
    "    return nDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7841802768331765"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_NDCG(ranking_list_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtask 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv(\"../part2/train_data.tsv\", sep='\\t')\n",
    "# validation_data = pd.read_csv(\"../part2/validation_data.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[:1000] # 일단 1000개만 해봄\n",
    "validation_data = validation_data[:1000] # 일단 1000개만 해봄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, weights=[0.99], flip_y=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 20)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample = RandomUnderSampler(sampling_strategy='majority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and apply the transform\n",
    "X_over, y_over = undersample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data.relevancy.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 4359542, 1.0: 4797})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_together(n, X, y):\n",
    "    rows = random.sample(np.arange(0,len(X.index)).tolist(),n)\n",
    "    return X.iloc[rows,], y.iloc[rows,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample(X, y, under = 1):\n",
    "    y_min = y[y.relevancy == under]\n",
    "    y_max = y[y.relevancy != under]\n",
    "    X_min = X.filter(y_min.index,axis = 0)\n",
    "    X_max = X.filter(y_max.index,axis = 0)\n",
    "\n",
    "    X_under, y_under = sample_together(len(y_min.index), X_max, y_max)\n",
    "    \n",
    "    X = pd.concat([X_under, X_min])\n",
    "    y = pd.concat([y_under, y_min])\n",
    "    return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4364334</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4364335</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4364336</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4364337</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4364338</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4364339 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         relevancy\n",
       "0              0.0\n",
       "1              0.0\n",
       "2              0.0\n",
       "3              0.0\n",
       "4              0.0\n",
       "...            ...\n",
       "4364334        0.0\n",
       "4364335        0.0\n",
       "4364336        0.0\n",
       "4364337        0.0\n",
       "4364338        0.0\n",
       "\n",
       "[4364339 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[['relevancy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = undersample(train_data, train_data[['relevancy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9594, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = X_train\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = undersample(validation_data, validation_data[['relevancy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2416, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data = X_val\n",
    "validation_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passages = validation_data.passage.values[:1000]\n",
    "# validation_data_temp = validation_data[:1000]\n",
    "# validation_data_temp['passage_cleaned']=validation_data_temp.passage.apply(lambda x: preprocessing(x))\n",
    "# validation_data_temp['queries_cleaned']=validation_data_temp.queries.apply(lambda x: preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['passage_cleaned']=train_data.passage.apply(lambda x: preprocessing(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['query_cleaned']=train_data.queries.apply(lambda x: preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>queries</th>\n",
       "      <th>passage</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>passage_cleaned</th>\n",
       "      <th>query_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2972137</th>\n",
       "      <td>820899</td>\n",
       "      <td>2043455</td>\n",
       "      <td>what is the fastest a plane can go</td>\n",
       "      <td>Anatomical terms for describing planes: Median...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[anatomical, term, describe, plan, median, mid...</td>\n",
       "      <td>[fastest, plane, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115050</th>\n",
       "      <td>956403</td>\n",
       "      <td>2855689</td>\n",
       "      <td>when ventricles in brain are widely spaced and...</td>\n",
       "      <td>Purpose. Cerebrospinal fluid is a clear liquid...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[purpose, cerebrospinal, fluid, clear, liquid,...</td>\n",
       "      <td>[ventricles, brain, widely, space, csp]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4076058</th>\n",
       "      <td>1093507</td>\n",
       "      <td>420771</td>\n",
       "      <td>7) how does human rights law differ from the l...</td>\n",
       "      <td>Moral rights are a set of rights that are sepa...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[moral, right, set, right, separate, author, c...</td>\n",
       "      <td>[human, right, law, differ, law, war]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993259</th>\n",
       "      <td>1099050</td>\n",
       "      <td>2842031</td>\n",
       "      <td>how far in advance to send invitations birthda...</td>\n",
       "      <td>11/10/2016. Flipz is a gymnastics academy loca...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[flipz, gymnastics, academy, locate, columbia,...</td>\n",
       "      <td>[far, advance, send, invitations, birthday, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3026315</th>\n",
       "      <td>46711</td>\n",
       "      <td>2934006</td>\n",
       "      <td>average wage gilded age</td>\n",
       "      <td>1 The average starting wage for welfare recipi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[average, start, wage, welfare, recipients, ri...</td>\n",
       "      <td>[average, wage, gild, age]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4343822</th>\n",
       "      <td>401287</td>\n",
       "      <td>860900</td>\n",
       "      <td>is a written prescription required for hydroco...</td>\n",
       "      <td>Refills of hydrocodone combination products wi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[refill, hydrocodone, combination, products, r...</td>\n",
       "      <td>[write, prescription, require, hydrocodone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4352626</th>\n",
       "      <td>541272</td>\n",
       "      <td>876066</td>\n",
       "      <td>was wilson a good president</td>\n",
       "      <td>Woodrow Wilson (1856-1924), the 28th U.S. pres...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[woodrow, wilson, 28th, us, president, serve, ...</td>\n",
       "      <td>[wilson, good, president]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4356455</th>\n",
       "      <td>845529</td>\n",
       "      <td>882642</td>\n",
       "      <td>what is the salary range of a dentist</td>\n",
       "      <td>Dentist Salary. (United States). The average s...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[dentist, salary, unite, state, average, salar...</td>\n",
       "      <td>[salary, range, dentist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4360007</th>\n",
       "      <td>850361</td>\n",
       "      <td>926854</td>\n",
       "      <td>what is the temperature in washington</td>\n",
       "      <td>July is the hottest month in Washington DC wit...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[july, hottest, month, washington, dc, average...</td>\n",
       "      <td>[temperature, washington]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4361710</th>\n",
       "      <td>969974</td>\n",
       "      <td>956426</td>\n",
       "      <td>where did the the trail of tears end</td>\n",
       "      <td>The Trail of Tears. The Indian-removal process...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[trail, tear, indianremoval, process, continue...</td>\n",
       "      <td>[trail, tear, end]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9594 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             qid      pid                                            queries  \\\n",
       "2972137   820899  2043455                 what is the fastest a plane can go   \n",
       "115050    956403  2855689  when ventricles in brain are widely spaced and...   \n",
       "4076058  1093507   420771  7) how does human rights law differ from the l...   \n",
       "3993259  1099050  2842031  how far in advance to send invitations birthda...   \n",
       "3026315    46711  2934006                            average wage gilded age   \n",
       "...          ...      ...                                                ...   \n",
       "4343822   401287   860900  is a written prescription required for hydroco...   \n",
       "4352626   541272   876066                        was wilson a good president   \n",
       "4356455   845529   882642              what is the salary range of a dentist   \n",
       "4360007   850361   926854              what is the temperature in washington   \n",
       "4361710   969974   956426               where did the the trail of tears end   \n",
       "\n",
       "                                                   passage  relevancy  \\\n",
       "2972137  Anatomical terms for describing planes: Median...        0.0   \n",
       "115050   Purpose. Cerebrospinal fluid is a clear liquid...        0.0   \n",
       "4076058  Moral rights are a set of rights that are sepa...        0.0   \n",
       "3993259  11/10/2016. Flipz is a gymnastics academy loca...        0.0   \n",
       "3026315  1 The average starting wage for welfare recipi...        0.0   \n",
       "...                                                    ...        ...   \n",
       "4343822  Refills of hydrocodone combination products wi...        1.0   \n",
       "4352626  Woodrow Wilson (1856-1924), the 28th U.S. pres...        1.0   \n",
       "4356455  Dentist Salary. (United States). The average s...        1.0   \n",
       "4360007  July is the hottest month in Washington DC wit...        1.0   \n",
       "4361710  The Trail of Tears. The Indian-removal process...        1.0   \n",
       "\n",
       "                                           passage_cleaned  \\\n",
       "2972137  [anatomical, term, describe, plan, median, mid...   \n",
       "115050   [purpose, cerebrospinal, fluid, clear, liquid,...   \n",
       "4076058  [moral, right, set, right, separate, author, c...   \n",
       "3993259  [flipz, gymnastics, academy, locate, columbia,...   \n",
       "3026315  [average, start, wage, welfare, recipients, ri...   \n",
       "...                                                    ...   \n",
       "4343822  [refill, hydrocodone, combination, products, r...   \n",
       "4352626  [woodrow, wilson, 28th, us, president, serve, ...   \n",
       "4356455  [dentist, salary, unite, state, average, salar...   \n",
       "4360007  [july, hottest, month, washington, dc, average...   \n",
       "4361710  [trail, tear, indianremoval, process, continue...   \n",
       "\n",
       "                                             query_cleaned  \n",
       "2972137                               [fastest, plane, go]  \n",
       "115050             [ventricles, brain, widely, space, csp]  \n",
       "4076058              [human, right, law, differ, law, war]  \n",
       "3993259  [far, advance, send, invitations, birthday, pa...  \n",
       "3026315                         [average, wage, gild, age]  \n",
       "...                                                    ...  \n",
       "4343822        [write, prescription, require, hydrocodone]  \n",
       "4352626                          [wilson, good, president]  \n",
       "4356455                           [salary, range, dentist]  \n",
       "4360007                          [temperature, washington]  \n",
       "4361710                                 [trail, tear, end]  \n",
       "\n",
       "[9594 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data['passage_cleaned']=validation_data.passage.apply(lambda x: preprocessing(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data['query_cleaned']=validation_data.queries.apply(lambda x: preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>queries</th>\n",
       "      <th>passage</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>passage_cleaned</th>\n",
       "      <th>query_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2972137</th>\n",
       "      <td>820899</td>\n",
       "      <td>2043455</td>\n",
       "      <td>what is the fastest a plane can go</td>\n",
       "      <td>Anatomical terms for describing planes: Median...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[anatomical, term, describe, plan, median, mid...</td>\n",
       "      <td>[fastest, plane, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115050</th>\n",
       "      <td>956403</td>\n",
       "      <td>2855689</td>\n",
       "      <td>when ventricles in brain are widely spaced and...</td>\n",
       "      <td>Purpose. Cerebrospinal fluid is a clear liquid...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[purpose, cerebrospinal, fluid, clear, liquid,...</td>\n",
       "      <td>[ventricles, brain, widely, space, csp]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4076058</th>\n",
       "      <td>1093507</td>\n",
       "      <td>420771</td>\n",
       "      <td>7) how does human rights law differ from the l...</td>\n",
       "      <td>Moral rights are a set of rights that are sepa...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[moral, right, set, right, separate, author, c...</td>\n",
       "      <td>[human, right, law, differ, law, war]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993259</th>\n",
       "      <td>1099050</td>\n",
       "      <td>2842031</td>\n",
       "      <td>how far in advance to send invitations birthda...</td>\n",
       "      <td>11/10/2016. Flipz is a gymnastics academy loca...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[flipz, gymnastics, academy, locate, columbia,...</td>\n",
       "      <td>[far, advance, send, invitations, birthday, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3026315</th>\n",
       "      <td>46711</td>\n",
       "      <td>2934006</td>\n",
       "      <td>average wage gilded age</td>\n",
       "      <td>1 The average starting wage for welfare recipi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[average, start, wage, welfare, recipients, ri...</td>\n",
       "      <td>[average, wage, gild, age]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4343822</th>\n",
       "      <td>401287</td>\n",
       "      <td>860900</td>\n",
       "      <td>is a written prescription required for hydroco...</td>\n",
       "      <td>Refills of hydrocodone combination products wi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[refill, hydrocodone, combination, products, r...</td>\n",
       "      <td>[write, prescription, require, hydrocodone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4352626</th>\n",
       "      <td>541272</td>\n",
       "      <td>876066</td>\n",
       "      <td>was wilson a good president</td>\n",
       "      <td>Woodrow Wilson (1856-1924), the 28th U.S. pres...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[woodrow, wilson, 28th, us, president, serve, ...</td>\n",
       "      <td>[wilson, good, president]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4356455</th>\n",
       "      <td>845529</td>\n",
       "      <td>882642</td>\n",
       "      <td>what is the salary range of a dentist</td>\n",
       "      <td>Dentist Salary. (United States). The average s...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[dentist, salary, unite, state, average, salar...</td>\n",
       "      <td>[salary, range, dentist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4360007</th>\n",
       "      <td>850361</td>\n",
       "      <td>926854</td>\n",
       "      <td>what is the temperature in washington</td>\n",
       "      <td>July is the hottest month in Washington DC wit...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[july, hottest, month, washington, dc, average...</td>\n",
       "      <td>[temperature, washington]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4361710</th>\n",
       "      <td>969974</td>\n",
       "      <td>956426</td>\n",
       "      <td>where did the the trail of tears end</td>\n",
       "      <td>The Trail of Tears. The Indian-removal process...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[trail, tear, indianremoval, process, continue...</td>\n",
       "      <td>[trail, tear, end]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9594 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             qid      pid                                            queries  \\\n",
       "2972137   820899  2043455                 what is the fastest a plane can go   \n",
       "115050    956403  2855689  when ventricles in brain are widely spaced and...   \n",
       "4076058  1093507   420771  7) how does human rights law differ from the l...   \n",
       "3993259  1099050  2842031  how far in advance to send invitations birthda...   \n",
       "3026315    46711  2934006                            average wage gilded age   \n",
       "...          ...      ...                                                ...   \n",
       "4343822   401287   860900  is a written prescription required for hydroco...   \n",
       "4352626   541272   876066                        was wilson a good president   \n",
       "4356455   845529   882642              what is the salary range of a dentist   \n",
       "4360007   850361   926854              what is the temperature in washington   \n",
       "4361710   969974   956426               where did the the trail of tears end   \n",
       "\n",
       "                                                   passage  relevancy  \\\n",
       "2972137  Anatomical terms for describing planes: Median...        0.0   \n",
       "115050   Purpose. Cerebrospinal fluid is a clear liquid...        0.0   \n",
       "4076058  Moral rights are a set of rights that are sepa...        0.0   \n",
       "3993259  11/10/2016. Flipz is a gymnastics academy loca...        0.0   \n",
       "3026315  1 The average starting wage for welfare recipi...        0.0   \n",
       "...                                                    ...        ...   \n",
       "4343822  Refills of hydrocodone combination products wi...        1.0   \n",
       "4352626  Woodrow Wilson (1856-1924), the 28th U.S. pres...        1.0   \n",
       "4356455  Dentist Salary. (United States). The average s...        1.0   \n",
       "4360007  July is the hottest month in Washington DC wit...        1.0   \n",
       "4361710  The Trail of Tears. The Indian-removal process...        1.0   \n",
       "\n",
       "                                           passage_cleaned  \\\n",
       "2972137  [anatomical, term, describe, plan, median, mid...   \n",
       "115050   [purpose, cerebrospinal, fluid, clear, liquid,...   \n",
       "4076058  [moral, right, set, right, separate, author, c...   \n",
       "3993259  [flipz, gymnastics, academy, locate, columbia,...   \n",
       "3026315  [average, start, wage, welfare, recipients, ri...   \n",
       "...                                                    ...   \n",
       "4343822  [refill, hydrocodone, combination, products, r...   \n",
       "4352626  [woodrow, wilson, 28th, us, president, serve, ...   \n",
       "4356455  [dentist, salary, unite, state, average, salar...   \n",
       "4360007  [july, hottest, month, washington, dc, average...   \n",
       "4361710  [trail, tear, indianremoval, process, continue...   \n",
       "\n",
       "                                             query_cleaned  \n",
       "2972137                               [fastest, plane, go]  \n",
       "115050             [ventricles, brain, widely, space, csp]  \n",
       "4076058              [human, right, law, differ, law, war]  \n",
       "3993259  [far, advance, send, invitations, birthday, pa...  \n",
       "3026315                         [average, wage, gild, age]  \n",
       "...                                                    ...  \n",
       "4343822        [write, prescription, require, hydrocodone]  \n",
       "4352626                          [wilson, good, president]  \n",
       "4356455                           [salary, range, dentist]  \n",
       "4360007                          [temperature, washington]  \n",
       "4361710                                 [trail, tear, end]  \n",
       "\n",
       "[9594 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.sample(frac = 1).reset_index(drop=True, inplace=False)\n",
    "validation_data = validation_data.sample(frac = 1).reset_index(drop=True, inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>queries</th>\n",
       "      <th>passage</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>passage_cleaned</th>\n",
       "      <th>query_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1085535</td>\n",
       "      <td>7118287</td>\n",
       "      <td>cardiovascular meaning</td>\n",
       "      <td>Dictionary entry overview: What does cardiovas...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[dictionary, entry, overview, cardiovascular, ...</td>\n",
       "      <td>[cardiovascular, mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>690606</td>\n",
       "      <td>8245686</td>\n",
       "      <td>what is a melamine sponge</td>\n",
       "      <td>Gather together what you'll paint, tape if you...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[gather, together, paint, tape, want, clean, e...</td>\n",
       "      <td>[melamine, sponge]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002889</td>\n",
       "      <td>4722064</td>\n",
       "      <td>when was john t scopes trial</td>\n",
       "      <td>Before the time you must appear in court, you ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[time, must, appear, court, may, mail, bring, ...</td>\n",
       "      <td>[john, scopes, trial]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002426</td>\n",
       "      <td>4035472</td>\n",
       "      <td>when was the shazam movie released?sadasdasdas...</td>\n",
       "      <td>Jem and the Holograms will be released in Octo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[jem, holograms, release, october, hasbro, pro...</td>\n",
       "      <td>[shazam, movie, release, sadasdasdasdsadasasda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>517386</td>\n",
       "      <td>5209492</td>\n",
       "      <td>the medical term meaning the separation of the...</td>\n",
       "      <td>The focus light rays are then directed to the ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[focus, light, ray, direct, back, eye, retina,...</td>\n",
       "      <td>[medical, term, mean, separation, retina, chor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9589</th>\n",
       "      <td>1095085</td>\n",
       "      <td>7617014</td>\n",
       "      <td>how productive is corn per acre</td>\n",
       "      <td>How big is an acre? Officially, one acre is 4,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[big, acre, officially, one, acre, square, yar...</td>\n",
       "      <td>[productive, corn, per, acre]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9590</th>\n",
       "      <td>1095278</td>\n",
       "      <td>7964885</td>\n",
       "      <td>average temp in fort worth in march</td>\n",
       "      <td>The weather is screwy this year. West Texas ha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[weather, screwy, year, west, texas, lot, rain...</td>\n",
       "      <td>[average, temp, fort, worth, march]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9591</th>\n",
       "      <td>385652</td>\n",
       "      <td>7462070</td>\n",
       "      <td>how to use stall bars</td>\n",
       "      <td>That is the pro level – no doubts about that. ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[pro, level, doubt, flag, wholebody, exercise,...</td>\n",
       "      <td>[use, stall, bar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9592</th>\n",
       "      <td>1051214</td>\n",
       "      <td>1590950</td>\n",
       "      <td>what is meningitis.</td>\n",
       "      <td>Meningitis is an inflammation of the membranes...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[meningitis, inflammation, membranes, meninges...</td>\n",
       "      <td>[meningitis]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9593</th>\n",
       "      <td>632394</td>\n",
       "      <td>2485870</td>\n",
       "      <td>what does argan oil do for skin</td>\n",
       "      <td>I use apricot kernel oil and argan oil. Both h...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[use, apricot, kernel, oil, argan, oil, exactl...</td>\n",
       "      <td>[argan, oil, skin]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9594 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          qid      pid                                            queries  \\\n",
       "0     1085535  7118287                             cardiovascular meaning   \n",
       "1      690606  8245686                          what is a melamine sponge   \n",
       "2     1002889  4722064                       when was john t scopes trial   \n",
       "3     1002426  4035472  when was the shazam movie released?sadasdasdas...   \n",
       "4      517386  5209492  the medical term meaning the separation of the...   \n",
       "...       ...      ...                                                ...   \n",
       "9589  1095085  7617014                    how productive is corn per acre   \n",
       "9590  1095278  7964885                average temp in fort worth in march   \n",
       "9591   385652  7462070                              how to use stall bars   \n",
       "9592  1051214  1590950                                what is meningitis.   \n",
       "9593   632394  2485870                    what does argan oil do for skin   \n",
       "\n",
       "                                                passage  relevancy  \\\n",
       "0     Dictionary entry overview: What does cardiovas...        1.0   \n",
       "1     Gather together what you'll paint, tape if you...        0.0   \n",
       "2     Before the time you must appear in court, you ...        0.0   \n",
       "3     Jem and the Holograms will be released in Octo...        0.0   \n",
       "4     The focus light rays are then directed to the ...        0.0   \n",
       "...                                                 ...        ...   \n",
       "9589  How big is an acre? Officially, one acre is 4,...        1.0   \n",
       "9590  The weather is screwy this year. West Texas ha...        0.0   \n",
       "9591  That is the pro level – no doubts about that. ...        1.0   \n",
       "9592  Meningitis is an inflammation of the membranes...        1.0   \n",
       "9593  I use apricot kernel oil and argan oil. Both h...        0.0   \n",
       "\n",
       "                                        passage_cleaned  \\\n",
       "0     [dictionary, entry, overview, cardiovascular, ...   \n",
       "1     [gather, together, paint, tape, want, clean, e...   \n",
       "2     [time, must, appear, court, may, mail, bring, ...   \n",
       "3     [jem, holograms, release, october, hasbro, pro...   \n",
       "4     [focus, light, ray, direct, back, eye, retina,...   \n",
       "...                                                 ...   \n",
       "9589  [big, acre, officially, one, acre, square, yar...   \n",
       "9590  [weather, screwy, year, west, texas, lot, rain...   \n",
       "9591  [pro, level, doubt, flag, wholebody, exercise,...   \n",
       "9592  [meningitis, inflammation, membranes, meninges...   \n",
       "9593  [use, apricot, kernel, oil, argan, oil, exactl...   \n",
       "\n",
       "                                          query_cleaned  \n",
       "0                                [cardiovascular, mean]  \n",
       "1                                    [melamine, sponge]  \n",
       "2                                 [john, scopes, trial]  \n",
       "3     [shazam, movie, release, sadasdasdasdsadasasda...  \n",
       "4     [medical, term, mean, separation, retina, chor...  \n",
       "...                                                 ...  \n",
       "9589                      [productive, corn, per, acre]  \n",
       "9590                [average, temp, fort, worth, march]  \n",
       "9591                                  [use, stall, bar]  \n",
       "9592                                       [meningitis]  \n",
       "9593                                 [argan, oil, skin]  \n",
       "\n",
       "[9594 rows x 7 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading word2vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kang\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# loading pre-trained embeddings, each word is represented as a 300 dimensional vector\n",
    "import gensim\n",
    "W2V_PATH=\"../GoogleNews-vectors-negative300.bin\"\n",
    "model_w2v = gensim.models.KeyedVectors.load_word2vec_format(W2V_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding documents and queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and pad every document to make them of the same size\n",
    "passage_tokenizer=Tokenizer()\n",
    "passage_tokenizer.fit_on_texts(train_data.passage_cleaned)\n",
    "passage_max_length = 128 # document length including padding\n",
    "query_max_length = 64 # query length including padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9594, 128, 300)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_length = train_data.shape[0]\n",
    "document_word_embeddings_train=np.zeros((train_data_length, passage_max_length,300)) # 64 == padding\n",
    "passages = train_data.passage_cleaned\n",
    "for i in range(len(passages)):\n",
    "    passage = passages[i]\n",
    "    passage_length = len(passage)\n",
    "    for j in range(passage_length): \n",
    "        word = passage[j]\n",
    "        if word in model_w2v:\n",
    "            document_word_embeddings_train[i][j] = model_w2v[word]\n",
    "document_word_embeddings_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9594, 300)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_vector_list_length = document_word_embeddings_train.shape[0]\n",
    "average_document_vectors_train = np.zeros((average_vector_list_length,300))\n",
    "for i in range(average_vector_list_length):\n",
    "    average_document_vectors_train[i] = np.mean(document_word_embeddings_train[i], axis=0)\n",
    "average_document_vectors_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and pad every document to make them of the same size\n",
    "passage_tokenizer=Tokenizer()\n",
    "passage_tokenizer.fit_on_texts(validation_data.passage_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2416, 128, 300)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data_length = validation_data.shape[0]\n",
    "document_word_embeddings_val=np.zeros((validation_data_length, passage_max_length,300)) # 64 == padding\n",
    "passages = validation_data.passage_cleaned\n",
    "for i in range(len(passages)):\n",
    "    passage = passages[i]\n",
    "    passage_length = len(passage)\n",
    "    for j in range(passage_length): \n",
    "        word = passage[j]\n",
    "        if word in model_w2v:\n",
    "            document_word_embeddings_val[i][j] = model_w2v[word]\n",
    "document_word_embeddings_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2416, 300)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_vector_list_length = document_word_embeddings_val.shape[0]\n",
    "average_document_vectors_val = np.zeros((average_vector_list_length,300))\n",
    "for i in range(average_vector_list_length):\n",
    "    average_document_vectors_val[i] = np.mean(document_word_embeddings_val[i], axis=0)\n",
    "average_document_vectors_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding quries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제점:\n",
    "1. 특정 단어가 word embedding에 없는 경우가 있음\n",
    "-> query_vector 의 모든 entry가 0임\n",
    "\n",
    "-> cosine similarity 계산할 때 nan 이 나옴\n",
    "\n",
    "temp solution:\n",
    "cosine similarity 가 0일떄 결과값에 0 을 넣어줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and pad every document to make them of the same size\n",
    "query_tokenizer=Tokenizer()\n",
    "query_tokenizer.fit_on_texts(train_data.query_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9594, 64, 300)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_length = train_data.shape[0]\n",
    "query_word_embeddings_train=np.zeros((train_data_length, query_max_length,300)) # 64 == padding\n",
    "queries = train_data.query_cleaned\n",
    "for i in range(len(queries)):\n",
    "    query = queries[i]\n",
    "    query_length = len(query)\n",
    "    for j in range(query_length): \n",
    "        word = query[j]\n",
    "        if word in model_w2v:\n",
    "            query_word_embeddings_train[i][j] = model_w2v[word]\n",
    "query_word_embeddings_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9594, 300)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_vector_list_length = query_word_embeddings_train.shape[0]\n",
    "average_query_vectors_train = np.zeros((average_vector_list_length,300))\n",
    "for i in range(average_vector_list_length):\n",
    "    average_query_vectors_train[i] = np.mean(query_word_embeddings_train[i], axis=0)\n",
    "average_query_vectors_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and pad every document to make them of the same size\n",
    "query_tokenizer=Tokenizer()\n",
    "query_tokenizer.fit_on_texts(validation_data.query_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2416, 64, 300)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data_length = validation_data.shape[0]\n",
    "query_word_embeddings_val=np.zeros((validation_data_length, query_max_length,300)) # 64 == padding\n",
    "queries = validation_data.query_cleaned\n",
    "for i in range(len(queries)):\n",
    "    query = queries[i]\n",
    "    query_length = len(query)\n",
    "    for j in range(query_length): \n",
    "        word = query[j]\n",
    "        if word in model_w2v:\n",
    "            query_word_embeddings_val[i][j] = model_w2v[word]\n",
    "query_word_embeddings_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2416, 300)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_vector_list_length = query_word_embeddings_val.shape[0]\n",
    "average_query_vectors_val = np.zeros((average_vector_list_length,300))\n",
    "for i in range(average_vector_list_length):\n",
    "    average_query_vectors_val[i] = np.mean(query_word_embeddings_val[i], axis=0)\n",
    "average_query_vectors_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating training data(adding features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_features = 2 # cosine_sim, bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9594, 2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.zeros((average_query_vectors_train.shape[0], num_of_features))\n",
    "x_val = np.zeros((average_query_vectors_val.shape[0], num_of_features))\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Simliarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim_formula(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    if np.isnan(cos_sim):\n",
    "        cos_sim = 0\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kang\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(x_train.shape[0]):\n",
    "    query_vector = average_query_vectors_train[i]\n",
    "    passage_vector = average_document_vectors_train[i]\n",
    "    x_train[i][0] = cosine_sim_formula(query_vector, passage_vector)\n",
    "    if np.isnan(x_train[i][0]):\n",
    "        x_train[i][0] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['co_similarity'] = x_train[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>queries</th>\n",
       "      <th>passage</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>passage_cleaned</th>\n",
       "      <th>query_cleaned</th>\n",
       "      <th>co_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1085535</td>\n",
       "      <td>7118287</td>\n",
       "      <td>cardiovascular meaning</td>\n",
       "      <td>Dictionary entry overview: What does cardiovas...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[dictionary, entry, overview, cardiovascular, ...</td>\n",
       "      <td>[cardiovascular, mean]</td>\n",
       "      <td>0.744068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>690606</td>\n",
       "      <td>8245686</td>\n",
       "      <td>what is a melamine sponge</td>\n",
       "      <td>Gather together what you'll paint, tape if you...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[gather, together, paint, tape, want, clean, e...</td>\n",
       "      <td>[melamine, sponge]</td>\n",
       "      <td>0.370762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002889</td>\n",
       "      <td>4722064</td>\n",
       "      <td>when was john t scopes trial</td>\n",
       "      <td>Before the time you must appear in court, you ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[time, must, appear, court, may, mail, bring, ...</td>\n",
       "      <td>[john, scopes, trial]</td>\n",
       "      <td>0.448473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002426</td>\n",
       "      <td>4035472</td>\n",
       "      <td>when was the shazam movie released?sadasdasdas...</td>\n",
       "      <td>Jem and the Holograms will be released in Octo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[jem, holograms, release, october, hasbro, pro...</td>\n",
       "      <td>[shazam, movie, release, sadasdasdasdsadasasda...</td>\n",
       "      <td>0.614503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>517386</td>\n",
       "      <td>5209492</td>\n",
       "      <td>the medical term meaning the separation of the...</td>\n",
       "      <td>The focus light rays are then directed to the ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[focus, light, ray, direct, back, eye, retina,...</td>\n",
       "      <td>[medical, term, mean, separation, retina, chor...</td>\n",
       "      <td>0.755159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9589</th>\n",
       "      <td>1095085</td>\n",
       "      <td>7617014</td>\n",
       "      <td>how productive is corn per acre</td>\n",
       "      <td>How big is an acre? Officially, one acre is 4,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[big, acre, officially, one, acre, square, yar...</td>\n",
       "      <td>[productive, corn, per, acre]</td>\n",
       "      <td>0.849283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9590</th>\n",
       "      <td>1095278</td>\n",
       "      <td>7964885</td>\n",
       "      <td>average temp in fort worth in march</td>\n",
       "      <td>The weather is screwy this year. West Texas ha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[weather, screwy, year, west, texas, lot, rain...</td>\n",
       "      <td>[average, temp, fort, worth, march]</td>\n",
       "      <td>0.534492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9591</th>\n",
       "      <td>385652</td>\n",
       "      <td>7462070</td>\n",
       "      <td>how to use stall bars</td>\n",
       "      <td>That is the pro level – no doubts about that. ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[pro, level, doubt, flag, wholebody, exercise,...</td>\n",
       "      <td>[use, stall, bar]</td>\n",
       "      <td>0.583181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9592</th>\n",
       "      <td>1051214</td>\n",
       "      <td>1590950</td>\n",
       "      <td>what is meningitis.</td>\n",
       "      <td>Meningitis is an inflammation of the membranes...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[meningitis, inflammation, membranes, meninges...</td>\n",
       "      <td>[meningitis]</td>\n",
       "      <td>0.728519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9593</th>\n",
       "      <td>632394</td>\n",
       "      <td>2485870</td>\n",
       "      <td>what does argan oil do for skin</td>\n",
       "      <td>I use apricot kernel oil and argan oil. Both h...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[use, apricot, kernel, oil, argan, oil, exactl...</td>\n",
       "      <td>[argan, oil, skin]</td>\n",
       "      <td>0.793455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9594 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          qid      pid                                            queries  \\\n",
       "0     1085535  7118287                             cardiovascular meaning   \n",
       "1      690606  8245686                          what is a melamine sponge   \n",
       "2     1002889  4722064                       when was john t scopes trial   \n",
       "3     1002426  4035472  when was the shazam movie released?sadasdasdas...   \n",
       "4      517386  5209492  the medical term meaning the separation of the...   \n",
       "...       ...      ...                                                ...   \n",
       "9589  1095085  7617014                    how productive is corn per acre   \n",
       "9590  1095278  7964885                average temp in fort worth in march   \n",
       "9591   385652  7462070                              how to use stall bars   \n",
       "9592  1051214  1590950                                what is meningitis.   \n",
       "9593   632394  2485870                    what does argan oil do for skin   \n",
       "\n",
       "                                                passage  relevancy  \\\n",
       "0     Dictionary entry overview: What does cardiovas...        1.0   \n",
       "1     Gather together what you'll paint, tape if you...        0.0   \n",
       "2     Before the time you must appear in court, you ...        0.0   \n",
       "3     Jem and the Holograms will be released in Octo...        0.0   \n",
       "4     The focus light rays are then directed to the ...        0.0   \n",
       "...                                                 ...        ...   \n",
       "9589  How big is an acre? Officially, one acre is 4,...        1.0   \n",
       "9590  The weather is screwy this year. West Texas ha...        0.0   \n",
       "9591  That is the pro level – no doubts about that. ...        1.0   \n",
       "9592  Meningitis is an inflammation of the membranes...        1.0   \n",
       "9593  I use apricot kernel oil and argan oil. Both h...        0.0   \n",
       "\n",
       "                                        passage_cleaned  \\\n",
       "0     [dictionary, entry, overview, cardiovascular, ...   \n",
       "1     [gather, together, paint, tape, want, clean, e...   \n",
       "2     [time, must, appear, court, may, mail, bring, ...   \n",
       "3     [jem, holograms, release, october, hasbro, pro...   \n",
       "4     [focus, light, ray, direct, back, eye, retina,...   \n",
       "...                                                 ...   \n",
       "9589  [big, acre, officially, one, acre, square, yar...   \n",
       "9590  [weather, screwy, year, west, texas, lot, rain...   \n",
       "9591  [pro, level, doubt, flag, wholebody, exercise,...   \n",
       "9592  [meningitis, inflammation, membranes, meninges...   \n",
       "9593  [use, apricot, kernel, oil, argan, oil, exactl...   \n",
       "\n",
       "                                          query_cleaned  co_similarity  \n",
       "0                                [cardiovascular, mean]       0.744068  \n",
       "1                                    [melamine, sponge]       0.370762  \n",
       "2                                 [john, scopes, trial]       0.448473  \n",
       "3     [shazam, movie, release, sadasdasdasdsadasasda...       0.614503  \n",
       "4     [medical, term, mean, separation, retina, chor...       0.755159  \n",
       "...                                                 ...            ...  \n",
       "9589                      [productive, corn, per, acre]       0.849283  \n",
       "9590                [average, temp, fort, worth, march]       0.534492  \n",
       "9591                                  [use, stall, bar]       0.583181  \n",
       "9592                                       [meningitis]       0.728519  \n",
       "9593                                 [argan, oil, skin]       0.793455  \n",
       "\n",
       "[9594 rows x 8 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kang\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x_val)):\n",
    "    query_vector = average_query_vectors_val[i]\n",
    "    passage_vector = average_document_vectors_val[i]\n",
    "    x_val[i][0] = cosine_sim_formula(query_vector, passage_vector)\n",
    "    if np.isnan(x_val[i][0]):\n",
    "        x_val[i][0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data['co_similarity'] = x_val[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>queries</th>\n",
       "      <th>passage</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>passage_cleaned</th>\n",
       "      <th>query_cleaned</th>\n",
       "      <th>co_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194870</td>\n",
       "      <td>3995086</td>\n",
       "      <td>ghost meaning urban</td>\n",
       "      <td>1590s, phantom, ghost, a figurative use from L...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1590s, phantom, ghost, figurative, use, latin...</td>\n",
       "      <td>[ghost, mean, urban]</td>\n",
       "      <td>0.552700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1099823</td>\n",
       "      <td>7221124</td>\n",
       "      <td>homes in seminole florida</td>\n",
       "      <td>Find Seminole, FL real estate for sale. Today,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[find, seminole, fl, real, estate, sale, today...</td>\n",
       "      <td>[home, seminole, florida]</td>\n",
       "      <td>0.434982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1085192</td>\n",
       "      <td>7121349</td>\n",
       "      <td>what credit score do i need to use va home loan</td>\n",
       "      <td>In fact, one of the most common questions we r...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[fact, one, common, question, receive, get, va...</td>\n",
       "      <td>[credit, score, need, use, va, home, loan]</td>\n",
       "      <td>0.806369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1092093</td>\n",
       "      <td>7199457</td>\n",
       "      <td>number of people ditching their landline</td>\n",
       "      <td>Phone #: 803-273-4855 Type: Landline Address: ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[phone, type, landline, address, n, main, st, ...</td>\n",
       "      <td>[number, people, ditch, landline]</td>\n",
       "      <td>0.584402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1087361</td>\n",
       "      <td>7102509</td>\n",
       "      <td>what are requirements for minor in spanish at uta</td>\n",
       "      <td>The Spanish Minor. The minor in Spanish consis...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[spanish, minor, minor, spanish, consist, mini...</td>\n",
       "      <td>[requirements, minor, spanish, uta]</td>\n",
       "      <td>0.724819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2411</th>\n",
       "      <td>583325</td>\n",
       "      <td>7453701</td>\n",
       "      <td>what care must you give vinyl siding</td>\n",
       "      <td>Nevertheless, common sense dictates that build...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[nevertheless, common, sense, dictate, builder...</td>\n",
       "      <td>[care, must, give, vinyl, side]</td>\n",
       "      <td>0.669764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412</th>\n",
       "      <td>1084982</td>\n",
       "      <td>3386613</td>\n",
       "      <td>what did stromatolites add to the atmosphere</td>\n",
       "      <td>Since sound waves are a compression and rarefa...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[since, sound, wave, compression, rarefaction,...</td>\n",
       "      <td>[stromatolites, add, atmosphere]</td>\n",
       "      <td>0.601271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2413</th>\n",
       "      <td>197024</td>\n",
       "      <td>6285138</td>\n",
       "      <td>greenhorns meaning</td>\n",
       "      <td>The name Conway is a Celtic baby name. In Celt...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[name, conway, celtic, baby, name, celtic, mea...</td>\n",
       "      <td>[greenhorns, mean]</td>\n",
       "      <td>0.397129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2414</th>\n",
       "      <td>1003114</td>\n",
       "      <td>7753905</td>\n",
       "      <td>where was paul newman born</td>\n",
       "      <td>Paul Newman. Paul Newman (born January 26, 192...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[paul, newman, paul, newman, bear, january, se...</td>\n",
       "      <td>[paul, newman, bear]</td>\n",
       "      <td>0.785465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2415</th>\n",
       "      <td>127876</td>\n",
       "      <td>7075540</td>\n",
       "      <td>define tapestry</td>\n",
       "      <td>TAPESTRY Defined for English Language Learners...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[tapestry, define, english, language, learners...</td>\n",
       "      <td>[define, tapestry]</td>\n",
       "      <td>0.657356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2416 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          qid      pid                                            queries  \\\n",
       "0      194870  3995086                                ghost meaning urban   \n",
       "1     1099823  7221124                          homes in seminole florida   \n",
       "2     1085192  7121349    what credit score do i need to use va home loan   \n",
       "3     1092093  7199457           number of people ditching their landline   \n",
       "4     1087361  7102509  what are requirements for minor in spanish at uta   \n",
       "...       ...      ...                                                ...   \n",
       "2411   583325  7453701               what care must you give vinyl siding   \n",
       "2412  1084982  3386613       what did stromatolites add to the atmosphere   \n",
       "2413   197024  6285138                                 greenhorns meaning   \n",
       "2414  1003114  7753905                         where was paul newman born   \n",
       "2415   127876  7075540                                    define tapestry   \n",
       "\n",
       "                                                passage  relevancy  \\\n",
       "0     1590s, phantom, ghost, a figurative use from L...        0.0   \n",
       "1     Find Seminole, FL real estate for sale. Today,...        1.0   \n",
       "2     In fact, one of the most common questions we r...        1.0   \n",
       "3     Phone #: 803-273-4855 Type: Landline Address: ...        0.0   \n",
       "4     The Spanish Minor. The minor in Spanish consis...        1.0   \n",
       "...                                                 ...        ...   \n",
       "2411  Nevertheless, common sense dictates that build...        1.0   \n",
       "2412  Since sound waves are a compression and rarefa...        0.0   \n",
       "2413  The name Conway is a Celtic baby name. In Celt...        0.0   \n",
       "2414  Paul Newman. Paul Newman (born January 26, 192...        1.0   \n",
       "2415  TAPESTRY Defined for English Language Learners...        1.0   \n",
       "\n",
       "                                        passage_cleaned  \\\n",
       "0     [1590s, phantom, ghost, figurative, use, latin...   \n",
       "1     [find, seminole, fl, real, estate, sale, today...   \n",
       "2     [fact, one, common, question, receive, get, va...   \n",
       "3     [phone, type, landline, address, n, main, st, ...   \n",
       "4     [spanish, minor, minor, spanish, consist, mini...   \n",
       "...                                                 ...   \n",
       "2411  [nevertheless, common, sense, dictate, builder...   \n",
       "2412  [since, sound, wave, compression, rarefaction,...   \n",
       "2413  [name, conway, celtic, baby, name, celtic, mea...   \n",
       "2414  [paul, newman, paul, newman, bear, january, se...   \n",
       "2415  [tapestry, define, english, language, learners...   \n",
       "\n",
       "                                   query_cleaned  co_similarity  \n",
       "0                           [ghost, mean, urban]       0.552700  \n",
       "1                      [home, seminole, florida]       0.434982  \n",
       "2     [credit, score, need, use, va, home, loan]       0.806369  \n",
       "3              [number, people, ditch, landline]       0.584402  \n",
       "4            [requirements, minor, spanish, uta]       0.724819  \n",
       "...                                          ...            ...  \n",
       "2411             [care, must, give, vinyl, side]       0.669764  \n",
       "2412            [stromatolites, add, atmosphere]       0.601271  \n",
       "2413                          [greenhorns, mean]       0.397129  \n",
       "2414                        [paul, newman, bear]       0.785465  \n",
       "2415                          [define, tapestry]       0.657356  \n",
       "\n",
       "[2416 rows x 8 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data.relevancy.values\n",
    "y_val = validation_data.relevancy.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_no_dup_passages = train_data.drop_duplicates(subset=['pid'], inplace=False)\n",
    "validation_data_no_dup_passages = validation_data.drop_duplicates(subset=['pid'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9578, 8)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2416, 8)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_no_dup_passages.shape\n",
    "validation_data_no_dup_passages.shape\n",
    "N_train = train_data_no_dup_passages.shape[0]\n",
    "N_val = validation_data_no_dup_passages.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_passage_average_length(dataframe_no_dup_passages):\n",
    "    number_of_passages = validation_data_no_dup_passages.shape[0]\n",
    "    count_total_length = 0\n",
    "    for idx, row in dataframe_no_dup_passages.iterrows():\n",
    "        count_total_length += len(row['passage_cleaned'])\n",
    "    return count_total_length/number_of_passages \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_word_occurences, avdl = get_passage_average_length_and_total_word_occurences_corpus()\n",
    "avdl_train = get_passage_average_length(train_data_no_dup_passages)\n",
    "avdl_val = get_passage_average_length(validation_data_no_dup_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "k1 = 1.2\n",
    "k2 = 100\n",
    "b = 0.75\n",
    "R = 0\n",
    "r = 0\n",
    "# N = len(validation_data_no_dup_passages)\n",
    "\n",
    "def K_cal(dl, avdl):\n",
    "    return k1 * ((1-b) + b * (float(dl)/float(avdl)) )\n",
    "\n",
    "\n",
    "def BM25_cal_for_preprocessed_words(query_tokens, passage_tokens, N, avdl):\n",
    "    query_length = len(query_tokens)\n",
    "    query_token_freq_dict = nltk.FreqDist(query_tokens)\n",
    "    passage_token_freq_dict = nltk.FreqDist(passage_tokens)\n",
    "    dl = len(passage_tokens)\n",
    "    K = K_cal(dl, avdl)\n",
    "    score = 0\n",
    "    for token in query_tokens:\n",
    "        try:\n",
    "            n = len(inverted_index[token])\n",
    "        except:\n",
    "            n = 0\n",
    "        f = passage_token_freq_dict[token]\n",
    "        qf = query_token_freq_dict[token]\n",
    "        first_term = log( ( (r + 0.5) / (R - r + 0.5) ) / ( (n - r + 0.5) / (N - n - R + r + 0.5)) )\n",
    "        second_term = ((k1 + 1) * f) / (K + f)\n",
    "        third_term = ((k2+1) * qf) / (k2 + qf)\n",
    "        score += first_term * second_term * third_term\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in train_data.iterrows():\n",
    "    query_cleaned = row['query_cleaned']\n",
    "    passage_cleaned = row['passage_cleaned']\n",
    "    bm25_score = BM25_cal_for_preprocessed_words(query_cleaned, passage_cleaned, N_train, avdl_train)\n",
    "    x_train[idx][1] = bm25_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in validation_data.iterrows():\n",
    "    query_cleaned = row['query_cleaned']\n",
    "    passage_cleaned = row['passage_cleaned']\n",
    "    bm25_score = BM25_cal_for_preprocessed_words(query_cleaned, passage_cleaned, N_val, avdl_val)\n",
    "    x_val[idx][1] = bm25_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['bm25'] = x_train[:,1]\n",
    "validation_data['bm25'] = x_val[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>queries</th>\n",
       "      <th>passage</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>passage_cleaned</th>\n",
       "      <th>query_cleaned</th>\n",
       "      <th>co_similarity</th>\n",
       "      <th>bm25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1085535</td>\n",
       "      <td>7118287</td>\n",
       "      <td>cardiovascular meaning</td>\n",
       "      <td>Dictionary entry overview: What does cardiovas...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[dictionary, entry, overview, cardiovascular, ...</td>\n",
       "      <td>[cardiovascular, mean]</td>\n",
       "      <td>0.744068</td>\n",
       "      <td>34.467970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>690606</td>\n",
       "      <td>8245686</td>\n",
       "      <td>what is a melamine sponge</td>\n",
       "      <td>Gather together what you'll paint, tape if you...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[gather, together, paint, tape, want, clean, e...</td>\n",
       "      <td>[melamine, sponge]</td>\n",
       "      <td>0.370762</td>\n",
       "      <td>17.559426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002889</td>\n",
       "      <td>4722064</td>\n",
       "      <td>when was john t scopes trial</td>\n",
       "      <td>Before the time you must appear in court, you ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[time, must, appear, court, may, mail, bring, ...</td>\n",
       "      <td>[john, scopes, trial]</td>\n",
       "      <td>0.448473</td>\n",
       "      <td>19.129331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002426</td>\n",
       "      <td>4035472</td>\n",
       "      <td>when was the shazam movie released?sadasdasdas...</td>\n",
       "      <td>Jem and the Holograms will be released in Octo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[jem, holograms, release, october, hasbro, pro...</td>\n",
       "      <td>[shazam, movie, release, sadasdasdasdsadasasda...</td>\n",
       "      <td>0.614503</td>\n",
       "      <td>34.327847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>517386</td>\n",
       "      <td>5209492</td>\n",
       "      <td>the medical term meaning the separation of the...</td>\n",
       "      <td>The focus light rays are then directed to the ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[focus, light, ray, direct, back, eye, retina,...</td>\n",
       "      <td>[medical, term, mean, separation, retina, chor...</td>\n",
       "      <td>0.755159</td>\n",
       "      <td>47.251678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9589</th>\n",
       "      <td>1095085</td>\n",
       "      <td>7617014</td>\n",
       "      <td>how productive is corn per acre</td>\n",
       "      <td>How big is an acre? Officially, one acre is 4,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[big, acre, officially, one, acre, square, yar...</td>\n",
       "      <td>[productive, corn, per, acre]</td>\n",
       "      <td>0.849283</td>\n",
       "      <td>54.794223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9590</th>\n",
       "      <td>1095278</td>\n",
       "      <td>7964885</td>\n",
       "      <td>average temp in fort worth in march</td>\n",
       "      <td>The weather is screwy this year. West Texas ha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[weather, screwy, year, west, texas, lot, rain...</td>\n",
       "      <td>[average, temp, fort, worth, march]</td>\n",
       "      <td>0.534492</td>\n",
       "      <td>28.665872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9591</th>\n",
       "      <td>385652</td>\n",
       "      <td>7462070</td>\n",
       "      <td>how to use stall bars</td>\n",
       "      <td>That is the pro level – no doubts about that. ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[pro, level, doubt, flag, wholebody, exercise,...</td>\n",
       "      <td>[use, stall, bar]</td>\n",
       "      <td>0.583181</td>\n",
       "      <td>33.372590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9592</th>\n",
       "      <td>1051214</td>\n",
       "      <td>1590950</td>\n",
       "      <td>what is meningitis.</td>\n",
       "      <td>Meningitis is an inflammation of the membranes...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[meningitis, inflammation, membranes, meninges...</td>\n",
       "      <td>[meningitis]</td>\n",
       "      <td>0.728519</td>\n",
       "      <td>19.134410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9593</th>\n",
       "      <td>632394</td>\n",
       "      <td>2485870</td>\n",
       "      <td>what does argan oil do for skin</td>\n",
       "      <td>I use apricot kernel oil and argan oil. Both h...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[use, apricot, kernel, oil, argan, oil, exactl...</td>\n",
       "      <td>[argan, oil, skin]</td>\n",
       "      <td>0.793455</td>\n",
       "      <td>34.363703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9594 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          qid      pid                                            queries  \\\n",
       "0     1085535  7118287                             cardiovascular meaning   \n",
       "1      690606  8245686                          what is a melamine sponge   \n",
       "2     1002889  4722064                       when was john t scopes trial   \n",
       "3     1002426  4035472  when was the shazam movie released?sadasdasdas...   \n",
       "4      517386  5209492  the medical term meaning the separation of the...   \n",
       "...       ...      ...                                                ...   \n",
       "9589  1095085  7617014                    how productive is corn per acre   \n",
       "9590  1095278  7964885                average temp in fort worth in march   \n",
       "9591   385652  7462070                              how to use stall bars   \n",
       "9592  1051214  1590950                                what is meningitis.   \n",
       "9593   632394  2485870                    what does argan oil do for skin   \n",
       "\n",
       "                                                passage  relevancy  \\\n",
       "0     Dictionary entry overview: What does cardiovas...        1.0   \n",
       "1     Gather together what you'll paint, tape if you...        0.0   \n",
       "2     Before the time you must appear in court, you ...        0.0   \n",
       "3     Jem and the Holograms will be released in Octo...        0.0   \n",
       "4     The focus light rays are then directed to the ...        0.0   \n",
       "...                                                 ...        ...   \n",
       "9589  How big is an acre? Officially, one acre is 4,...        1.0   \n",
       "9590  The weather is screwy this year. West Texas ha...        0.0   \n",
       "9591  That is the pro level – no doubts about that. ...        1.0   \n",
       "9592  Meningitis is an inflammation of the membranes...        1.0   \n",
       "9593  I use apricot kernel oil and argan oil. Both h...        0.0   \n",
       "\n",
       "                                        passage_cleaned  \\\n",
       "0     [dictionary, entry, overview, cardiovascular, ...   \n",
       "1     [gather, together, paint, tape, want, clean, e...   \n",
       "2     [time, must, appear, court, may, mail, bring, ...   \n",
       "3     [jem, holograms, release, october, hasbro, pro...   \n",
       "4     [focus, light, ray, direct, back, eye, retina,...   \n",
       "...                                                 ...   \n",
       "9589  [big, acre, officially, one, acre, square, yar...   \n",
       "9590  [weather, screwy, year, west, texas, lot, rain...   \n",
       "9591  [pro, level, doubt, flag, wholebody, exercise,...   \n",
       "9592  [meningitis, inflammation, membranes, meninges...   \n",
       "9593  [use, apricot, kernel, oil, argan, oil, exactl...   \n",
       "\n",
       "                                          query_cleaned  co_similarity  \\\n",
       "0                                [cardiovascular, mean]       0.744068   \n",
       "1                                    [melamine, sponge]       0.370762   \n",
       "2                                 [john, scopes, trial]       0.448473   \n",
       "3     [shazam, movie, release, sadasdasdasdsadasasda...       0.614503   \n",
       "4     [medical, term, mean, separation, retina, chor...       0.755159   \n",
       "...                                                 ...            ...   \n",
       "9589                      [productive, corn, per, acre]       0.849283   \n",
       "9590                [average, temp, fort, worth, march]       0.534492   \n",
       "9591                                  [use, stall, bar]       0.583181   \n",
       "9592                                       [meningitis]       0.728519   \n",
       "9593                                 [argan, oil, skin]       0.793455   \n",
       "\n",
       "           bm25  \n",
       "0     34.467970  \n",
       "1     17.559426  \n",
       "2     19.129331  \n",
       "3     34.327847  \n",
       "4     47.251678  \n",
       "...         ...  \n",
       "9589  54.794223  \n",
       "9590  28.665872  \n",
       "9591  33.372590  \n",
       "9592  19.134410  \n",
       "9593  34.363703  \n",
       "\n",
       "[9594 rows x 9 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_pickle('train_data.pkl')\n",
    "validation_data.to_pickle('validation_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word embedding cosine similarity 보다 정확하지 않을 가능성이 있고, time consuming 한 작업이라, 이 feature는 추가하지 않았다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data into metrics form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0],x_train.shape[1], 1)\n",
    "x_val = x_val.reshape(x_val.shape[0],x_val.shape[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logisitc Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 현재 문제 \n",
    "1. 딥러닝 과제처럼, 한 data씩 처리 할건지, 아니면 인터넷 예제처럼 모든 value를 metrics 에 넣어서 한번에 처리할건지\n",
    "2. 딥러닝 과제처럼 처리했을 경우, gradient descent function은 무엇인지, 인터넷 예제처럼 했을 때도, 인터넷에 있는 gradient descent function 이 어떻게 derive 됬는지 알기\n",
    "\n",
    "A: 결국엔 둘다 똑같은 방법인데, metrics 로 처리하는 경우는 한번에 모든 데이터를 처리하는 경우이고, 딥러닝 과제는 한 data sampling 을 하나씩 처리하는 방법이다. 결국엔 둘 다 똑같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.74406832],\n",
       "        [34.46797033]],\n",
       "\n",
       "       [[ 0.37076238],\n",
       "        [17.55942644]],\n",
       "\n",
       "       [[ 0.44847348],\n",
       "        [19.12933093]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.58318121],\n",
       "        [33.37259037]],\n",
       "\n",
       "       [[ 0.72851865],\n",
       "        [19.13440973]],\n",
       "\n",
       "       [[ 0.79345451],\n",
       "        [34.36370305]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9594, 2, 1)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37076238],\n",
       "       [17.55942644]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x_train[1]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.rand(1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21645978, 0.05495869]])"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.531434157722148"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(weights, x)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = weights.reshape(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45.76190532])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((1, 2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.lr = 0.01\n",
    "        self.epoch = 100\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss_train = None\n",
    "        self.loss_val = None\n",
    "\n",
    "        self.acc_train = None\n",
    "        self.train_correct = None\n",
    "        self.val_correct = None\n",
    "        \n",
    "        self.losslist_train = []\n",
    "        self.losslist_val = []\n",
    "        self.acclist_train = []\n",
    "        self.acclist_val = []\n",
    "    \n",
    "\n",
    "        \n",
    "    def fit(self, trainxs, trainys, trainxs_val, trainys_val):\n",
    "        n_samples, n_features, _ = trainxs.shape\n",
    "        n_samples_val = trainxs_val.shape[0]\n",
    "\n",
    "        # init parameters\n",
    "#         self.weights = np.zeros((n_features, 1))\n",
    "        self.weights = np.zeros((1, n_features))\n",
    "        self.bias = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        # gradient descent\n",
    "        for e in range(self.epoch):\n",
    "            idx = self.shuffleIdx(trainxs.shape[0])\n",
    "            X = trainxs[idx]\n",
    "            Y = trainys[idx]\n",
    "            \n",
    "            \n",
    "            self.loss_train = 0\n",
    "            self.loss_val = 0\n",
    "\n",
    "            self.acc_train = 0\n",
    "            self.train_correct = 0\n",
    "            self.val_correct = 0\n",
    "            total_weight_gradient1 = 0\n",
    "            total_weight_gradient2 = 0\n",
    "            \n",
    "            total_bias_gradient = 0\n",
    "            \n",
    "            for i in range(trainxs.shape[0]):\n",
    "                x = X[i]\n",
    "                y = Y[i]\n",
    "                # approximate output variable (y) with linear combination of weights and x, plus bias\n",
    "                linear_equation = np.dot(self.weights, x)[0][0] + self.bias\n",
    "                # apply sigmoid function\n",
    "                prediction = self.sigmoid(linear_equation)\n",
    "                if prediction >= 0.5:\n",
    "                    yprime = 1\n",
    "                else:\n",
    "                    yprime = 0\n",
    "\n",
    "                if yprime == y:\n",
    "                    self.train_correct += 1\n",
    "\n",
    "                self.loss_train += self.loss_function(y, prediction)\n",
    "\n",
    "                # compute gradients\n",
    "                dw1 = (prediction - y) * x[0]  #derivative w.r.t weight1\n",
    "                dw2 = (prediction - y) * x[1] #derivative w.r.t weight2\n",
    "                db = prediction - y  #derivative w.r.t bias\n",
    "                    \n",
    "                    \n",
    "                total_weight_gradient1 += dw1\n",
    "                total_weight_gradient2 += dw2\n",
    "                total_bias_gradient += db\n",
    "               \n",
    "                 # COMPUTING LOSS AND ACCURACY OF VALIDATION SET\n",
    "                if (i < trainxs_val.shape[0]):\n",
    "                    val_x = trainxs_val[i]\n",
    "                    val_y = trainys_val[i]\n",
    "                    linear_equation = np.dot(self.weights, val_x)[0][0] + self.bias\n",
    "                    prediction = self.sigmoid(linear_equation)\n",
    "\n",
    "                    if prediction  >= 0.5:\n",
    "                        yprime = 1\n",
    "                    else:\n",
    "                        yprime = 0\n",
    "\n",
    "                    if yprime == val_y:\n",
    "                        self.val_correct += 1\n",
    "\n",
    "                    self.loss_val += ((val_y - prediction)**2)/2\n",
    "\n",
    "                    \n",
    "            # COMPUTING THE AVERAGE OF GRADIENTS FOR EACH EPOCH SINCE WE ARE DOING        \n",
    "            # FULL-BATCH GRADIENT DESCEND AND UPDATING THE PARAMETERS AFTER EACH EPOCH\n",
    "            total_weight_gradient1 = total_weight_gradient1 / n_samples\n",
    "            total_weight_gradient2 = total_weight_gradient2 / n_samples\n",
    "            total_bias_gradient = total_bias_gradient / n_samples\n",
    "\n",
    "            # update parameters\n",
    "            self.weights[0][0] -= self.lr * total_weight_gradient1\n",
    "            self.weights[0][1] -= self.lr * total_weight_gradient2\n",
    "            self.bias -= self.lr * total_bias_gradient\n",
    "            \n",
    "            self.loss_train = self.loss_train/n_samples\n",
    "            self.losslist_train.append(self.loss_train)\n",
    "\n",
    "            self.loss_val = self.loss_val/(n_samples_val)\n",
    "            self.losslist_val.append(self.loss_val)\n",
    "\n",
    "            self.train_correct = self.train_correct/n_samples\n",
    "            self.acclist_train.append(self.train_correct)\n",
    "\n",
    "            self.val_correct = self.val_correct/(n_samples_val)\n",
    "            self.acclist_val.append(self.val_correct)\n",
    "            \n",
    "            print(\"- Loss on Training / Validation Data at Epoch {}: {:.4f} / {:.4f}\".format(e+1, self.loss_train, self.loss_val)) \n",
    "            print(\"- Accuracy on Training / Validation Data at Epoch {}: {} / {}\\n\".format(e+1, self.train_correct, self.val_correct))\n",
    "\n",
    "            \n",
    "    def shuffleIdx(self, n):\n",
    "        rng = default_rng()\n",
    "        rand_idx = rng.permutation(n)\n",
    "        return rand_idx\n",
    "\n",
    "    def predict(self, X):\n",
    "#         linear_equation = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted_cls = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i]\n",
    "            linear_equation = np.dot(self.weights, x)[0][0] + self.bias\n",
    "            y_predicted = self.sigmoid(linear_equation)\n",
    "            print(y_predicted)\n",
    "            if y_predicted > 0.5:\n",
    "                y_predicted_cls[i] = 1\n",
    "            else:\n",
    "                y_predicted_cls[i] = 0\n",
    "#             y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "        return y_predicted_cls\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def loss_function(self, y, prediction):\n",
    "        return -log((1 - prediction)**(1 - y)) - log(prediction**y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loss on Training / Validation Data at Epoch 1: 0.6931 / 0.1250\n",
      "- Accuracy on Training / Validation Data at Epoch 1: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 2: 0.8013 / 0.1317\n",
      "- Accuracy on Training / Validation Data at Epoch 2: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 3: 1.1796 / 0.1742\n",
      "- Accuracy on Training / Validation Data at Epoch 3: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 4: 1.8239 / 0.1994\n",
      "- Accuracy on Training / Validation Data at Epoch 4: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 5: 0.8140 / 0.1415\n",
      "- Accuracy on Training / Validation Data at Epoch 5: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 6: 1.3464 / 0.1748\n",
      "- Accuracy on Training / Validation Data at Epoch 6: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 7: 1.1652 / 0.1732\n",
      "- Accuracy on Training / Validation Data at Epoch 7: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 8: 1.8172 / 0.1990\n",
      "- Accuracy on Training / Validation Data at Epoch 8: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 9: 0.8170 / 0.1419\n",
      "- Accuracy on Training / Validation Data at Epoch 9: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 10: 1.3559 / 0.1753\n",
      "- Accuracy on Training / Validation Data at Epoch 10: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 11: 1.1570 / 0.1727\n",
      "- Accuracy on Training / Validation Data at Epoch 11: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 12: 1.8131 / 0.1988\n",
      "- Accuracy on Training / Validation Data at Epoch 12: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 13: 0.8185 / 0.1421\n",
      "- Accuracy on Training / Validation Data at Epoch 13: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 14: 1.3607 / 0.1755\n",
      "- Accuracy on Training / Validation Data at Epoch 14: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 15: 1.1521 / 0.1724\n",
      "- Accuracy on Training / Validation Data at Epoch 15: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 16: 1.8107 / 0.1986\n",
      "- Accuracy on Training / Validation Data at Epoch 16: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 17: 0.8189 / 0.1422\n",
      "- Accuracy on Training / Validation Data at Epoch 17: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 18: 1.3627 / 0.1755\n",
      "- Accuracy on Training / Validation Data at Epoch 18: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 19: 1.1493 / 0.1723\n",
      "- Accuracy on Training / Validation Data at Epoch 19: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 20: 1.8092 / 0.1984\n",
      "- Accuracy on Training / Validation Data at Epoch 20: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 21: 0.8188 / 0.1422\n",
      "- Accuracy on Training / Validation Data at Epoch 21: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 22: 1.3629 / 0.1754\n",
      "- Accuracy on Training / Validation Data at Epoch 22: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 23: 1.1479 / 0.1722\n",
      "- Accuracy on Training / Validation Data at Epoch 23: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 24: 1.8085 / 0.1983\n",
      "- Accuracy on Training / Validation Data at Epoch 24: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 25: 0.8181 / 0.1421\n",
      "- Accuracy on Training / Validation Data at Epoch 25: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 26: 1.3619 / 0.1752\n",
      "- Accuracy on Training / Validation Data at Epoch 26: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 27: 1.1473 / 0.1722\n",
      "- Accuracy on Training / Validation Data at Epoch 27: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 28: 1.8083 / 0.1982\n",
      "- Accuracy on Training / Validation Data at Epoch 28: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 29: 0.8172 / 0.1420\n",
      "- Accuracy on Training / Validation Data at Epoch 29: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 30: 1.3601 / 0.1750\n",
      "- Accuracy on Training / Validation Data at Epoch 30: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 31: 1.1473 / 0.1723\n",
      "- Accuracy on Training / Validation Data at Epoch 31: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 32: 1.8083 / 0.1981\n",
      "- Accuracy on Training / Validation Data at Epoch 32: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 33: 0.8161 / 0.1419\n",
      "- Accuracy on Training / Validation Data at Epoch 33: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 34: 1.3577 / 0.1747\n",
      "- Accuracy on Training / Validation Data at Epoch 34: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 35: 1.1478 / 0.1724\n",
      "- Accuracy on Training / Validation Data at Epoch 35: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 36: 1.8086 / 0.1980\n",
      "- Accuracy on Training / Validation Data at Epoch 36: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 37: 0.8149 / 0.1418\n",
      "- Accuracy on Training / Validation Data at Epoch 37: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 38: 1.3549 / 0.1744\n",
      "- Accuracy on Training / Validation Data at Epoch 38: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 39: 1.1485 / 0.1725\n",
      "- Accuracy on Training / Validation Data at Epoch 39: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 40: 1.8091 / 0.1980\n",
      "- Accuracy on Training / Validation Data at Epoch 40: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 41: 0.8135 / 0.1417\n",
      "- Accuracy on Training / Validation Data at Epoch 41: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 42: 1.3518 / 0.1741\n",
      "- Accuracy on Training / Validation Data at Epoch 42: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 43: 1.1495 / 0.1726\n",
      "- Accuracy on Training / Validation Data at Epoch 43: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 44: 1.8096 / 0.1979\n",
      "- Accuracy on Training / Validation Data at Epoch 44: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 45: 0.8122 / 0.1416\n",
      "- Accuracy on Training / Validation Data at Epoch 45: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 46: 1.3486 / 0.1738\n",
      "- Accuracy on Training / Validation Data at Epoch 46: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 47: 1.1505 / 0.1727\n",
      "- Accuracy on Training / Validation Data at Epoch 47: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 48: 1.8102 / 0.1978\n",
      "- Accuracy on Training / Validation Data at Epoch 48: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 49: 0.8107 / 0.1414\n",
      "- Accuracy on Training / Validation Data at Epoch 49: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 50: 1.3452 / 0.1735\n",
      "- Accuracy on Training / Validation Data at Epoch 50: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 51: 1.1517 / 0.1728\n",
      "- Accuracy on Training / Validation Data at Epoch 51: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 52: 1.8108 / 0.1977\n",
      "- Accuracy on Training / Validation Data at Epoch 52: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 53: 0.8093 / 0.1413\n",
      "- Accuracy on Training / Validation Data at Epoch 53: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 54: 1.3417 / 0.1732\n",
      "- Accuracy on Training / Validation Data at Epoch 54: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 55: 1.1529 / 0.1730\n",
      "- Accuracy on Training / Validation Data at Epoch 55: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 56: 1.8115 / 0.1977\n",
      "- Accuracy on Training / Validation Data at Epoch 56: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 57: 0.8079 / 0.1412\n",
      "- Accuracy on Training / Validation Data at Epoch 57: 0.5 / 0.5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loss on Training / Validation Data at Epoch 58: 1.3383 / 0.1728\n",
      "- Accuracy on Training / Validation Data at Epoch 58: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 59: 1.1541 / 0.1731\n",
      "- Accuracy on Training / Validation Data at Epoch 59: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 60: 1.8121 / 0.1976\n",
      "- Accuracy on Training / Validation Data at Epoch 60: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 61: 0.8064 / 0.1410\n",
      "- Accuracy on Training / Validation Data at Epoch 61: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 62: 1.3348 / 0.1725\n",
      "- Accuracy on Training / Validation Data at Epoch 62: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 63: 1.1553 / 0.1733\n",
      "- Accuracy on Training / Validation Data at Epoch 63: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 64: 1.8127 / 0.1975\n",
      "- Accuracy on Training / Validation Data at Epoch 64: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 65: 0.8050 / 0.1409\n",
      "- Accuracy on Training / Validation Data at Epoch 65: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 66: 1.3312 / 0.1722\n",
      "- Accuracy on Training / Validation Data at Epoch 66: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 67: 1.1565 / 0.1734\n",
      "- Accuracy on Training / Validation Data at Epoch 67: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 68: 1.8134 / 0.1975\n",
      "- Accuracy on Training / Validation Data at Epoch 68: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 69: 0.8036 / 0.1407\n",
      "- Accuracy on Training / Validation Data at Epoch 69: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 70: 1.3277 / 0.1718\n",
      "- Accuracy on Training / Validation Data at Epoch 70: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 71: 1.1577 / 0.1736\n",
      "- Accuracy on Training / Validation Data at Epoch 71: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 72: 1.8140 / 0.1974\n",
      "- Accuracy on Training / Validation Data at Epoch 72: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 73: 0.8022 / 0.1406\n",
      "- Accuracy on Training / Validation Data at Epoch 73: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 74: 1.3242 / 0.1715\n",
      "- Accuracy on Training / Validation Data at Epoch 74: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 75: 1.1589 / 0.1737\n",
      "- Accuracy on Training / Validation Data at Epoch 75: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 76: 1.8146 / 0.1973\n",
      "- Accuracy on Training / Validation Data at Epoch 76: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 77: 0.8008 / 0.1405\n",
      "- Accuracy on Training / Validation Data at Epoch 77: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 78: 1.3208 / 0.1712\n",
      "- Accuracy on Training / Validation Data at Epoch 78: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 79: 1.1601 / 0.1738\n",
      "- Accuracy on Training / Validation Data at Epoch 79: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 80: 1.8152 / 0.1973\n",
      "- Accuracy on Training / Validation Data at Epoch 80: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 81: 0.7994 / 0.1403\n",
      "- Accuracy on Training / Validation Data at Epoch 81: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 82: 1.3173 / 0.1708\n",
      "- Accuracy on Training / Validation Data at Epoch 82: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 83: 1.1613 / 0.1740\n",
      "- Accuracy on Training / Validation Data at Epoch 83: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 84: 1.8158 / 0.1972\n",
      "- Accuracy on Training / Validation Data at Epoch 84: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 85: 0.7981 / 0.1402\n",
      "- Accuracy on Training / Validation Data at Epoch 85: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 86: 1.3139 / 0.1705\n",
      "- Accuracy on Training / Validation Data at Epoch 86: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 87: 1.1624 / 0.1741\n",
      "- Accuracy on Training / Validation Data at Epoch 87: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 88: 1.8163 / 0.1971\n",
      "- Accuracy on Training / Validation Data at Epoch 88: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 89: 0.7967 / 0.1400\n",
      "- Accuracy on Training / Validation Data at Epoch 89: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 90: 1.3105 / 0.1702\n",
      "- Accuracy on Training / Validation Data at Epoch 90: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 91: 1.1635 / 0.1743\n",
      "- Accuracy on Training / Validation Data at Epoch 91: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 92: 1.8169 / 0.1971\n",
      "- Accuracy on Training / Validation Data at Epoch 92: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 93: 0.7954 / 0.1399\n",
      "- Accuracy on Training / Validation Data at Epoch 93: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 94: 1.3071 / 0.1698\n",
      "- Accuracy on Training / Validation Data at Epoch 94: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 95: 1.1646 / 0.1744\n",
      "- Accuracy on Training / Validation Data at Epoch 95: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 96: 1.8174 / 0.1970\n",
      "- Accuracy on Training / Validation Data at Epoch 96: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 97: 0.7941 / 0.1398\n",
      "- Accuracy on Training / Validation Data at Epoch 97: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 98: 1.3037 / 0.1695\n",
      "- Accuracy on Training / Validation Data at Epoch 98: 0.5042735042735043 / 0.5024834437086093\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 99: 1.1657 / 0.1745\n",
      "- Accuracy on Training / Validation Data at Epoch 99: 0.5 / 0.5\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 100: 1.8179 / 0.1969\n",
      "- Accuracy on Training / Validation Data at Epoch 100: 0.5042735042735043 / 0.5024834437086093\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train, x_val, y_val)\n",
    "# predictions = regressor.predict(xtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.693147180560052]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.losslist_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.842780142686781e-06"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.weights[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5016197037028066\n",
      "0.5022890374129378\n",
      "0.504955777146196\n",
      "0.5010719010843796\n",
      "0.5025019361527566\n",
      "0.5029872983510858\n",
      "0.5012723754570151\n",
      "0.501226630293866\n",
      "0.5039526303849895\n",
      "0.5028443967140354\n",
      "0.5018858495539756\n",
      "0.5026668557464418\n",
      "0.5008947689684733\n",
      "0.5018289427279075\n",
      "0.5034750558938051\n",
      "0.5022330856652734\n",
      "0.5025812711305311\n",
      "0.502069239214692\n",
      "0.5013266607310077\n",
      "0.5059936914826352\n",
      "0.5014022129473298\n",
      "0.5014126703308914\n",
      "0.5018857113670544\n",
      "0.5028801324625938\n",
      "0.500891772404268\n",
      "0.5011839087086408\n",
      "0.5022520595394249\n",
      "0.5026290474948516\n",
      "0.504782385999067\n",
      "0.5075692474591417\n",
      "0.501664949004708\n",
      "0.5030347505956354\n",
      "0.5012588631038509\n",
      "0.5042270133593894\n",
      "0.5013071971187747\n",
      "0.5086019474603561\n",
      "0.5033255250681189\n",
      "0.5018654311271283\n",
      "0.502136001316667\n",
      "0.5023218158371903\n",
      "0.5011387154866549\n",
      "0.5024458644431011\n",
      "0.5011609661071165\n",
      "0.5015494997474127\n",
      "0.5025557010332156\n",
      "0.5027674936073048\n",
      "0.5009086311857309\n",
      "0.5013674239715816\n",
      "0.5033766868169738\n",
      "0.5030870371447693\n",
      "0.5012350986814106\n",
      "0.5011722929730792\n",
      "0.5008326289367889\n",
      "0.5024700280182658\n",
      "0.5023290394674002\n",
      "0.5029524591696745\n",
      "0.5014330982248283\n",
      "0.502202142924336\n",
      "0.5019720373834499\n",
      "0.5011554511993279\n",
      "0.5033260472553795\n",
      "0.5022464623090634\n",
      "0.5033741772388067\n",
      "0.5010579652690075\n",
      "0.5019066442509496\n",
      "0.50126915939356\n",
      "0.5013872539704095\n",
      "0.5030998500576181\n",
      "0.5017174978794425\n",
      "0.5041753392986366\n",
      "0.5007991178001714\n",
      "0.5014396572018166\n",
      "0.5025667145713314\n",
      "0.5012978609805092\n",
      "0.5041985259049834\n",
      "0.5028518511573036\n",
      "0.5019909578342371\n",
      "0.5009961469405687\n",
      "0.5046080830736765\n",
      "0.5032943571902992\n",
      "0.5014092166618326\n",
      "0.5009337527689862\n",
      "0.5034970676521382\n",
      "0.5050971062705129\n",
      "0.5007584829964005\n",
      "0.5008100129389988\n",
      "0.5021539411395489\n",
      "0.5034369397748265\n",
      "0.5013557221285951\n",
      "0.5019427109151939\n",
      "0.5034863849379624\n",
      "0.5013456863782534\n",
      "0.5013544070892021\n",
      "0.5020733159710231\n",
      "0.5014713102034662\n",
      "0.5021448385067213\n",
      "0.502213797694677\n",
      "0.5029822570629187\n",
      "0.5014469907932555\n",
      "0.5011387710688218\n",
      "0.5031547162180053\n",
      "0.5022819489965159\n",
      "0.5010673274700336\n",
      "0.503955926109475\n",
      "0.5024652175815524\n",
      "0.5023443693135762\n",
      "0.500778034896177\n",
      "0.5029558054805765\n",
      "0.5028204570374211\n",
      "0.5021183080883117\n",
      "0.5013480582893474\n",
      "0.5020923676278459\n",
      "0.5028161487646257\n",
      "0.5013912609566624\n",
      "0.5011173440656416\n",
      "0.5015112905984127\n",
      "0.5018599874034014\n",
      "0.503118103846641\n",
      "0.5008208722173108\n",
      "0.5013255667446026\n",
      "0.5025401257359456\n",
      "0.5025971268177395\n",
      "0.5017633450348097\n",
      "0.5053641979691814\n",
      "0.5008986009649226\n",
      "0.502042428019675\n",
      "0.5046124117733877\n",
      "0.5028948919271814\n",
      "0.5019949370511935\n",
      "0.502607206681581\n",
      "0.5024584932403641\n",
      "0.5036872844554962\n",
      "0.5010718639663956\n",
      "0.5006108474916251\n",
      "0.501127954380714\n",
      "0.5020488138548447\n",
      "0.5049594426503592\n",
      "0.5013109902649646\n",
      "0.500000242781991\n",
      "0.5010770731284719\n",
      "0.501960272041245\n",
      "0.50504690931853\n",
      "0.5013366676170603\n",
      "0.5021989524336282\n",
      "0.501363394720181\n",
      "0.5010004325476707\n",
      "0.5035873680169988\n",
      "0.5043555123137634\n",
      "0.5016884531602325\n",
      "0.5023112279438563\n",
      "0.5039091602597736\n",
      "0.5010674405050833\n",
      "0.5011279008419378\n",
      "0.5012967427104239\n",
      "0.5028161787272611\n",
      "0.5024971727820231\n",
      "0.5007488586690618\n",
      "0.5017377084593846\n",
      "0.50329693450104\n",
      "0.5043074321784372\n",
      "0.501461039511929\n",
      "0.501290952004709\n",
      "0.5012436262472786\n",
      "0.5011193865873098\n",
      "0.5011387889722484\n",
      "0.502972413859565\n",
      "0.5021997305291279\n",
      "0.5023979471869958\n",
      "0.5021735183151272\n",
      "0.5017844047904371\n",
      "0.5028559828006621\n",
      "0.5032143925113955\n",
      "0.5013712963680363\n",
      "0.502088217729633\n",
      "0.5032606175266054\n",
      "0.5010395494315787\n",
      "0.5022384528162268\n",
      "0.5031465350657592\n",
      "0.5040962880629168\n",
      "0.503894626207858\n",
      "0.5022891196644779\n",
      "0.5012267144840096\n",
      "0.5018654937446986\n",
      "0.5026883119502569\n",
      "0.5007782352699526\n",
      "0.5019278108771444\n",
      "0.5017970184705123\n",
      "0.5022556933703728\n",
      "0.5021935347880038\n",
      "0.5019066465557664\n",
      "0.5025442998067764\n",
      "0.5008564478287154\n",
      "0.5009371601504096\n",
      "0.5040845404662488\n",
      "0.5007396715491821\n",
      "0.5037261968431708\n",
      "0.5027471034643051\n",
      "0.5047800457729434\n",
      "0.5019781799936335\n",
      "0.5009085492316745\n",
      "0.5013929297544611\n",
      "0.5018451257234731\n",
      "0.5023464744290904\n",
      "0.5010530466816163\n",
      "0.5009835010732326\n",
      "0.5040232943347471\n",
      "0.5026980726665894\n",
      "0.5028275845921817\n",
      "0.5012524019633267\n",
      "0.5012173837746562\n",
      "0.5016842419916102\n",
      "0.5020338810478222\n",
      "0.5012078115366781\n",
      "0.5008564835313981\n",
      "0.5019721195353954\n",
      "0.5017897562401988\n",
      "0.5021182795965128\n",
      "0.5011280409409106\n",
      "0.502793294174252\n",
      "0.5044187524235761\n",
      "0.5015980035659393\n",
      "0.5012884619728734\n",
      "0.5024792813691243\n",
      "0.5008564020696945\n",
      "0.5019279098881497\n",
      "0.5025178340564275\n",
      "0.5031239884943035\n",
      "0.5016419732323198\n",
      "0.5058506100286035\n",
      "0.5021346948771049\n",
      "0.5010394998522449\n",
      "0.5018857886966859\n",
      "0.5009085742547452\n",
      "0.5018741739391278\n",
      "0.5014234083852046\n",
      "0.5017126934895678\n",
      "0.5031240714799755\n",
      "0.5041701867679692\n",
      "0.5016885130890018\n",
      "0.5042283258567716\n",
      "0.5020424702269447\n",
      "0.5011191972847396\n",
      "0.5034414041885329\n",
      "0.5032144567707818\n",
      "0.5022185793846705\n",
      "0.5026940346501313\n",
      "0.5020488693921507\n",
      "0.501959356625853\n",
      "0.5025402191911539\n",
      "0.5022993282613042\n",
      "0.5032152235121331\n",
      "0.5008443653971052\n",
      "0.5023037038984024\n",
      "0.50708777500362\n",
      "0.501516673922885\n",
      "0.5013672299466946\n",
      "0.5039432616087597\n",
      "0.503475402826995\n",
      "0.5012684491177789\n",
      "0.5036935351246952\n",
      "0.5013305809322312\n",
      "0.5018452010026979\n",
      "0.5010967975005732\n",
      "0.5040723905385404\n",
      "0.5017898176796571\n",
      "0.5011720350149054\n",
      "0.5017424057489184\n",
      "0.5022331839423416\n",
      "0.5013884345101289\n",
      "0.5013254530980444\n",
      "0.501024054969705\n",
      "0.502093584961405\n",
      "0.5008984850981674\n",
      "0.5013556261234497\n",
      "0.5044925980783407\n",
      "0.5016649614728553\n",
      "0.5016885066506491\n",
      "0.5033330304373134\n",
      "0.5021183252020064\n",
      "0.5012202493755077\n",
      "0.5011839559826572\n",
      "0.5011069375279654\n",
      "0.5035634928949615\n",
      "0.5012101452606111\n",
      "0.5013864903548936\n",
      "0.5029821977112561\n",
      "0.5023484776514117\n",
      "0.5018260280084723\n",
      "0.5023562157258049\n",
      "0.5019720475646222\n",
      "0.501106996567408\n",
      "0.5016420150652927\n",
      "0.502613840122267\n",
      "0.5033279153425266\n",
      "0.5008565063336838\n",
      "0.5019721411115162\n",
      "0.5022926735929161\n",
      "0.5023979992570651\n",
      "0.5017363026216642\n",
      "0.5012235923921613\n",
      "0.5012792579932626\n",
      "0.5020670459996422\n",
      "0.5011278980132506\n",
      "0.5012792622724956\n",
      "0.5033861750665536\n",
      "0.5022491433621115\n",
      "0.5009521794448148\n",
      "0.5012182998345565\n",
      "0.5023651427960284\n",
      "0.5154355737160038\n",
      "0.5022556467166149\n",
      "0.503268924575532\n",
      "0.5041306758160831\n",
      "0.5016197195448786\n",
      "0.5012020290531103\n",
      "0.5010769820092466\n",
      "0.501927856628895\n",
      "0.5042208334458294\n",
      "0.5019040990763959\n",
      "0.5014282878538162\n",
      "0.5022874269168579\n",
      "0.5023831550056881\n",
      "0.5011871194815767\n",
      "0.5040009470894404\n",
      "0.5025005909506851\n",
      "0.5020424936345035\n",
      "0.501664861335102\n",
      "0.5042718734629135\n",
      "0.5058736452186782\n",
      "0.504966444114565\n",
      "0.5031101637487243\n",
      "0.5013872951540246\n",
      "0.5011869980269462\n",
      "0.5000001592782177\n",
      "0.5009226930073809\n",
      "0.5013610806183879\n",
      "0.5020718230956891\n",
      "0.502305431551119\n",
      "0.50205346376855\n",
      "0.5019720126555121\n",
      "0.5036223446828857\n",
      "0.5022667783056903\n",
      "0.50201847490334\n",
      "0.5022926936669041\n",
      "0.5029422438951134\n",
      "0.5020728455292193\n",
      "0.5027821395097696\n",
      "0.5010000872432456\n",
      "0.5027685734633972\n",
      "0.502531367174628\n",
      "0.5033351076459771\n",
      "0.5026846792220462\n",
      "0.5007991517205316\n",
      "0.5011497760018496\n",
      "0.5025398434880629\n",
      "0.5011497393491668\n",
      "0.5042580709127553\n",
      "0.5015494685273745\n",
      "0.5043403388256069\n",
      "0.50355245550049\n",
      "0.5021735541867508\n",
      "0.5031801077741312\n",
      "0.5036143342080697\n",
      "0.5020935662339249\n",
      "0.5008210281905183\n",
      "0.502170417363784\n",
      "0.5013328432217956\n",
      "0.5036156532206398\n",
      "0.502193501474718\n",
      "0.501138780028888\n",
      "0.5016649718539008\n",
      "0.5011193379561674\n",
      "0.500996141343613\n",
      "0.5011723141494103\n",
      "0.502297359120804\n",
      "0.5018442857537884\n",
      "0.5011838662758565\n",
      "0.5035039716409706\n",
      "0.5008687721173222\n",
      "0.5015769652620692\n",
      "0.5007885012317841\n",
      "0.5019950221549874\n",
      "0.5040998061951023\n",
      "0.5016883560837863\n",
      "0.502304341191562\n",
      "0.5036517447468924\n",
      "0.5012350682656825\n",
      "0.5025651169269917\n",
      "0.5019950266155039\n",
      "0.5013404306584676\n",
      "0.5011723272905804\n",
      "0.5012437470610808\n",
      "0.5022400038705283\n",
      "0.5022781467345608\n",
      "0.5025325298622404\n",
      "0.5011171469174092\n",
      "0.5006885629781728\n",
      "0.5007131601052401\n",
      "0.5030635966098808\n",
      "0.5034156475342244\n",
      "0.5022517824737609\n",
      "0.5024121439219439\n",
      "0.502152128386735\n",
      "0.5027856109214448\n",
      "0.5035697012343402\n",
      "0.5025222616786394\n",
      "0.5010000631285746\n",
      "0.5009878930390348\n",
      "0.5032820349823752\n",
      "0.503055520363161\n",
      "0.5013466306025843\n",
      "0.5006581517277477\n",
      "0.5024932153928492\n",
      "0.5033845818486617\n",
      "0.5018653681267973\n",
      "0.5013713315956005\n",
      "0.502073320887518\n",
      "0.5030998571842885\n",
      "0.5012100555553161\n",
      "0.50215393493714\n",
      "0.5013736313878041\n",
      "0.5031334906417863\n",
      "0.5022027111588533\n",
      "0.5026307155537321\n",
      "0.5022771880604314\n",
      "0.500937143037839\n",
      "0.5018170074095196\n",
      "0.5013037947978771\n",
      "0.5019601525352995\n",
      "0.501583452947657\n",
      "0.5023083336512802\n",
      "0.5011839409067901\n",
      "0.5024985324304999\n",
      "0.5024296254996242\n",
      "0.5018069678799003\n",
      "0.5008817700490564\n",
      "0.5013885035380841\n",
      "0.5036728665218394\n",
      "0.5011496976831045\n",
      "0.5000001278871611\n",
      "0.5032816857034388\n",
      "0.5012685299795013\n",
      "0.5022145585895642\n",
      "0.502143500377123\n",
      "0.5044196841514434\n",
      "0.5024585441999566\n",
      "0.5022414262753795\n",
      "0.5030515960792842\n",
      "0.5018259952973455\n",
      "0.5044465584632488\n",
      "0.5047599925788127\n",
      "0.503120663694048\n",
      "0.5019426931680923\n",
      "0.5010486986009268\n",
      "0.5031733814383225\n",
      "0.5035396093771839\n",
      "0.5022290092371947\n",
      "0.5008442973454027\n",
      "0.5011553842098977\n",
      "0.5019085662226158\n",
      "0.503307157934765\n",
      "0.5012019947271595\n",
      "0.5014041367751998\n",
      "0.5008443398564746\n",
      "0.5020534904052617\n",
      "0.5027736244428113\n",
      "0.5029619039473731\n",
      "0.5019454460517622\n",
      "0.5021793914013436\n",
      "0.5013293938066101\n",
      "0.5020431718047169\n",
      "0.5082135153040646\n",
      "0.5019066361080272\n",
      "0.5037073938727397\n",
      "0.5012173697985017\n",
      "0.5010484089885086\n",
      "0.5020424789894835\n",
      "0.5018450958441233\n",
      "0.5011608761009817\n",
      "0.50257645083359\n",
      "0.5016648174492682\n",
      "0.5024379447357322\n",
      "0.5021157427032071\n",
      "0.5033711047048467\n",
      "0.5027627500594305\n",
      "0.5038221006707717\n",
      "0.5022556669609743\n",
      "0.5021182310494577\n",
      "0.5018618342530742\n",
      "0.5014193845712682\n",
      "0.5010770642655569\n",
      "0.5024394824180064\n",
      "0.5037459007772516\n",
      "0.5015979969993396\n",
      "0.5013329449500247\n",
      "0.5013927669342466\n",
      "0.5017424065732802\n",
      "0.5018260395557914\n",
      "0.5009835783881844\n",
      "0.5020900428427971\n",
      "0.501875246163226\n",
      "0.5012722215162108\n",
      "0.501195738228589\n",
      "0.5035649452173265\n",
      "0.5019040094098316\n",
      "0.5013712140934796\n",
      "0.5023043055586723\n",
      "0.501999759268796\n",
      "0.5024135846233095\n",
      "0.5010578870790416\n",
      "0.5020923583936828\n",
      "0.5023035449114215\n",
      "0.5008817515358495\n",
      "0.5013610795777632\n",
      "0.5021144008142141\n",
      "0.5023615896500391\n",
      "0.5039924006156808\n",
      "0.5017126693866734\n",
      "0.5011192607357844\n",
      "0.5048604409393795\n",
      "0.5037073697499749\n",
      "0.5013143620783791\n",
      "0.5021140244278742\n",
      "0.5037471738251507\n",
      "0.5021734764702179\n",
      "0.5020184692903389\n",
      "0.5014752625446404\n",
      "0.5064211295006884\n",
      "0.5017844761739797\n",
      "0.501712734463846\n",
      "0.5014475408991297\n",
      "0.5028033230006828\n",
      "0.5010394198191156\n",
      "0.5015230426867053\n",
      "0.5042023129439853\n",
      "0.5012350449067798\n",
      "0.5022290284666069\n",
      "0.5039943045713102\n",
      "0.5022991634834436\n",
      "0.5011860878611988\n",
      "0.5013719602480727\n",
      "0.5012523479308107\n",
      "0.503224494673014\n",
      "0.501688544938291\n",
      "0.5022138389787271\n",
      "0.5018741354303339\n",
      "0.5017633106423921\n",
      "0.5012850059221572\n",
      "0.5010770808764617\n",
      "0.5008098417700642\n",
      "0.5031944808699601\n",
      "0.5012588141623179\n",
      "0.5014193854917154\n",
      "0.5033477582570772\n",
      "0.501712616566703\n",
      "0.5013926443693556\n",
      "0.5035546405517395\n",
      "0.501479892026095\n",
      "0.5028033519027495\n",
      "0.5013801033053968\n",
      "0.5018168741332905\n",
      "0.5043471810112318\n",
      "0.5008687538083677\n",
      "0.5030032104958999\n",
      "0.5027675387355784\n",
      "0.5024036369888639\n",
      "0.5023969362141917\n",
      "0.5016842423895032\n",
      "0.5013255817683449\n",
      "0.502902282025295\n",
      "0.5036404011019282\n",
      "0.5018656325588092\n",
      "0.5011279896071513\n",
      "0.5025764628570013\n",
      "0.5020691947123133\n",
      "0.5033329907275396\n",
      "0.5010770419235054\n",
      "0.5000001394823984\n",
      "0.5016161009882955\n",
      "0.5028263540877161\n",
      "0.5019602914035742\n",
      "0.5033921855853555\n",
      "0.5012372840262213\n",
      "0.5021448399248059\n",
      "0.5009486242231489\n",
      "0.5027474560531923\n",
      "0.5050136134984727\n",
      "0.5031678528561382\n",
      "0.5025069793141037\n",
      "0.5023043305691923\n",
      "0.5020317195538162\n",
      "0.5016648958930074\n",
      "0.5011722574889087\n",
      "0.5026475836214078\n",
      "0.5000001215943795\n",
      "0.50128853687101\n",
      "0.5023675573692521\n",
      "0.5048152453868\n",
      "0.5012701605475004\n",
      "0.5020423842032378\n",
      "0.5022677943516235\n",
      "0.501243659896469\n",
      "0.501951313219125\n",
      "0.5083183949173753\n",
      "0.5025667874288878\n",
      "0.5032107910959107\n",
      "0.5032031369609353\n",
      "0.5012701413633643\n",
      "0.5019721022539132\n",
      "0.5033527002264417\n",
      "0.5023986179563422\n",
      "0.5010771433964347\n",
      "0.5011173715131785\n",
      "0.5017631735325958\n",
      "0.5011723159541552\n",
      "0.5024128930709054\n",
      "0.5018170079499795\n",
      "0.5022556690177952\n",
      "0.5010127990319964\n",
      "0.5010579266078284\n",
      "0.503267965782054\n",
      "0.5025446836031396\n",
      "0.5022517371873878\n",
      "0.5052865019738096\n",
      "0.5025743834447641\n",
      "0.5017633354952423\n",
      "0.5019278118491936\n",
      "0.5022197114313065\n",
      "0.5012702485962197\n",
      "0.5026063279143019\n",
      "0.5027820053351643\n",
      "0.5037760850289621\n",
      "0.5020424702930156\n",
      "0.5026063683695761\n",
      "0.5037729664370298\n",
      "0.5035340899008252\n",
      "0.5024155968935314\n",
      "0.5037712539963921\n",
      "0.5026594854605488\n",
      "0.501916687195146\n",
      "0.5022491998896739\n",
      "0.5029126783055538\n",
      "0.5012473202299917\n",
      "0.5015167681367156\n",
      "0.5015562730180484\n",
      "0.5010044969176991\n",
      "0.5023403280775308\n",
      "0.5030151042865854\n",
      "0.5022626701577317\n",
      "0.501330781879649\n",
      "0.5012457509054068\n",
      "0.502272164728804\n",
      "0.5035838055226787\n",
      "0.5025940416247906\n",
      "0.5016884307732787\n",
      "0.5023950443092942\n",
      "0.5011173364326909\n",
      "0.5014061158341462\n",
      "0.5014233398295215\n",
      "0.5013182371144705\n",
      "0.504540338368105\n",
      "0.5028580792297269\n",
      "0.502018413434311\n",
      "0.5017153866322193\n",
      "0.5031678427029599\n",
      "0.5040365447295165\n",
      "0.5012612898020166\n",
      "0.5018820520793695\n",
      "0.5035141242506007\n",
      "0.5024178231162998\n",
      "0.5011958076821302\n",
      "0.5018260185232474\n",
      "0.5012299237587765\n",
      "0.5048950527732448\n",
      "0.502207859638032\n",
      "0.5020489344553484\n",
      "0.5017376972839181\n",
      "0.5022001533267011\n",
      "0.5031679296989693\n",
      "0.5022001854793318\n",
      "0.5010579205481573\n",
      "0.501845483333574\n",
      "0.5031733796811132\n",
      "0.5027488562102979\n",
      "0.5014977317717872\n",
      "0.500956257338944\n",
      "0.5008817660679084\n",
      "0.502672965464508\n",
      "0.5020670868508533\n",
      "0.5011416251388107\n",
      "0.5007395885005713\n",
      "0.5018857535924313\n",
      "0.5009675719123835\n",
      "0.5020691845493037\n",
      "0.501127896165953\n",
      "0.504286835217801\n",
      "0.5026915059535833\n",
      "0.5014186145533617\n",
      "0.501048675326545\n",
      "0.502516906627513\n",
      "0.5034812669003846\n",
      "0.501865379592053\n",
      "0.50561034635272\n",
      "0.5013098998876201\n",
      "0.5052923871660775\n",
      "0.5013799824767597\n",
      "0.5010674658176159\n",
      "0.5014060768621188\n",
      "0.5020788548715984\n",
      "0.5035874141188331\n",
      "0.5022781464303414\n",
      "0.5017376683626988\n",
      "0.5030086249650025\n",
      "0.5022819558863774\n",
      "0.5028914136003226\n",
      "0.5021182700530232\n",
      "0.5026649470301046\n",
      "0.5010769578577018\n",
      "0.5058319954141821\n",
      "0.5022186267063657\n",
      "0.5020733786028767\n",
      "0.502168347615684\n",
      "0.5018443445980287\n",
      "0.5032107377760764\n",
      "0.5013182315107659\n",
      "0.5038100864574682\n",
      "0.5028448057692181\n",
      "0.501405680803068\n",
      "0.5051071605517342\n",
      "0.5042518818462832\n",
      "0.502066993296928\n",
      "0.5012884634799056\n",
      "0.5047730978056892\n",
      "0.5021586824602132\n",
      "0.5020423769438644\n",
      "0.5023951312727704\n",
      "0.5033196406804186\n",
      "0.5029382080965328\n",
      "0.502266713267001\n",
      "0.5012723548271787\n",
      "0.5036362058763234\n",
      "0.5014433638533106\n",
      "0.5026846756269752\n",
      "0.501443111951874\n",
      "0.5014817988432745\n",
      "0.5013038210644664\n",
      "0.5025222793163866\n",
      "0.5019254667421895\n",
      "0.5011496341840823\n",
      "0.5032760308086567\n",
      "0.5015361406242248\n",
      "0.5022819779633128\n",
      "0.5025693377037977\n",
      "0.5009085571447834\n",
      "0.5012456990732806\n",
      "0.5021793660073661\n",
      "0.5022345898174123\n",
      "0.5011386660067824\n",
      "0.5017153984350248\n",
      "0.5043357161841716\n",
      "0.504958413992683\n",
      "0.5028836935061844\n",
      "0.5031334970665947\n",
      "0.5020760901225443\n",
      "0.5008097946162605\n",
      "0.5013038026970884\n",
      "0.5012793583416764\n",
      "0.5033439962330968\n",
      "0.501949790693543\n",
      "0.5029706184425367\n",
      "0.5041055453889353\n",
      "0.5025953566561837\n",
      "0.5009372238748286\n",
      "0.5020890590368063\n",
      "0.5013791686202422\n",
      "0.501406119833207\n",
      "0.500919366534134\n",
      "0.5022171341932463\n",
      "0.5035259999482484\n",
      "0.5014193877510319\n",
      "0.503476090768013\n",
      "0.5020923224450912\n",
      "0.5009337242047712\n",
      "0.5010868245739395\n",
      "0.503401570462954\n",
      "0.5024251616653607\n",
      "0.5011279195331945\n",
      "0.5012828665181679\n",
      "0.5015056008464506\n",
      "0.5021435256233749\n",
      "0.5028263273720085\n",
      "0.5019667939293955\n",
      "0.5008325821817651\n",
      "0.5025194524151193\n",
      "0.5013953232493824\n",
      "0.5022345633197717\n",
      "0.5026173265552978\n",
      "0.5029259426149513\n",
      "0.5017845419997531\n",
      "0.5029251284496649\n",
      "0.504175362861056\n",
      "0.502378177093114\n",
      "0.5020609172842937\n",
      "0.5031180916500956\n",
      "0.5045043749145667\n",
      "0.5018068982789728\n",
      "0.5009264701138407\n",
      "0.501972118292053\n",
      "0.5008442471164951\n",
      "0.5010394658297839\n",
      "0.5012612146412122\n",
      "0.5021155958565684\n",
      "0.5012883040092623\n",
      "0.501071845892416\n",
      "0.5015563210897822\n",
      "0.5013737204031089\n",
      "0.5029724247255554\n",
      "0.5011173527401506\n",
      "0.5011386887893673\n",
      "0.5013072165407139\n",
      "0.5028328901917254\n",
      "0.502526110944873\n",
      "0.5019720803548421\n",
      "0.5030031959942125\n",
      "0.5010787608606774\n",
      "0.5010673975495438\n",
      "0.504999033772902\n",
      "0.5032030088263879\n",
      "0.502685255918121\n",
      "0.5019668293746247\n",
      "0.5021435167574193\n",
      "0.5018295507551301\n",
      "0.5044390425767818\n",
      "0.5011388052216578\n",
      "0.5022855023037219\n",
      "0.5013293814342819\n",
      "0.5007682576730371\n",
      "0.5020890435432322\n",
      "0.5028937979095571\n",
      "0.5012099265350389\n",
      "0.5046655606253003\n",
      "0.501906638851543\n",
      "0.5041097175393582\n",
      "0.5012967620366362\n",
      "0.5034834688725953\n",
      "0.504809562799537\n",
      "0.5011279342915006\n",
      "0.5027144231088925\n",
      "0.5015562362956086\n",
      "0.5019066272392716\n",
      "0.5034015735600721\n",
      "0.5048785133625568\n",
      "0.5007991536163612\n",
      "0.5021354649048482\n",
      "0.5025979374300514\n",
      "0.5008324294140049\n",
      "0.5051232188508797\n",
      "0.5028084979385052\n",
      "0.502097163120471\n",
      "0.5013190708183355\n",
      "0.5025402090690477\n",
      "0.5017126560066105\n",
      "0.5013752793871172\n",
      "0.5031455047634968\n",
      "0.5032606080843618\n",
      "0.501483751891217\n",
      "0.5034015130821605\n",
      "0.5032212338428427\n",
      "0.501057969760511\n",
      "0.50116951966439\n",
      "0.5012200608099664\n",
      "0.5010868089211999\n",
      "0.5019066611806091\n",
      "0.5022463927260122\n",
      "0.5016648908868201\n",
      "0.5032606721894705\n",
      "0.5008688112353221\n",
      "0.5027239532486859\n",
      "0.5047320619845503\n",
      "0.5023562369299969\n",
      "0.5022345617892184\n",
      "0.5010769645385094\n",
      "0.5021796019303325\n",
      "0.5025583168747548\n",
      "0.5032900686559276\n",
      "0.5019949535273485\n",
      "0.5027367771183411\n",
      "0.5024972465284514\n",
      "0.5034656836269652\n",
      "0.5022383512552024\n",
      "0.5013109418144401\n",
      "0.501297824664533\n",
      "0.5008817994039918\n",
      "0.5030078360823504\n",
      "0.5013673112115632\n",
      "0.5025222891936472\n",
      "0.5009085633049275\n",
      "0.5026235124195488\n",
      "0.5013182105994106\n",
      "0.500881741319087\n",
      "0.5012612626406163\n",
      "0.5013777581477958\n",
      "0.5028522719444052\n",
      "0.5019254988098666\n",
      "0.5021449183260247\n",
      "0.5020339647266141\n",
      "0.5019721012158537\n",
      "0.5022607440408593\n",
      "0.5012999297124817\n",
      "0.5064956336276571\n",
      "0.5023104386666402\n",
      "0.5013073740338235\n",
      "0.5014330952929076\n",
      "0.5033233682351274\n",
      "0.5058080779728945\n",
      "0.5027972020471543\n",
      "0.5013109912184931\n",
      "0.5008443979954368\n",
      "0.5041618706858666\n",
      "0.5018857041049095\n",
      "0.5014486977110935\n",
      "0.5012181494423703\n",
      "0.5015980206656362\n",
      "0.5009371599853096\n",
      "0.5013800340472071\n",
      "0.5011722298443724\n",
      "0.5012079530963266\n",
      "0.5054119290817235\n",
      "0.5018820288653658\n",
      "0.5022516826607338\n",
      "0.5045550898593619\n",
      "0.502340239915198\n",
      "0.5027824269973976\n",
      "0.5057075234097752\n",
      "0.5017346694436591\n",
      "0.5021836766355381\n",
      "0.5024379257068574\n",
      "0.5034573640630133\n",
      "0.5024379541292513\n",
      "0.504207083969779\n",
      "0.5011608512441111\n",
      "0.5007884852427694\n",
      "0.5027675495798243\n",
      "0.5014034559453512\n",
      "0.5030623759903807\n",
      "0.5021656999399876\n",
      "0.5033517428615077\n",
      "0.5024972917562827\n",
      "0.5028161950434642\n",
      "0.5033915441832325\n",
      "0.5022607968708968\n",
      "0.504972303741695\n",
      "0.5029501715451374\n",
      "0.5021448442233517\n",
      "0.5000001790687727\n",
      "0.5020423420348435\n",
      "0.5017633839371805\n",
      "0.5017633642525602\n",
      "0.5016884433938601\n",
      "0.502038446457967\n",
      "0.5030007268451631\n",
      "0.5024379397681831\n",
      "0.5014791660517932\n",
      "0.5020184060291597\n",
      "0.5016420235674863\n",
      "0.5029242977953984\n",
      "0.5023592251922112\n",
      "0.5005812331862824\n",
      "0.501642013095603\n",
      "0.5022368339723933\n",
      "0.5029030189589923\n",
      "0.5010868770148055\n",
      "0.5012351714875075\n",
      "0.5009675588722573\n",
      "0.5015768565441447\n",
      "0.5013255110508831\n",
      "0.5029259271995906\n",
      "0.5051329732649026\n",
      "0.5014400796559306\n",
      "0.5000000865568712\n",
      "0.501232895693498\n",
      "0.5017701612237399\n",
      "0.5012183463011893\n",
      "0.5013958435623688\n",
      "0.5098015357574899\n",
      "0.5018259970470822\n",
      "0.5020184191240581\n",
      "0.5030007305045925\n",
      "0.5020424536360704\n",
      "0.5019427373524883\n",
      "0.5012588638484223\n",
      "0.5010967165745195\n",
      "0.5047596544258228\n",
      "0.5032211700952874\n",
      "0.5024834335770384\n",
      "0.5033601084130473\n",
      "0.5007885608856456\n",
      "0.5022504099695906\n",
      "0.5007885187338447\n",
      "0.5026006389256349\n",
      "0.5022667147196945\n",
      "0.5025205264035032\n",
      "0.5018170309804572\n",
      "0.5013634308047342\n",
      "0.5014023007330494\n",
      "0.5008950274703752\n",
      "0.5012351286944995\n",
      "0.50280470373349\n",
      "0.5056605243960615\n",
      "0.5011174510777181\n",
      "0.5048498555317272\n",
      "0.5023527743149252\n",
      "0.5027234686781328\n",
      "0.5028131969991333\n",
      "0.5020183725118671\n",
      "0.5042339398044398\n",
      "0.5012437481650592\n",
      "0.5020670404035941\n",
      "0.5022345939331985\n",
      "0.5021057220035239\n",
      "0.5032286186372645\n",
      "0.5019721219826807\n",
      "0.5010000494222083\n",
      "0.5045125067950469\n",
      "0.5017561292905381\n",
      "0.5026236751054672\n",
      "0.5018169953246885\n",
      "0.5020923669181879\n",
      "0.5027808498690972\n",
      "0.5013953940830376\n",
      "0.5016197030629924\n",
      "0.5021355794778354\n",
      "0.5022621282002935\n",
      "0.5032110871669799\n",
      "0.5012967136553215\n",
      "0.5028558134657626\n",
      "0.5019949302701294\n",
      "0.5022137794752946\n",
      "0.5033740532026626\n",
      "0.5023444017713349\n",
      "0.5032475778294634\n",
      "0.5035540120949974\n",
      "0.5026979800859347\n",
      "0.5011279427930474\n",
      "0.5009877466477296\n",
      "0.502231700193643\n",
      "0.5015563667569373\n",
      "0.5039414127063854\n",
      "0.5014791917110681\n",
      "0.5017347075565329\n",
      "0.5023215919365498\n",
      "0.5026063510578765\n",
      "0.5042868235694208\n",
      "0.5017898332860049\n",
      "0.5036603512210321\n",
      "0.5021244250767793\n",
      "0.5034090694176991\n",
      "0.5053070492725351\n",
      "0.5039726707293907\n",
      "0.5029445918854527\n",
      "0.501418921845629\n",
      "0.5025019531428235\n",
      "0.5013733821155921\n",
      "0.5013556760885409\n",
      "0.5009486170254622\n",
      "0.5012701157974214\n",
      "0.5018454214546164\n",
      "0.5012328746357365\n",
      "0.501195806943305\n",
      "0.501077067002392\n",
      "0.5019907351761175\n",
      "0.5022346091552911\n",
      "0.50196683162337\n",
      "0.503243939845083\n",
      "0.5013266994363347\n",
      "0.5019721084288556\n",
      "0.501380042991285\n",
      "0.501652249094282\n",
      "0.5031793587461709\n",
      "0.5033693767286581\n",
      "0.5011069561263012\n",
      "0.502816255248572\n",
      "0.5036889155023162\n",
      "0.5022162347003833\n",
      "0.502472246186527\n",
      "0.5018069289410183\n",
      "0.5034018680754446\n",
      "0.5012437577911332\n",
      "0.5027556498055566\n",
      "0.502212913801348\n",
      "0.5011838776812912\n",
      "0.5019963876137683\n",
      "0.5008818387217826\n",
      "0.5020184044363593\n",
      "0.503029656222756\n",
      "0.5025222698255246\n",
      "0.5020671340462519\n",
      "0.5041800001016778\n",
      "0.5025222470300628\n",
      "0.5017376660690144\n",
      "0.5018068915109595\n",
      "0.502344316353108\n",
      "0.5027071450536924\n",
      "0.5020787544741746\n",
      "0.5035298915065644\n",
      "0.5014476112630716\n",
      "0.501183926865645\n",
      "0.5018741166786753\n",
      "0.5020737991773994\n",
      "0.5039418002118761\n",
      "0.5011387853161807\n",
      "0.5013953896732656\n",
      "0.5095059476597297\n",
      "0.5020414351538126\n",
      "0.5020732965108605\n",
      "0.5012437058946623\n",
      "0.5023979451067605\n",
      "0.5058912260015984\n",
      "0.501556356141027\n",
      "0.5012050224124028\n",
      "0.5014014860250555\n",
      "0.5011278995142969\n",
      "0.5026336058182922\n",
      "0.5018857701390204\n",
      "0.5025326468761988\n",
      "0.5027396947015065\n",
      "0.5032105284541837\n",
      "0.5021735132395927\n",
      "0.5011839328605404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5010232586978476\n",
      "0.5030646076716939\n",
      "0.5020150653318003\n",
      "0.5021721453222513\n",
      "0.5015979183278917\n",
      "0.5031180898489944\n",
      "0.5026709861386518\n",
      "0.5009336759078279\n",
      "0.5011070467161444\n",
      "0.504520219535903\n",
      "0.5023002228159358\n",
      "0.5010868450175897\n",
      "0.5010914727188701\n",
      "0.5011497048676945\n",
      "0.5019602698119329\n",
      "0.5020534463818692\n",
      "0.5000001497370264\n",
      "0.503690081723748\n",
      "0.5016195910425919\n",
      "0.5011957936818527\n",
      "0.502208275484515\n",
      "0.5031034387106187\n",
      "0.5026710340989913\n",
      "0.5017175194597259\n",
      "0.502304279419988\n",
      "0.5021517727614259\n",
      "0.5039738744576708\n",
      "0.5022735567528769\n",
      "0.5056871851565701\n",
      "0.5010045369141157\n",
      "0.5014174784810735\n",
      "0.5008564891541561\n",
      "0.5030817902011051\n",
      "0.5033138176180166\n",
      "0.5016647913819269\n",
      "0.5008564477439966\n",
      "0.5030123870973264\n",
      "0.5024710917789585\n",
      "0.5020671051877509\n",
      "0.5014360012181617\n",
      "0.5041307154105946\n",
      "0.5023739137112718\n",
      "0.5008211143594072\n",
      "0.5022586656651021\n",
      "0.5023403101912489\n",
      "0.5032820155188068\n",
      "0.5025690197935536\n",
      "0.5016214051294957\n",
      "0.5012182985890042\n",
      "0.5026063901169324\n",
      "0.5015768823157866\n",
      "0.513480330604122\n",
      "0.5012202198028693\n",
      "0.502320372854926\n",
      "0.502321532001302\n",
      "0.502067170158757\n",
      "0.5049660584343205\n",
      "0.5012183519884672\n",
      "0.5022161176679574\n",
      "0.5027735064634835\n",
      "0.5018170817157585\n",
      "0.5011361807127837\n",
      "0.5013929412315374\n",
      "0.5035114982385689\n",
      "0.5011262904503408\n",
      "0.5013548903770304\n",
      "0.5011939718041959\n",
      "0.5024652175773255\n",
      "0.5029617701371636\n",
      "0.5023979357755253\n",
      "0.5022491935604293\n",
      "0.5016884669690403\n",
      "0.5016883962583175\n",
      "0.502644778500474\n",
      "0.5021613568254616\n",
      "0.5010129715491876\n",
      "0.5014261083802485\n",
      "0.5018259590876197\n",
      "0.5063064548454936\n",
      "0.5015979849623319\n",
      "0.5022223042755021\n",
      "0.5018069324240078\n",
      "0.5047367613361953\n",
      "0.5010868141688549\n",
      "0.50127227366212\n",
      "0.5015359236474106\n",
      "0.5025069660730134\n",
      "0.5008689454459683\n",
      "0.5027488189609155\n",
      "0.5006653979510755\n",
      "0.5028132877951651\n",
      "0.5018454555760249\n",
      "0.5021056586611369\n",
      "0.5012701582608571\n",
      "0.5012235267509929\n",
      "0.5021721507775239\n",
      "0.5019040507645526\n",
      "0.50139137475086\n",
      "0.5009372081856428\n",
      "0.5013611967639543\n",
      "0.502883722314955\n",
      "0.504201628476597\n",
      "0.5035921520567449\n",
      "0.5037711787079474\n",
      "0.5022973741472713\n",
      "0.5008325934035451\n",
      "0.5027821050464679\n",
      "0.5018442400925539\n",
      "0.5025370100166903\n",
      "0.5030635843771398\n",
      "0.5010345581676677\n",
      "0.5024280515707048\n",
      "0.5010305431818535\n",
      "0.5012265909622275\n",
      "0.5034447149823097\n",
      "0.5030347224597014\n",
      "0.5013072736221079\n",
      "0.5030818326536296\n",
      "0.501393416548829\n",
      "0.5013480032552706\n",
      "0.5017125743152493\n",
      "0.5009054117182996\n",
      "0.5011471300291757\n",
      "0.504367161838337\n",
      "0.5016420106723115\n",
      "0.5024651961750403\n",
      "0.5030526856707536\n",
      "0.5013329406839698\n",
      "0.5018068868124576\n",
      "0.5017897143550034\n",
      "0.5009193588336965\n",
      "0.5020798596802971\n",
      "0.5013110418047046\n",
      "0.5024628846320337\n",
      "0.5019310312513129\n",
      "0.502355313747086\n",
      "0.502193532174222\n",
      "0.5009521762449732\n",
      "0.5016064519920663\n",
      "0.5012690722231764\n",
      "0.5018455031662888\n",
      "0.5022258272556883\n",
      "0.502303742435895\n",
      "0.5012827537613157\n",
      "0.5023969328913046\n",
      "0.5013556434621471\n",
      "0.5024825231814082\n",
      "0.5005646494122665\n",
      "0.5023215564997872\n",
      "0.5019132347716942\n",
      "0.502289102450595\n",
      "0.5018654091822017\n",
      "0.50228555848195\n",
      "0.5023718494738589\n",
      "0.5037438085426214\n",
      "0.5033238771553127\n",
      "0.5008099149504379\n",
      "0.5014788737869461\n",
      "0.5023407714193149\n",
      "0.5019278792418974\n",
      "0.5036287553354907\n",
      "0.5016278282980448\n",
      "0.5032759483119442\n",
      "0.5013073287034884\n",
      "0.5020534549687083\n",
      "0.5007991538044929\n",
      "0.5013267372084158\n",
      "0.5027820660530595\n",
      "0.5035873189373573\n",
      "0.5019254835764937\n",
      "0.501471321085898\n",
      "0.5029300878734696\n",
      "0.5014520722598557\n",
      "0.5025932283435715\n",
      "0.5020338288490972\n",
      "0.5017882785503187\n",
      "0.501252516184936\n",
      "0.501763374890586\n",
      "0.5018920830174208\n",
      "0.5007682697679648\n",
      "0.5019661190091024\n",
      "0.501966797429997\n",
      "0.5019949572064395\n",
      "0.5011070762356445\n",
      "0.5024036041041855\n",
      "0.5030866370199963\n",
      "0.5024729781853357\n",
      "0.5079916650515656\n",
      "0.501119282808178\n",
      "0.5012913169060723\n",
      "0.5049008559708514\n",
      "0.5013477413047741\n",
      "0.5029724166220856\n",
      "0.5031535573056175\n",
      "0.503180107044078\n",
      "0.5029500826137132\n",
      "0.5022597824131094\n",
      "0.5025324459084953\n",
      "0.5028275820412977\n",
      "0.5013609619098425\n",
      "0.502606294795078\n",
      "0.5060666217858341\n",
      "0.5024402889787375\n",
      "0.5030130610721469\n",
      "0.5024295006675673\n",
      "0.503167893228207\n",
      "0.5018653550568374\n",
      "0.5030918871931077\n",
      "0.5045519316411761\n",
      "0.5029033918760287\n",
      "0.500881745957339\n",
      "0.5039845597146816\n",
      "0.5023505698800005\n",
      "0.5011280483419213\n",
      "0.5013198542421919\n",
      "0.501361073379247\n",
      "0.5011279807998197\n",
      "0.5034812614137557\n",
      "0.50094102671383\n",
      "0.5009520043282443\n",
      "0.5022587001630414\n",
      "0.5008816920818867\n",
      "0.5010786909261629\n",
      "0.5017345790002207\n",
      "0.5041337678621404\n",
      "0.5020184094698282\n",
      "0.5017376597725104\n",
      "0.5021121821150071\n",
      "0.5024942577794027\n",
      "0.5022556134156273\n",
      "0.5016649314777618\n",
      "0.5033627101921795\n",
      "0.5013488596701775\n",
      "0.5012612119492553\n",
      "0.5012019621571006\n",
      "0.5022290821907649\n",
      "0.5012612840158642\n",
      "0.5022001880971257\n",
      "0.5027255185755666\n",
      "0.5036987013383868\n",
      "0.5010217165563006\n",
      "0.5031357874226039\n",
      "0.502473096219031\n",
      "0.5035792949796227\n",
      "0.5018454361045773\n",
      "0.5039993096814447\n",
      "0.5042213432403182\n",
      "0.5016648676670405\n",
      "0.5060085544432664\n",
      "0.502942246665789\n",
      "0.5010216861081624\n",
      "0.5019920993117643\n",
      "0.5022557552876967\n",
      "0.5048246803634078\n",
      "0.5033440027116908\n",
      "0.5006236430916089\n",
      "0.5000001149385119\n",
      "0.5021734742416124\n",
      "0.5017699509628117\n",
      "0.5009224492802292\n",
      "0.5006237385689541\n",
      "0.5023205237159173\n",
      "0.5018455190580466\n",
      "0.5026877387310115\n",
      "0.5008949618403993\n",
      "0.5016884209893606\n",
      "0.5032584234667082\n",
      "0.505349655814749\n",
      "0.504681846419577\n",
      "0.5031423010768341\n",
      "0.5019963058509964\n",
      "0.5040828196249573\n",
      "0.5014964028165845\n",
      "0.5011870275991354\n",
      "0.5011870575685873\n",
      "0.5050964947207228\n",
      "0.5012792554539475\n",
      "0.5011838452974468\n",
      "0.501927945982673\n",
      "0.5034750503102606\n",
      "0.5009371426793333\n",
      "0.5008817496737638\n",
      "0.5052768921456832\n",
      "0.5040344654283293\n",
      "0.5012234785835091\n",
      "0.5013479956329284\n",
      "0.5013863682145004\n",
      "0.5042868156749818\n",
      "0.501086614281048\n",
      "0.5008816980068798\n",
      "0.5024296356996955\n",
      "0.5023242682061373\n",
      "0.500748923382401\n",
      "0.5018260644714154\n",
      "0.5019957449762651\n",
      "0.5019039020414495\n",
      "0.5041752605601044\n",
      "0.5013072971770185\n",
      "0.5012897778111483\n",
      "0.5031678348394975\n",
      "0.5021657317015394\n",
      "0.5028559198435342\n",
      "0.5019949354584885\n",
      "0.5039471782200109\n",
      "0.502078849733156\n",
      "0.5011070148445709\n",
      "0.5018654730936855\n",
      "0.5046226064931921\n",
      "0.5008325331002432\n",
      "0.501927921609598\n",
      "0.5012109128181433\n",
      "0.5022399717429084\n",
      "0.5026191223089773\n",
      "0.5087439821230931\n",
      "0.5012079375080908\n",
      "0.5035264514790041\n",
      "0.5029126408871767\n",
      "0.5011957948196767\n",
      "0.5013365935733571\n",
      "0.5021057162212573\n",
      "0.5012100433310349\n",
      "0.5011496372347766\n",
      "0.5024628663629777\n",
      "0.5058935200861342\n",
      "0.5025689616193818\n",
      "0.5018654135188803\n",
      "0.5017345884100289\n",
      "0.500983530564719\n",
      "0.5024472405713245\n",
      "0.5041338923111071\n",
      "0.5019497312278489\n",
      "0.5020936345980144\n",
      "0.5013654178689821\n",
      "0.5021182552152794\n",
      "0.5032266406592224\n",
      "0.5024871848780351\n",
      "0.5016419926167546\n",
      "0.5037438116997947\n",
      "0.5035263714630471\n",
      "0.5020340413308143\n",
      "0.5017127224429282\n",
      "0.5028800852478039\n",
      "0.5019560121583894\n",
      "0.503273937534004\n",
      "0.5012700241309908\n",
      "0.5034821813742614\n",
      "0.5033671259553383\n",
      "0.5012624019792422\n",
      "0.5023784342079199\n",
      "0.5012523949808723\n",
      "0.5030031237647334\n",
      "0.5017882664660354\n",
      "0.5020686933223357\n",
      "0.5026417929820312\n",
      "0.5022772832297213\n",
      "0.5006803923833666\n",
      "0.5006805566515063\n",
      "0.5011174139389603\n",
      "0.5020149932971887\n",
      "0.5026896567900128\n",
      "0.5019906777319422\n",
      "0.5036392195470717\n",
      "0.5014684458284675\n",
      "0.5030113427785349\n",
      "0.5022138154378861\n",
      "0.5008098641486592\n",
      "0.5054671456800915\n",
      "0.502212938565455\n",
      "0.5010674078955851\n",
      "0.5010486822154352\n",
      "0.5027676114582679\n",
      "0.5019498208162056\n",
      "0.5000001404813246\n",
      "0.5024651414370105\n",
      "0.5019096727490686\n",
      "0.5036843031565477\n",
      "0.5023452124844741\n",
      "0.5008950045403647\n",
      "0.5027422636796797\n",
      "0.5023592414992938\n",
      "0.5011279688243402\n",
      "0.5009410587573977\n",
      "0.5020425102027352\n",
      "0.5013585387026156\n",
      "0.5014891090192937\n",
      "0.5044197854488689\n",
      "0.5024885665925046\n",
      "0.5024529916916343\n",
      "0.5016884240751928\n",
      "0.5011836662842843\n",
      "0.5018170699157424\n",
      "0.5012690776778856\n",
      "0.503167614230825\n",
      "0.5019721125038977\n",
      "0.5020183813544358\n",
      "0.5010770931788896\n",
      "0.5010395024245266\n",
      "0.5021969736498281\n",
      "0.5026845142603942\n",
      "0.505145548150388\n",
      "0.5012977341481387\n",
      "0.5013255488661267\n",
      "0.5012351013745321\n",
      "0.5008209571656832\n",
      "0.5013072476320829\n",
      "0.5030759182805833\n",
      "0.5020671245457999\n",
      "0.5027960160955602\n",
      "0.5023950645748902\n",
      "0.5034093657314717\n",
      "0.5020150423798114\n",
      "0.5036396869013596\n",
      "0.5034775888148225\n",
      "0.5018855517106268\n",
      "0.5019349678503113\n",
      "0.5017950691639099\n",
      "0.5017375893635391\n",
      "0.5026448688123887\n",
      "0.5015358803941269\n",
      "0.503201796613193\n",
      "0.5012019451383066\n",
      "0.5032152371647828\n",
      "0.5043357993868189\n",
      "0.5016884565591245\n",
      "0.5012437166077254\n",
      "0.5022523740363468\n",
      "0.5016885491560504\n",
      "0.5012624092593526\n",
      "0.5033138142282906\n",
      "0.5018518490822772\n",
      "0.5034955556902025\n",
      "0.5041089121130924\n",
      "0.5017633286095968\n",
      "0.5008210438568208\n",
      "0.5020062605380429\n",
      "0.5014266901343524\n",
      "0.5032382106150305\n",
      "0.5013109310185218\n",
      "0.502172194897615\n",
      "0.5023720272463083\n",
      "0.5020184335991339\n",
      "0.5012578386398859\n",
      "0.501927899023312\n",
      "0.5019066327704946\n",
      "0.5025325585143848\n",
      "0.5033581297064914\n",
      "0.5025246406227211\n",
      "0.5014903615117557\n",
      "0.5021935444707003\n",
      "0.5028398316880378\n",
      "0.5011722534309\n",
      "0.5013871237460258\n",
      "0.5013072536052451\n",
      "0.5029240031948052\n",
      "0.5012702466709046\n",
      "0.5021156643725945\n",
      "0.5017561449378465\n",
      "0.501949741282091\n",
      "0.5023034895851043\n",
      "0.505865971567991\n",
      "0.5023130562667265\n",
      "0.5013849786361336\n",
      "0.5012362716702543\n",
      "0.5021355363816792\n",
      "0.5016841261980172\n",
      "0.5013767207307138\n",
      "0.5012691316097744\n",
      "0.5013712136568399\n",
      "0.5011172452766454\n",
      "0.5020934278332375\n",
      "0.5036602954111133\n",
      "0.5011781748373785\n",
      "0.5019997824601616\n",
      "0.5020150478319677\n",
      "0.5031034267140936\n",
      "0.5036440177832767\n",
      "0.503120624895589\n",
      "0.501762409989466\n",
      "0.5023131197199258\n",
      "0.5009225904884582\n",
      "0.5013694335997361\n",
      "0.5043739726793742\n",
      "0.5021537994170873\n",
      "0.5009520587665931\n",
      "0.5038556083884641\n",
      "0.5026235069358381\n",
      "0.5012792612365419\n",
      "0.5018170892158895\n",
      "0.503275963051537\n",
      "0.5021971409786942\n",
      "0.5021793921755418\n",
      "0.5033536351185416\n",
      "0.5028870936589047\n",
      "0.5010216829472676\n",
      "0.5009561612973426\n",
      "0.5049870788102396\n",
      "0.5031127216470478\n",
      "0.5011070111384718\n",
      "0.5018857697285891\n",
      "0.5037073961254906\n",
      "0.5015167492277027\n",
      "0.5045700576057185\n",
      "0.5012351704217308\n",
      "0.5023214826780765\n",
      "0.5019434396796659\n",
      "0.5013267702024325\n",
      "0.502676837766354\n",
      "0.5016214272953453\n",
      "0.5016647536844806\n",
      "0.5013329165480914\n",
      "0.501818569029395\n",
      "0.50430326857766\n",
      "0.5021538797901305\n",
      "0.5010845581707051\n",
      "0.5031103975216724\n",
      "0.50246544576379\n",
      "0.5010579213460697\n",
      "0.5030608223504467\n",
      "0.5023218382143997\n",
      "0.5019602151079111\n",
      "0.5015362679677391\n",
      "0.5028329275518013\n",
      "0.5027488165899114\n",
      "0.5019775143712413\n",
      "0.5027254754689027\n",
      "0.5028941347841298\n",
      "0.50090860062621\n",
      "0.5008325692736503\n",
      "0.5040024172916479\n",
      "0.501621392183341\n",
      "0.5032606584742034\n",
      "0.5015031356744254\n",
      "0.5020424871991425\n",
      "0.5040421371440887\n",
      "0.5012700176395395\n",
      "0.5034921751234173\n",
      "0.5010853505556643\n",
      "0.5025939613949918\n",
      "0.5013000097141961\n",
      "0.5036603057015069\n",
      "0.5019310225202973\n",
      "0.5018069581970763\n",
      "0.503653759706366\n",
      "0.5017632158042596\n",
      "0.5024155298711278\n",
      "0.5020044056319694\n",
      "0.5028236963447592\n",
      "0.5026985044291814\n",
      "0.5012099280978414\n",
      "0.5020691693705986\n",
      "0.5019948411044555\n",
      "0.5026845130250931\n",
      "0.5019755493992188\n",
      "0.5019950110406084\n",
      "0.5010485132320591\n",
      "0.5027726888611099\n",
      "0.502647654072443\n",
      "0.5020182668345449\n",
      "0.5044949384409813\n",
      "0.5015774859306512\n",
      "0.5009521758646535\n",
      "0.5010868287667686\n",
      "0.5020184642689945\n",
      "0.5024538835270617\n",
      "0.5000002292066307\n",
      "0.5023876757753013\n",
      "0.5014609934704837\n",
      "0.5029878962371273\n",
      "0.5019426924240206\n",
      "0.5028837474580313\n",
      "0.5033514941549901\n",
      "0.50219369307728\n",
      "0.5011620610300271\n",
      "0.501845482825285\n",
      "0.5016680505102321\n",
      "0.5012833132420967\n",
      "0.5022464270307676\n",
      "0.5006508289713438\n",
      "0.5025326508639413\n",
      "0.501737582452152\n",
      "0.503009128169857\n",
      "0.501234269344368\n",
      "0.5022021395773215\n",
      "0.5012327773745158\n",
      "0.501048610978034\n",
      "0.5033846311506979\n",
      "0.5012884676775018\n",
      "0.503299304766007\n",
      "0.5025006337233776\n",
      "0.5033066321885475\n",
      "0.500891805130756\n",
      "0.5019497593356618\n",
      "0.5020424542606464\n",
      "0.5022222030369284\n",
      "0.501172264975421\n",
      "0.5019039560126102\n",
      "0.5006884245299013\n",
      "0.5033681390655201\n",
      "0.5024295969292607\n",
      "0.5027204193226361\n",
      "0.5022332957162223\n",
      "0.5024123135068651\n",
      "0.5025207818463532\n",
      "0.5024911718895068\n",
      "0.502216347366693\n",
      "0.5032031250400717\n",
      "0.5021427059058675\n",
      "0.5041799936708596\n",
      "0.5030347700521464\n",
      "0.5016648512303618\n",
      "0.5017990540560918\n",
      "0.5018454452554777\n",
      "0.500788613279036\n",
      "0.5018451147087463\n",
      "0.5012351668085924\n",
      "0.5021448884498645\n",
      "0.5034955671087684\n",
      "0.5018969077026845\n",
      "0.5021572531660382\n",
      "0.5017444796778298\n",
      "0.5019497662504248\n",
      "0.5008689675723274\n",
      "0.5007883789903386\n",
      "0.5016196879785358\n",
      "0.5010306153153288\n",
      "0.5029518232337671\n",
      "0.5020423592916106\n",
      "0.501324888819217\n",
      "0.5014228965151447\n",
      "0.5026149320271243\n",
      "0.5017882227609436\n",
      "0.5021970687081051\n",
      "0.5035496952643689\n",
      "0.5015768982485036\n",
      "0.5018672691283887\n",
      "0.5030406271563328\n",
      "0.502185368619969\n",
      "0.5024401696104263\n",
      "0.5013109320306618\n",
      "0.5049460824741601\n",
      "0.5023913924940278\n",
      "0.5016214477027879\n",
      "0.5038498362364142\n",
      "0.5012299880356573\n",
      "0.5022726421067698\n",
      "0.5025223096941619\n",
      "0.5016679846424461\n",
      "0.501642006891715\n",
      "0.5010043572622851\n",
      "0.5038624848729353\n",
      "0.5014408625815662\n",
      "0.5031334080811559\n",
      "0.5013169333289541\n",
      "0.5022735239381544\n",
      "0.5031535592410539\n",
      "0.5014145253643317\n",
      "0.5008723054770691\n",
      "0.5021386189845851\n",
      "0.5009639450141269\n",
      "0.5028204830190641\n",
      "0.5021539423905177\n",
      "0.5018527430850203\n",
      "0.5029693150711427\n",
      "0.5020423788785845\n",
      "0.5011859429914756\n",
      "0.5021143370135555\n",
      "0.5016197630364281\n",
      "0.5018857705550493\n",
      "0.5016419069113089\n",
      "0.5022001571634153\n",
      "0.5030920953794337\n",
      "0.5025325867233196\n",
      "0.5016197264906325\n",
      "0.5025094964316404\n",
      "0.5026536879690874\n",
      "0.5011069940116026\n",
      "0.5013993242804714\n",
      "0.5027487541112409\n",
      "0.5016649997263923\n",
      "0.501021777097035\n",
      "0.5025435270971912\n",
      "0.5028815497896404\n",
      "0.5014610969789717\n",
      "0.5022491514938888\n",
      "0.5044111444098475\n",
      "0.5011554222660694\n",
      "0.5036338133992916\n",
      "0.5023182620377307\n",
      "0.5009411745567983\n",
      "0.5016419815113273\n",
      "0.5025046804469883\n",
      "0.5030721082846507\n",
      "0.5037247694604323\n",
      "0.5015362866076853\n",
      "0.5018069753054222\n",
      "0.5015979597527013\n",
      "0.5026984803950828\n",
      "0.5029172686427036\n",
      "0.5016884934491929\n",
      "0.5081956208114788\n",
      "0.5022078835850395\n",
      "0.5015362897634453\n",
      "0.5055444879588201\n",
      "0.5023979565732651\n",
      "0.5019309792370434\n",
      "0.501949741225473\n",
      "0.500809876672439\n",
      "0.5015139915972349\n",
      "0.5031334650820726\n",
      "0.5039773677422543\n",
      "0.5023254693589055\n",
      "0.5022021280574892\n",
      "0.5011548636932923\n",
      "0.5019039600949273\n",
      "0.5035420306368913\n",
      "0.5018654033722928\n",
      "0.5025094506306689\n",
      "0.5077735406995999\n",
      "0.501789752967085\n",
      "0.501279240097453\n",
      "0.501395260882889\n",
      "0.5024730554806689\n",
      "0.5019949853392948\n",
      "0.5028559115236337\n",
      "0.5037742715772984\n",
      "0.5025370128362593\n",
      "0.5024036538122265\n",
      "0.5021572224704358\n",
      "0.5021448073792458\n",
      "0.502725397455095\n",
      "0.5049321981003579\n",
      "0.5013017051715344\n",
      "0.5018451922136055\n",
      "0.5011496808545955\n",
      "0.5073851301153299\n",
      "0.5022138771632112\n",
      "0.5011070425310252\n",
      "0.5032144548762899\n",
      "0.5010868584979599\n",
      "0.5025226126626146\n",
      "0.5034078492954956\n",
      "0.5025094587395665\n",
      "0.5034420485966479\n",
      "0.5010722795956909\n",
      "0.5012691261905514\n",
      "0.5013329548062949\n",
      "0.5012897504326701\n",
      "0.5033071228894335\n",
      "0.5021426767369704\n",
      "0.502079945325143\n",
      "0.503060965481013\n",
      "0.5023034522483794\n",
      "0.5013809829055524\n",
      "0.5053014013616105\n",
      "0.5036035475662076\n",
      "0.5039072524068605\n",
      "0.5017633623589982\n",
      "0.5016640537872595\n",
      "0.5018069100708928\n",
      "0.5027122574504915\n",
      "0.5013929084611367\n",
      "0.5013634024436232\n",
      "0.5008815425642053\n",
      "0.5022020957741024\n",
      "0.5018105205623828\n",
      "0.5012827295341148\n",
      "0.501437038410568\n",
      "0.5017633525386079\n",
      "0.5022586760214143\n",
      "0.5015001714947348\n",
      "0.5011860246094803\n",
      "0.5031767721080449\n",
      "0.5011387732998815\n",
      "0.5021352402269936\n",
      "0.501211204347073\n",
      "0.5017700188566603\n",
      "0.5190697510463562\n",
      "0.5032123693722397\n",
      "0.5064760053303448\n",
      "0.502811092381277\n",
      "0.5035212151357282\n",
      "0.5028949335229089\n",
      "0.5030150133161496\n",
      "0.502285499901682\n",
      "0.5009520037994465\n",
      "0.5018260035945719\n",
      "0.5018653605063548\n",
      "0.502981929335839\n",
      "0.5013711648197305\n",
      "0.5025045899833978\n",
      "0.5008323925633996\n",
      "0.5019279771288079\n",
      "0.5030347628056585\n",
      "0.5015167297699731\n",
      "0.5012701656556415\n",
      "0.5019086452079846\n",
      "0.5030918760811524\n",
      "0.5007303904908046\n",
      "0.501906628785924\n",
      "0.5021182173109555\n",
      "0.5015231557189324\n",
      "0.5013859851279637\n",
      "0.5017127202287195\n",
      "0.5011386772240636\n",
      "0.5012722514112764\n",
      "0.5036143829335665\n",
      "0.5010769805402501\n",
      "0.5\n",
      "0.5019753780311227\n",
      "0.5018858184173197\n",
      "0.5012860534054576\n",
      "0.5013545002161242\n",
      "0.50101308667665\n",
      "0.5111749143763051\n",
      "0.5035264436401812\n",
      "0.5027565634031266\n",
      "0.502798652232361\n",
      "0.5011860283540589\n",
      "0.5023095295087757\n",
      "0.5013994240790394\n",
      "0.500758341473862\n",
      "0.5023778056768207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5070740684798477\n",
      "0.5000000974279336\n",
      "0.5051294230818153\n",
      "0.5044774044117174\n",
      "0.5012978033083617\n",
      "0.5026449240072575\n",
      "0.5007885707846557\n",
      "0.5037096098715492\n",
      "0.5026846886186788\n",
      "0.5008209522914149\n",
      "0.5039328013640902\n",
      "0.5030818094133609\n",
      "0.5014554482777474\n",
      "0.5027184619622311\n",
      "0.5027226998689998\n",
      "0.5021355741598862\n",
      "0.5013993811081426\n",
      "0.5028632277157377\n",
      "0.5029421989059404\n",
      "0.5024122297595023\n",
      "0.5010853654072832\n",
      "0.5023782462618823\n",
      "0.5021066066780293\n",
      "0.5026235357617707\n",
      "0.5024250926429358\n",
      "0.5012436471391646\n",
      "0.502151347457188\n",
      "0.5024293990789106\n",
      "0.5044331038335004\n",
      "0.50000016228982\n",
      "0.5024825839711405\n",
      "0.5026217524045278\n",
      "0.5036659482579712\n",
      "0.5027461135424431\n",
      "0.5009264522727424\n",
      "0.5028560007729302\n",
      "0.501149666336497\n",
      "0.5011070328854277\n",
      "0.5011810882091315\n",
      "0.5024834341728682\n",
      "0.5016420141312264\n",
      "0.5013146496292095\n",
      "0.5010718074299595\n",
      "0.5023675786476776\n",
      "0.5018653987933607\n",
      "0.5039691181514971\n",
      "0.501340472623693\n",
      "0.5032122968363625\n",
      "0.5020184573887461\n",
      "0.5019721588477799\n",
      "0.5034955684812946\n",
      "0.5008324617867602\n",
      "0.5016649284151982\n",
      "0.5021793841475427\n",
      "0.5015768532161012\n",
      "0.502281081354272\n",
      "0.5006807165625689\n",
      "0.5018442896184043\n",
      "0.5021572358306491\n",
      "0.5008564346181303\n",
      "0.5006965710023467\n",
      "0.5013567848859666\n",
      "0.501201878577994\n",
      "0.5023163365304567\n",
      "0.5011388014418973\n",
      "0.5022992126001878\n",
      "0.5021427390988452\n",
      "0.501461009262329\n",
      "0.5008815425642053\n",
      "0.5045751896327898\n",
      "0.503516415856222\n",
      "0.5044022018115382\n",
      "0.5075172645025302\n",
      "0.5022946832858493\n",
      "0.5020718406340677\n",
      "0.5014194982277371\n",
      "0.5016648995761642\n",
      "0.5011494450237897\n",
      "0.5030357947682544\n",
      "0.5007990948159787\n",
      "0.5020011728373907\n",
      "0.5013255487400132\n",
      "0.5043933833828204\n",
      "0.5045314140182234\n",
      "0.5019963018643712\n",
      "0.5020259038933529\n",
      "0.502446927559605\n",
      "0.5010170817955779\n",
      "0.5007991431469825\n",
      "0.5033661033064819\n",
      "0.5047539286052776\n",
      "0.5038055598803367\n",
      "0.5042382814868144\n",
      "0.5023036731839555\n",
      "0.5017633991651411\n",
      "0.5028379626877527\n",
      "0.5012721256522596\n",
      "0.5041872636936685\n",
      "0.512649478291732\n",
      "0.5027367584332886\n",
      "0.5019279047234511\n",
      "0.5029111980383523\n",
      "0.5041506399585882\n",
      "0.5027627228037326\n",
      "0.5024199332387158\n",
      "0.5027538215035893\n",
      "0.5010985928344791\n",
      "0.501816953152205\n",
      "0.5032740870144256\n",
      "0.5029583546949652\n",
      "0.5026536517469928\n",
      "0.5019349701202251\n",
      "0.5011609905769956\n",
      "0.5012623629358687\n",
      "0.5012827551783604\n",
      "0.50074893092837\n",
      "0.5020022121542679\n",
      "0.5026768014126127\n",
      "0.5034021582743093\n",
      "0.5027716828872917\n",
      "0.5028329337125815\n",
      "0.5012078434019174\n",
      "0.5019008718287257\n",
      "0.5049664066803369\n",
      "0.5024276984097438\n",
      "0.5034240915516903\n",
      "0.5020339766248084\n",
      "0.5009226408612069\n",
      "0.5032948151568823\n",
      "0.5019279936460294\n",
      "0.5024303951347664\n",
      "0.5018068625731484\n",
      "0.5017897592061725\n",
      "0.5010968496071047\n",
      "0.5018757913126671\n",
      "0.5012202387040278\n",
      "0.5008949050606281\n",
      "0.5036144297122862\n",
      "0.5013243328907102\n",
      "0.5013147011990342\n",
      "0.5022290947664254\n",
      "0.5022533067193506\n",
      "0.5048473332479828\n",
      "0.5035790287303721\n",
      "0.5019720168180118\n",
      "0.5023216031703894\n",
      "0.5048931848410885\n",
      "0.502576741459677\n",
      "0.5021722008526321\n",
      "0.5010967623550253\n",
      "0.501949804975158\n",
      "0.5010232706266865\n",
      "0.5040861333327581\n",
      "0.5079477123931093\n",
      "0.501086775415174\n",
      "0.5033517854636932\n",
      "0.5022253349709299\n",
      "0.5025194435080296\n",
      "0.502416747926343\n",
      "0.5069016406422246\n",
      "0.5024700282341478\n",
      "0.5008949861554817\n",
      "0.5035261493537431\n",
      "0.5011387447678651\n",
      "0.5027743478097301\n",
      "0.5025207894099869\n",
      "0.5015980185676694\n",
      "0.5011173381430444\n",
      "0.5021936425896079\n",
      "0.5012884449607811\n",
      "0.5018171382621682\n",
      "0.501332968415695\n",
      "0.5007132366904996\n",
      "0.5008209579167837\n",
      "0.5011173461681047\n",
      "0.50270420300672\n",
      "0.503354923420013\n",
      "0.5038695742339074\n",
      "0.5031535464287461\n",
      "0.5020422855651533\n",
      "0.5011119156394424\n",
      "0.5016521185642686\n",
      "0.5012491518717361\n",
      "0.5019279140286766\n",
      "0.501252424166417\n",
      "0.5033549603684853\n",
      "0.5019543408997592\n",
      "0.5017883391542806\n",
      "0.5013865460511042\n",
      "0.5021794030814029\n",
      "0.5027217842745754\n",
      "0.5007215533595093\n",
      "0.5008098124041768\n",
      "0.5018450904480608\n",
      "0.5042860417066055\n",
      "0.5012183139509503\n",
      "0.5021143598411879\n",
      "0.5045980381205996\n",
      "0.5010966046808263\n",
      "0.5012455080203527\n",
      "0.501318242027027\n",
      "0.5020431872041784\n",
      "0.50238603894632\n",
      "0.5021345394412382\n",
      "0.5011495545174105\n",
      "0.5007883195353449\n",
      "0.5046066554312963\n",
      "0.5011838652824088\n",
      "0.5021794168279885\n",
      "0.5022399030138441\n",
      "0.5053850160213382\n",
      "0.5033834750375105\n",
      "0.5036603611250673\n",
      "0.5038478599803555\n",
      "0.5012265088907247\n",
      "0.5012612237600669\n",
      "0.5027588265319362\n",
      "0.5009485713092654\n",
      "0.501377805609473\n",
      "0.5013792489657752\n",
      "0.501806958119569\n",
      "0.5025446603355833\n",
      "0.5015460489314628\n",
      "0.5012267128500808\n",
      "0.5005986566553111\n",
      "0.5019279047164068\n",
      "0.5037176880891866\n",
      "0.5039118304731006\n",
      "0.5039237564519902\n",
      "0.5007305864597814\n",
      "0.5035756436403779\n",
      "0.502387707182681\n",
      "0.5035482625416136\n",
      "0.5010671698019321\n",
      "0.5024972963594487\n",
      "0.5036011685377213\n",
      "0.5036603309152858\n",
      "0.5049596310516056\n",
      "0.5033292199414596\n",
      "0.5026213201466189\n",
      "0.5013182186797032\n",
      "0.5012018473360745\n",
      "0.5023034856676436\n",
      "0.5017633080057186\n",
      "0.5038566426346303\n",
      "0.5023290037955634\n",
      "0.5039504869331023\n",
      "0.5019066097825927\n",
      "0.5008325164749932\n",
      "0.5033513037294395\n",
      "0.5024568874991646\n",
      "0.5027487748477425\n",
      "0.5017173468268309\n",
      "0.5011173766024506\n",
      "0.5034369227269154\n",
      "0.5015362936539155\n",
      "0.5018857180673955\n",
      "0.5017897206075349\n",
      "0.5009084628456459\n",
      "0.5020431946077201\n",
      "0.5012523660808962\n",
      "0.5013308413011025\n",
      "0.5014976935002914\n",
      "0.5067662370507907\n",
      "0.5019602586838654\n",
      "0.5022991678543587\n",
      "0.5021057430127972\n",
      "0.5008563947940443\n",
      "0.5028512143501215\n",
      "0.5018741794195819\n",
      "0.5014610714400873\n",
      "0.5014188990954784\n",
      "0.5016885036161327\n",
      "0.5020717707502034\n",
      "0.5006654212890274\n",
      "0.5011070086997134\n",
      "0.5011279743788493\n",
      "0.5050300110191094\n",
      "0.5017898677653573\n",
      "0.5021997658635021\n",
      "0.5012524544031571\n",
      "0.5015768996202051\n",
      "0.5012525075412291\n",
      "0.5019603036209334\n",
      "0.5013073346162035\n",
      "0.5016196509365993\n",
      "0.5025222872190872\n",
      "0.5021244270202703\n",
      "0.5039194600485902\n",
      "0.5029727820171751\n",
      "0.5019997703501716\n",
      "0.5028512134588771\n",
      "0.5007991585804052\n",
      "0.5014882161141838\n",
      "0.5025370097863986\n",
      "0.5035414479765973\n",
      "0.5030865780784141\n",
      "0.5021796097866481\n",
      "0.5027163706778411\n",
      "0.5009834528480541\n",
      "0.5021143513096807\n",
      "0.50096771774928\n",
      "0.5011069618038053\n",
      "0.5026846384680904\n",
      "0.5021827842751411\n",
      "0.5020430711477843\n",
      "0.5054969602847063\n",
      "0.5016648733528847\n",
      "0.5011279375433288\n",
      "0.5023652764547\n",
      "0.5018259783793175\n",
      "0.5013110331071239\n",
      "0.502018354863335\n",
      "0.5021448970399036\n",
      "0.5053058698478403\n",
      "0.502453074473247\n",
      "0.502277234425664\n",
      "0.5027255274108625\n",
      "0.5040930508081968\n",
      "0.5014195264184529\n",
      "0.5016884404339398\n",
      "0.503040786960209\n",
      "0.5014373533084789\n",
      "0.501425978106369\n",
      "0.5012202463642589\n",
      "0.5011939474472251\n",
      "0.5023659422544353\n",
      "0.5036602338922999\n",
      "0.5027255155749242\n",
      "0.5017126899188246\n",
      "0.5022020928922266\n",
      "0.5023242165655323\n",
      "0.5012351142702445\n",
      "0.5025175134728078\n",
      "0.5019949453935479\n",
      "0.5008949948444528\n",
      "0.5019720980927669\n",
      "0.5020925052633411\n",
      "0.5015563138372628\n",
      "0.5029617955277114\n",
      "0.5012967261017085\n",
      "0.5015563476173177\n",
      "0.5012173300697985\n",
      "0.50194974105902\n",
      "0.5033067133736068\n",
      "0.5010130578066019\n",
      "0.5015918314346905\n",
      "0.5019997596620086\n",
      "0.5046684203518658\n",
      "0.5028879382592187\n",
      "0.5013018081876673\n",
      "0.5024718420198965\n",
      "0.501386426947112\n",
      "0.5022643458082238\n",
      "0.5046633244221455\n",
      "0.5013329379157673\n",
      "0.5033842453042044\n",
      "0.5029837482137927\n",
      "0.5058635290735602\n",
      "0.5009521399389681\n",
      "0.5029657922103725\n",
      "0.5032988018996175\n",
      "0.5000001118179791\n",
      "0.503626042083839\n",
      "0.5019963391423251\n",
      "0.5027821172477611\n",
      "0.5026476257187174\n",
      "0.5026014736936577\n",
      "0.5023652061003151\n",
      "0.5034766422206193\n",
      "0.5018452022637541\n",
      "0.5021156462983056\n",
      "0.5021346556122775\n",
      "0.5012069112662597\n",
      "0.5013428070865282\n",
      "0.5016197432531334\n",
      "0.5011638081763385\n",
      "0.5011334534904125\n",
      "0.5009372338265192\n",
      "0.5008949694962664\n",
      "0.5016648490057017\n",
      "0.500956186418825\n",
      "0.5009518968823936\n",
      "0.5011939342439313\n",
      "0.5023592794476807\n",
      "0.5023205345776514\n",
      "0.5028871462634195\n",
      "0.5020183977914051\n",
      "0.5010673572415281\n",
      "0.5012111758202085\n",
      "0.502431758310911\n",
      "0.505874132351522\n",
      "0.500000093219212\n",
      "0.5028162497469354\n",
      "0.502179532462195\n",
      "0.5011839270832829\n",
      "0.5015979579465101\n",
      "0.5008817327622936\n",
      "0.5022847655358205\n",
      "0.5030280782876906\n",
      "0.5007885999033435\n",
      "0.5028398367252211\n",
      "0.501275948513531\n",
      "0.5009086787082119\n",
      "0.5009878476576753\n",
      "0.5034015259285259\n",
      "0.5008325809252011\n",
      "0.5009085850457444\n",
      "0.5026144108513438\n",
      "0.5014260923474587\n",
      "0.5037610797553421\n",
      "0.5017377005845084\n",
      "0.5008817925465187\n",
      "0.50151679308594\n",
      "0.5033968429340745\n",
      "0.5009998891981293\n",
      "0.50133671534015\n",
      "0.5013550485710272\n",
      "0.5021346440641734\n",
      "0.5034435052891323\n",
      "0.5023035184586907\n",
      "0.503391523097041\n",
      "0.5018970573889239\n",
      "0.5007132001840131\n",
      "0.5022503104141226\n",
      "0.5030707314641686\n",
      "0.5009371031903783\n",
      "0.5023033843435069\n",
      "0.5022992186132146\n",
      "0.5028329878385184\n",
      "0.5038555176743182\n",
      "0.5020222817706197\n",
      "0.5027743195308144\n",
      "0.5022303122557179\n",
      "0.5027218112355263\n",
      "0.501995034825543\n",
      "0.5022170706242577\n",
      "0.5012523195068956\n",
      "0.5011497847804304\n",
      "0.5012524172467827\n",
      "0.5012217443484962\n",
      "0.5019497151175745\n",
      "0.5019254753821476\n",
      "0.5017126276797397\n",
      "0.5033698110117247\n",
      "0.5023451630138239\n",
      "0.5048932245471632\n",
      "0.5021301252903195\n",
      "0.5028085099112563\n",
      "0.5014736286469268\n",
      "0.5014194594882233\n",
      "0.5024667857634655\n",
      "0.503124064703864\n",
      "0.5043742131878673\n",
      "0.5007884242553458\n",
      "0.5016884508166433\n",
      "0.5010045737958526\n",
      "0.5012101354621435\n",
      "0.5018857159129897\n",
      "0.5017897255648096\n",
      "0.5018752688859863\n",
      "0.5012437291169891\n",
      "0.5013848555249796\n",
      "0.5032901336882832\n",
      "0.5020534733460005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtask 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import DMatrix,train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data into the form which XGboost can take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_features = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>queries</th>\n",
       "      <th>passage</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>passage_cleaned</th>\n",
       "      <th>query_cleaned</th>\n",
       "      <th>co_similarity</th>\n",
       "      <th>bm25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>1079831</td>\n",
       "      <td>1006123</td>\n",
       "      <td>world trade organization definition</td>\n",
       "      <td>Definition of free trade. : trade based on the...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[definition, free, trade, trade, base, unrestr...</td>\n",
       "      <td>[world, trade, organization, definition]</td>\n",
       "      <td>0.652645</td>\n",
       "      <td>20.733095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1078920</td>\n",
       "      <td>100198</td>\n",
       "      <td>women benefits from taking dim</td>\n",
       "      <td>Effects of Medications during Pregnancy. We kn...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[effect, medications, pregnancy, know, little,...</td>\n",
       "      <td>[women, benefit, take, dim]</td>\n",
       "      <td>0.478454</td>\n",
       "      <td>16.041879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>1078752</td>\n",
       "      <td>1011079</td>\n",
       "      <td>withdrawal symptoms of amitriptyline</td>\n",
       "      <td>Amitriptyline for the treatment of depression....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[amitriptyline, treatment, depression, amitrip...</td>\n",
       "      <td>[withdrawal, symptoms, amitriptyline]</td>\n",
       "      <td>0.693089</td>\n",
       "      <td>14.635104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1101861</td>\n",
       "      <td>1001625</td>\n",
       "      <td>windstream troubleshooting phone number</td>\n",
       "      <td>Fisher Price Smart Cycle Manual Troubleshootin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[fisher, price, smart, cycle, manual, troubles...</td>\n",
       "      <td>[windstream, troubleshoot, phone, number]</td>\n",
       "      <td>0.604961</td>\n",
       "      <td>11.993841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>1068924</td>\n",
       "      <td>1014493</td>\n",
       "      <td>why hemorrhagic disease tests due to vitamin k...</td>\n",
       "      <td>Vitamin B12 is also said to help some sleep di...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[vitamin, b12, also, say, help, sleep, disorde...</td>\n",
       "      <td>[hemorrhagic, disease, test, due, vitamin, k, ...</td>\n",
       "      <td>0.785148</td>\n",
       "      <td>28.156938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>15063</td>\n",
       "      <td>1011490</td>\n",
       "      <td>alabama central credit union routing number</td>\n",
       "      <td>Generations Federal Credit Union Routing Numbe...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[generations, federal, credit, union, rout, nu...</td>\n",
       "      <td>[alabama, central, credit, union, rout, number]</td>\n",
       "      <td>0.768005</td>\n",
       "      <td>41.377591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>14151</td>\n",
       "      <td>1012028</td>\n",
       "      <td>age requirements for name change</td>\n",
       "      <td>For example, someone's age might be an indepen...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[example, someone, age, might, independent, va...</td>\n",
       "      <td>[age, requirements, name, change]</td>\n",
       "      <td>0.554912</td>\n",
       "      <td>18.958650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>1086532</td>\n",
       "      <td>1008690</td>\n",
       "      <td>advanced weighing technology definition</td>\n",
       "      <td>La Crosse Technology brings you the most affor...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[la, crosse, technology, bring, affordable, re...</td>\n",
       "      <td>[advance, weigh, technology, definition]</td>\n",
       "      <td>0.527106</td>\n",
       "      <td>15.648874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>8854</td>\n",
       "      <td>1011732</td>\n",
       "      <td>________ disparity refers to the slightly diff...</td>\n",
       "      <td>Salaries for women in residency are lower than...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[salaries, women, residency, lower, men, respe...</td>\n",
       "      <td>[________, disparity, refer, slightly, differe...</td>\n",
       "      <td>0.508428</td>\n",
       "      <td>10.672300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2</td>\n",
       "      <td>1001876</td>\n",
       "      <td>Androgen receptor define</td>\n",
       "      <td>Like all steroid hormones, androgens produce e...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[like, steroid, hormones, androgens, produce, ...</td>\n",
       "      <td>[androgen, receptor, define]</td>\n",
       "      <td>0.782608</td>\n",
       "      <td>13.887665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         qid      pid                                            queries  \\\n",
       "376  1079831  1006123                world trade organization definition   \n",
       "104  1078920   100198                     women benefits from taking dim   \n",
       "676  1078752  1011079               withdrawal symptoms of amitriptyline   \n",
       "74   1101861  1001625            windstream troubleshooting phone number   \n",
       "865  1068924  1014493  why hemorrhagic disease tests due to vitamin k...   \n",
       "..       ...      ...                                                ...   \n",
       "720    15063  1011490        alabama central credit union routing number   \n",
       "753    14151  1012028                   age requirements for name change   \n",
       "519  1086532  1008690            advanced weighing technology definition   \n",
       "731     8854  1011732  ________ disparity refers to the slightly diff...   \n",
       "98         2  1001876                           Androgen receptor define   \n",
       "\n",
       "                                               passage  relevancy  \\\n",
       "376  Definition of free trade. : trade based on the...        0.0   \n",
       "104  Effects of Medications during Pregnancy. We kn...        0.0   \n",
       "676  Amitriptyline for the treatment of depression....        0.0   \n",
       "74   Fisher Price Smart Cycle Manual Troubleshootin...        0.0   \n",
       "865  Vitamin B12 is also said to help some sleep di...        0.0   \n",
       "..                                                 ...        ...   \n",
       "720  Generations Federal Credit Union Routing Numbe...        0.0   \n",
       "753  For example, someone's age might be an indepen...        0.0   \n",
       "519  La Crosse Technology brings you the most affor...        0.0   \n",
       "731  Salaries for women in residency are lower than...        0.0   \n",
       "98   Like all steroid hormones, androgens produce e...        0.0   \n",
       "\n",
       "                                       passage_cleaned  \\\n",
       "376  [definition, free, trade, trade, base, unrestr...   \n",
       "104  [effect, medications, pregnancy, know, little,...   \n",
       "676  [amitriptyline, treatment, depression, amitrip...   \n",
       "74   [fisher, price, smart, cycle, manual, troubles...   \n",
       "865  [vitamin, b12, also, say, help, sleep, disorde...   \n",
       "..                                                 ...   \n",
       "720  [generations, federal, credit, union, rout, nu...   \n",
       "753  [example, someone, age, might, independent, va...   \n",
       "519  [la, crosse, technology, bring, affordable, re...   \n",
       "731  [salaries, women, residency, lower, men, respe...   \n",
       "98   [like, steroid, hormones, androgens, produce, ...   \n",
       "\n",
       "                                         query_cleaned  co_similarity  \\\n",
       "376           [world, trade, organization, definition]       0.652645   \n",
       "104                        [women, benefit, take, dim]       0.478454   \n",
       "676              [withdrawal, symptoms, amitriptyline]       0.693089   \n",
       "74           [windstream, troubleshoot, phone, number]       0.604961   \n",
       "865  [hemorrhagic, disease, test, due, vitamin, k, ...       0.785148   \n",
       "..                                                 ...            ...   \n",
       "720    [alabama, central, credit, union, rout, number]       0.768005   \n",
       "753                  [age, requirements, name, change]       0.554912   \n",
       "519           [advance, weigh, technology, definition]       0.527106   \n",
       "731  [________, disparity, refer, slightly, differe...       0.508428   \n",
       "98                        [androgen, receptor, define]       0.782608   \n",
       "\n",
       "          bm25  \n",
       "376  20.733095  \n",
       "104  16.041879  \n",
       "676  14.635104  \n",
       "74   11.993841  \n",
       "865  28.156938  \n",
       "..         ...  \n",
       "720  41.377591  \n",
       "753  18.958650  \n",
       "519  15.648874  \n",
       "731  10.672300  \n",
       "98   13.887665  \n",
       "\n",
       "[1000 rows x 9 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.sort_values(by=['queries'], ascending=False)\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdaMart_x_train = np.zeros((train_data.shape[0], num_of_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       ...,\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdaMart_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.65264477, 20.73309484],\n",
       "       [ 0.47845439, 16.04187895],\n",
       "       [ 0.6930888 , 14.63510436],\n",
       "       ...,\n",
       "       [ 0.52710561, 15.64887427],\n",
       "       [ 0.50842772, 10.67230032],\n",
       "       [ 0.78260784, 13.88766498]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdaMart_x_train = train_data[['co_similarity', 'bm25']].values\n",
    "lambdaMart_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdaMart_y_train = train_data.relevancy.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_count_dict_train = train_data['queries'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'what cities allow pit bulls': 5,\n",
       " 'how much bitcoins are there in the world': 5,\n",
       " 'is chromium reactive': 5,\n",
       " 'what does rice stand for with a muscle injury': 5,\n",
       " 'what are two main gases in': 4,\n",
       " 'what does starling mean': 4,\n",
       " 'price chopper locations in ct': 4,\n",
       " 'gold price in ksa dammam': 4,\n",
       " 'what is one role of the element phosphorus?': 3,\n",
       " 'what medications could cause pemphigoid': 3,\n",
       " 'where is bellevue canada?': 3,\n",
       " 'what is the fdic and what does it do': 3,\n",
       " 'average temperatures cairo november': 3,\n",
       " 'how long does respite last': 3,\n",
       " 'how big can leopard tortoises get': 3,\n",
       " 'what is respite': 3,\n",
       " 'price of silver per ounce history': 3,\n",
       " 'what is chattahoochee': 3,\n",
       " 'aspen dental corporate telephone number': 3,\n",
       " 'how much is the average dj for a wedding': 3,\n",
       " 'what is supply chain mps': 3,\n",
       " 'how does a hydrate differ from an anhydrate': 3,\n",
       " 'what country does fennel come from': 3,\n",
       " 'what are the two major subdivisions of the nervous system?': 3,\n",
       " 'who makes jammy dodgers': 3,\n",
       " 'where is metro ny distribution center': 3,\n",
       " 'are snails good for plants': 3,\n",
       " 'does the independent or dependent variable go on the x-axis': 3,\n",
       " 'how does rational choice theory predict': 2,\n",
       " 'who authored desperation': 2,\n",
       " 'hypotenuse define': 2,\n",
       " 'population of paris texas': 2,\n",
       " 'how many days are in three years': 2,\n",
       " 'how many people can sit in a car at sci fye theater disney': 2,\n",
       " 'temperature in fort myers': 2,\n",
       " 'highest cash rewards credit cards': 2,\n",
       " 'who is christopher columbus define': 2,\n",
       " 'canada most dense area': 2,\n",
       " 'how long do you have to brush your teeth': 2,\n",
       " 'how fast is a dixie chopper magnum': 2,\n",
       " 'cgd autoimmune': 2,\n",
       " 'how long do i bake squash seeds': 2,\n",
       " 'how is page yield calculated for ink cartridge': 2,\n",
       " 'what happens when stop drinking alcohol': 2,\n",
       " 'what is the specific name for walrus classifying organisms': 2,\n",
       " 'what helps hives': 2,\n",
       " 'in history what is a sphere of influence': 2,\n",
       " 'what is an integrated studies degree': 2,\n",
       " 'how many dna bases are in your body': 2,\n",
       " 'how long would the border wall need to be?': 2,\n",
       " 'how many pages is the coquette': 2,\n",
       " 'what foods contain retinol': 2,\n",
       " 'cast of the seal team': 2,\n",
       " 'was the philippines a us territory': 2,\n",
       " 'what is the state tax rate for orange county ca': 2,\n",
       " 'who proposed the xy theory': 2,\n",
       " 'what are causes of swelling of my tongue?': 2,\n",
       " 'during the depression what percent of american workers were unemployed': 2,\n",
       " 'difference between subjunctive mood and conditional': 2,\n",
       " 'what are the brachial plexus nerves': 2,\n",
       " 'what is a lantern fish': 2,\n",
       " 'what was the goal of americanization programs in settlement houses?': 2,\n",
       " 'what are your upper two teeth called': 2,\n",
       " 'does solar radiation cause global warming': 2,\n",
       " 'how many words for an introduction manuscript': 2,\n",
       " 'is lenovo better than hp': 2,\n",
       " 'when will foxconn begin manufacturing': 2,\n",
       " 'who is the guy who appears in all the marvel movies': 2,\n",
       " 'what is congenital sclerocornea?': 2,\n",
       " 'budget meaning': 2,\n",
       " 'average annual income for rn in illinois': 2,\n",
       " 'is it windy in arizona': 2,\n",
       " 'is minneapolis sick and safe time law': 2,\n",
       " 'what is compressed air': 2,\n",
       " 'what is the sales tax rate in lawrenceville  georgia': 2,\n",
       " 'what is employee churn': 2,\n",
       " 'unspecific synonym': 2,\n",
       " 'what years were total eclipses in the past': 2,\n",
       " 'what is the temp for costa rica': 2,\n",
       " 'what is the meaning of global warming': 2,\n",
       " 'what side of the car should a car seat be on': 2,\n",
       " 'what is german smear': 2,\n",
       " 'is mp3obsession safe and legal': 2,\n",
       " 'temperature sensors - analog output': 2,\n",
       " 'which was one immediate effect': 2,\n",
       " 'what is a shotgun': 2,\n",
       " 'how set up automatic reply in outlook': 2,\n",
       " 'which feature do gas giants have that terrestrial planets do not?': 2,\n",
       " 'what are the hazards of ammonium': 2,\n",
       " 'what are the average annual sales made with dunkin donuts': 2,\n",
       " 'what is the shingles jab?': 2,\n",
       " 'how many feet from a fire hydr': 2,\n",
       " 'what decisions rules can determine upheld or dismiss a claim': 2,\n",
       " 'what internal temperature should chicken': 2,\n",
       " 'how many ounces in dunkin donuts iced coffee medium': 2,\n",
       " 'blood diseases that are sexually transmitted': 2,\n",
       " 'cadillac meaning': 2,\n",
       " 'how many hours are in fmla': 2,\n",
       " 'what do teachers need from administrators': 2,\n",
       " 'what is the prevalence of overweight among children and adolescents in the u.s, and what are the contributing factors?': 2,\n",
       " 'does global entry include tsa': 2,\n",
       " 'how does humira work for ulcerative colitis': 2,\n",
       " 'what is effect of taurine': 2,\n",
       " 'what is a soft close toilet seat': 2,\n",
       " 'mitt trainings': 2,\n",
       " 'difference between protime and inr': 2,\n",
       " 'which hormone is directly responsible for the development of secondary sex characteristics in males?': 2,\n",
       " 'what geological features are shared by all terrestrial planets': 2,\n",
       " 'the hormone that is essential for growth is produced in the?___________________': 2,\n",
       " 'what hormone stimulates the production of sperm gcse': 2,\n",
       " 'is the hulk stronger than the thing': 2,\n",
       " 'monoxide poisoning symptoms': 2,\n",
       " 'when was the shazam movie released?sadasdasdasdsadasasdasdasdasdd': 2,\n",
       " 'tceq renewal hours': 2,\n",
       " 'how many feet from a fire': 2,\n",
       " 'which of the following events must cause equilibrium price to rise?': 2,\n",
       " 'which legal protection requires you to defend it with lawsuits': 2,\n",
       " 'can you find emphysema on a xray': 2,\n",
       " 'where is the total solar eclipse visible': 2,\n",
       " 'what is hp system bios': 2,\n",
       " 'average temperatures des moines iowa': 2,\n",
       " 'what is pies and pints': 2,\n",
       " 'what disease causes shaking hands': 2,\n",
       " 'is mile longer than yards': 2,\n",
       " 'where is stanford?': 2,\n",
       " 'who was yankee doodle': 2,\n",
       " 'how many members in the iaca': 2,\n",
       " 'when is the solar eclipse in south carolina': 2,\n",
       " 'when is the rainy season in costa rica': 2,\n",
       " 'is a call an option?': 2,\n",
       " 'does LOS ANGELES have buses or metro': 2,\n",
       " 'what does mepa stand for in ma': 2,\n",
       " 'what is a character attribute': 2,\n",
       " 'how old is dr. nowzaradan': 2,\n",
       " 'where is danvers, ma': 2,\n",
       " 'weight watchers points for oatmeal packet': 2,\n",
       " 'what supplies blood to the heart muscle': 2,\n",
       " 'how old was gary coleman when he start acting': 2,\n",
       " 'is a revocable trust a separate legal entity': 2,\n",
       " 'what muscle does isometric shoulder flexion facilitate': 2,\n",
       " 'how long is the flight from chicago to cairo': 2,\n",
       " 'biggest island of hawaii': 2,\n",
       " 'ira transfer vs rollover ira': 2,\n",
       " 'are land grading costs deductible': 2,\n",
       " 'cva causes': 1,\n",
       " 'how long should i wait to handle my ball python after feeding': 1,\n",
       " 'att email toll free number': 1,\n",
       " 'what causes sores on my head': 1,\n",
       " 'what type of shot do you get for whooping cough': 1,\n",
       " 'the cast of emoji movie': 1,\n",
       " 'where do cows live': 1,\n",
       " 'coloring pages numbers': 1,\n",
       " 'how long does a mobile check hold': 1,\n",
       " 'what is surety bond and how it works': 1,\n",
       " 'what is net gain': 1,\n",
       " 'what is average roi for advertising': 1,\n",
       " 'google family security credit union routing number': 1,\n",
       " 'how to do a basic squat': 1,\n",
       " 'what is andrea tantaros doing': 1,\n",
       " 'how much do you make at tesla': 1,\n",
       " 'can lice be transmitted to pets': 1,\n",
       " 'where is south easton ma': 1,\n",
       " 'what does mpid stand for military': 1,\n",
       " 'does quasi experimental use pretest and posttest': 1,\n",
       " 'benefits of the master cleanse lemonade diet': 1,\n",
       " 'columbus texas is in what county': 1,\n",
       " 'what is methylphenidate generic for': 1,\n",
       " 'how to pass the certified occupancy course': 1,\n",
       " 'what type of substance minimizes the changes in h+ and oh- in a solution.': 1,\n",
       " 'how much weight do you gain during menopause': 1,\n",
       " 'how much should you save per month': 1,\n",
       " 'what year was the rapid city flood': 1,\n",
       " 'homocysteine routine test': 1,\n",
       " 'what is java for': 1,\n",
       " \"what is varignon's theorem\": 1,\n",
       " 'how long to roast whole.chicken': 1,\n",
       " 'how much does a bar of gold worth': 1,\n",
       " 'what does increment of': 1,\n",
       " 'can chewing gum prevent heartburn': 1,\n",
       " 'is alcoholics anonymous tax deductible': 1,\n",
       " 'women benefits from taking dim': 1,\n",
       " 'what is (prospectus': 1,\n",
       " 'what is donald trump a draft dodger': 1,\n",
       " 'what is the benefit of trans fats': 1,\n",
       " 'what is the website brainhoney': 1,\n",
       " 'is merrimack county part of state of nh': 1,\n",
       " 'how many taxes do you pay when you withdraw from your ira': 1,\n",
       " 'cad heart related': 1,\n",
       " 'what is an amazon center': 1,\n",
       " 'which pair of chromosomes would result in the birth of a male': 1,\n",
       " 'what character did michael b. jordan in creed': 1,\n",
       " 'how long are you contagious with a stomach bug': 1,\n",
       " 'how long does priority mail take to get delivered': 1,\n",
       " 'what is the purpose of polarized sunglasses': 1,\n",
       " 'days sales in cash ratio definition': 1,\n",
       " 'what the watermelon parts are called': 1,\n",
       " 'what is rally in agile': 1,\n",
       " 'what are the muted colors': 1,\n",
       " 'weather in shirdi, india': 1,\n",
       " 'how fast does stomach flu come on': 1,\n",
       " 'maluma height and weight': 1,\n",
       " 'where is fairview': 1,\n",
       " 'how long after pap smear will i receive results': 1,\n",
       " 'definition for daring': 1,\n",
       " 'how to obtain notary identification florida': 1,\n",
       " 'where is the street webster': 1,\n",
       " 'medical causes for low sodium': 1,\n",
       " 'what is issue two on the ohio ballot': 1,\n",
       " 'what is a double acrostic puzzle': 1,\n",
       " 'how much does general anesthesia cost': 1,\n",
       " 'statute miles definition': 1,\n",
       " 'who played alicia on the big bang theory': 1,\n",
       " 'what does bail type cash or bond mean': 1,\n",
       " \"how much is a sheet of plywood at lowe's\": 1,\n",
       " 'symptoms for color blindness': 1,\n",
       " 'calorie sweet potatoes': 1,\n",
       " 'what is pleurisy symptoms': 1,\n",
       " 'what unit is mccall in': 1,\n",
       " 'health effects of styrofoam cups': 1,\n",
       " 'what county is elkhart texas in': 1,\n",
       " 'what is the plural of titmouse': 1,\n",
       " 'what county is dayton mn': 1,\n",
       " 'longest mountain range in iran and iraq': 1,\n",
       " 'where is george michael buried': 1,\n",
       " 'the wellness collective': 1,\n",
       " 'what is the most protected crop in japan?': 1,\n",
       " 'how many carbs in a quest bar': 1,\n",
       " 'what is phone number for verizon tv': 1,\n",
       " \"why does cupid represent valentine's day\": 1,\n",
       " 'lewiston idaho fedex express phone number': 1,\n",
       " 'types of java fonts': 1,\n",
       " 'definition of objective in project management': 1,\n",
       " 'how long does it take to get pmi applications approvals': 1,\n",
       " 'how to change a bathroom water valve': 1,\n",
       " 'is it the law in california that your child has to be vaccinated': 1,\n",
       " 'who sang delta dawn?': 1,\n",
       " 'what hours are considered brunch': 1,\n",
       " 'what city is orange county ca': 1,\n",
       " 'definition of dignity for kids': 1,\n",
       " 'what is a tag sale': 1,\n",
       " 'what is a nigerian maggot': 1,\n",
       " 'in its infancy synonym': 1,\n",
       " 'what is a variable account in a annuity': 1,\n",
       " \"what's the meaning of the name blackburn\": 1,\n",
       " 'what are jewelry designers?': 1,\n",
       " 'most popular depression glass patterns': 1,\n",
       " 'what is wells fargo go far reward': 1,\n",
       " 'what is enteritis symptoms': 1,\n",
       " 'what is a major function of the neurological system?': 1,\n",
       " 'what muscles could be involved in a rotator cuff injury': 1,\n",
       " 'sources of protein list for diabetics': 1,\n",
       " 'how much money is donald trump worth': 1,\n",
       " 'who was george w kirk': 1,\n",
       " 'what color represents selflessness': 1,\n",
       " 'where is ymca kimball camp located': 1,\n",
       " 'average money market interest rates': 1,\n",
       " 'withdrawal symptoms of amitriptyline': 1,\n",
       " 'how tall should vent stack be': 1,\n",
       " 'what is the difference between a medical doctor and a doctor of osteopathic': 1,\n",
       " 'what is legal outside counsel': 1,\n",
       " 'banana cultivars': 1,\n",
       " 'cost of attending kennesaw state university': 1,\n",
       " 'what is a correlational case study?': 1,\n",
       " 'who was william henry preceded by': 1,\n",
       " 'is hummus high in protein': 1,\n",
       " 'should you take extra vitamin b12 along with b complex': 1,\n",
       " 'do you have to collect copays': 1,\n",
       " 'does forskolin diet work': 1,\n",
       " 'does flomax cause liver damage': 1,\n",
       " 'what is employee turnover definition': 1,\n",
       " 'world trade organization definition': 1,\n",
       " 'the definition of lend': 1,\n",
       " 'what job can you get with a degree in art education': 1,\n",
       " 'what is alloy c': 1,\n",
       " 'cortana what is the apocalypse': 1,\n",
       " 'which region does the black bear live in': 1,\n",
       " 'another name for brooke': 1,\n",
       " 'do leafy greens have iron': 1,\n",
       " 'where is montella italy': 1,\n",
       " 'what are aneurysm': 1,\n",
       " 'which chromosome controls sex characteristics': 1,\n",
       " 'what is the nutritional value of oatmeal': 1,\n",
       " 'audible phone number customer service': 1,\n",
       " 'temperature in april in bali': 1,\n",
       " 'where did hip hop/rap come from': 1,\n",
       " 'meaning of last name, harrison': 1,\n",
       " 'what are the primary characteristics of the inner planets?': 1,\n",
       " 'what does the abbreviation cc mean in an email': 1,\n",
       " 'cost analysis of school based obesity prevention program': 1,\n",
       " 'is paris in europe': 1,\n",
       " 'northampton county tax department pa phone number': 1,\n",
       " 'what are all the chemical properties': 1,\n",
       " 'culpeper flag': 1,\n",
       " 'what is relaxation training techniques': 1,\n",
       " 'which number is upc?': 1,\n",
       " 'what federal statute gives the epa authority to regulate pesticides': 1,\n",
       " 'calories in serving of jelly': 1,\n",
       " 'botulinum definition': 1,\n",
       " 'who is the ninja turtle with the blue mask': 1,\n",
       " 'can wisdom teeth cause hearing issues': 1,\n",
       " 'what is the mathematical formula for volume of a cylinder': 1,\n",
       " 'what is a corporate bylaws': 1,\n",
       " 'is clindamycin an antibiotic': 1,\n",
       " 'is dhgate a scam': 1,\n",
       " 'how many chemicals has the fda banned in cosmetics in the us?': 1,\n",
       " 'can seizure meds cause low sodium?': 1,\n",
       " 'what president of the united states was born in lamar missouri and farmed for several years': 1,\n",
       " 'what is the difference between thrombosis and embolism?': 1,\n",
       " 'cortana what is the weather in whitefish montana': 1,\n",
       " 'does tarrant county assessor figure your taxes on assessed or appraised': 1,\n",
       " 'what does semen consist of': 1,\n",
       " 'which skin tag removal product works best': 1,\n",
       " 'what type of chemical bond is found between paired bases of the dna double helix': 1,\n",
       " 'centra credit union routing number': 1,\n",
       " 'how long between the birth of each kitten does it take': 1,\n",
       " 'what does buerger allen exercises help': 1,\n",
       " 'where is the federal penitentiary in ind': 1,\n",
       " 'what county is morrilton arkansas in': 1,\n",
       " 'which prefix means toward': 1,\n",
       " 'what is a spanner wrench used for': 1,\n",
       " 'population in davidson county tn': 1,\n",
       " 'weather in gig harbor, wa': 1,\n",
       " 'the function of the olfactory nerve concerns': 1,\n",
       " 'what is cefazolin used to treat': 1,\n",
       " 'what test is for lupus': 1,\n",
       " 'what does population density': 1,\n",
       " 'what convinces employees want': 1,\n",
       " 'average time to sit on the toilet': 1,\n",
       " 'which county is knoxville, tn in': 1,\n",
       " 'texas roadhouse glen mills pa phone number': 1,\n",
       " 'where on the map is macedonia located number': 1,\n",
       " 'most nigeria read newspaper': 1,\n",
       " 'who proposed the geocentric theory': 1,\n",
       " 'is cholecystitis fatal': 1,\n",
       " 'what degree do you need to become a nurse': 1,\n",
       " 'how long should i slow cook ribs': 1,\n",
       " 'what is the merv rating on filters': 1,\n",
       " 'what year did the korean war happen': 1,\n",
       " 'server virtualization defined': 1,\n",
       " 'idaho definition of signed': 1,\n",
       " 'what does distraction mean': 1,\n",
       " 'how old is elly tran ha': 1,\n",
       " 'do male jaguar cubs grow faster than females?': 1,\n",
       " 'is risperdal a mood stabilizer': 1,\n",
       " 'where are bacteria found?': 1,\n",
       " 'cost estimate glass shower': 1,\n",
       " 'what do whales in the coral reef eat': 1,\n",
       " 'where do people speak icelandic': 1,\n",
       " 'what happens if water sits your lungs': 1,\n",
       " 'when was the northridge california earthquake': 1,\n",
       " 'what is grenadine in drinks': 1,\n",
       " 'what are the trees used for paper': 1,\n",
       " 'what wine goes with corned beef': 1,\n",
       " 'golf cost': 1,\n",
       " 'what are properties of ponstel': 1,\n",
       " 'what is the lemonade cleanse': 1,\n",
       " 'population density definition': 1,\n",
       " 'cannot print to adobe pdf printer': 1,\n",
       " 'describe java.awt package': 1,\n",
       " 'what does the greek word kai mean': 1,\n",
       " \"what was al capone's first crime\": 1,\n",
       " 'is kratom banned in dea': 1,\n",
       " 'weather for tennessee in april': 1,\n",
       " 'who is the president of the republic of texas right now mark smith': 1,\n",
       " 'is blighted ovum the same as molar pregnancy': 1,\n",
       " 'where was robert anderson born and when': 1,\n",
       " 'what vitamin is lacking in elderly women': 1,\n",
       " 'what is smsvchost.exe': 1,\n",
       " 'how many feet is a survey pole': 1,\n",
       " 'what is the goal for the child with a cognitive impairment?': 1,\n",
       " 'what body parts does pku affect': 1,\n",
       " 'har-co credit union routing number': 1,\n",
       " 'what is a ob tech': 1,\n",
       " 'what foods are good if you have gout?': 1,\n",
       " 'what removes plaque from arteries': 1,\n",
       " 'bara properties inc': 1,\n",
       " 'what county is attleboro ma': 1,\n",
       " 'what is the cost of bellevue community classes': 1,\n",
       " 'average days in inventory turnover': 1,\n",
       " 'how to use stall bars': 1,\n",
       " 'benefits management fairport, ny': 1,\n",
       " 'difference between scholarships grants loans': 1,\n",
       " 'what product to use for dry hair': 1,\n",
       " 'how long does tap water have to sit before i put my fish in it': 1,\n",
       " 'what does it mean to be alienated': 1,\n",
       " 'examples is bottleneck effect in biology': 1,\n",
       " 'what important job do the lysosomes have': 1,\n",
       " 'american translators association how to study': 1,\n",
       " 'intermediate anatomy definition': 1,\n",
       " 'what does cilostazol treat': 1,\n",
       " 'weather in sparta greece': 1,\n",
       " 'how much does it cost for a concrete driveway': 1,\n",
       " 'what does carte blanche mean': 1,\n",
       " 'which city is harvard in': 1,\n",
       " 'what does an underactive thyroid cause': 1,\n",
       " 'are tesla electric cars': 1,\n",
       " 'definition of capias issued on a background': 1,\n",
       " 'what is the sodium level in white bread': 1,\n",
       " 'how many electrons and protons and neutrons in gold?': 1,\n",
       " 'where did the ottomans deport the armenians': 1,\n",
       " 'when did hunger games book come out': 1,\n",
       " 'number of hershey kisses in a bag': 1,\n",
       " 'what are hamitic peoples': 1,\n",
       " 'largest cathedral in world': 1,\n",
       " 'what movie does dumbledore fight voldemort in': 1,\n",
       " 'average salary seguin': 1,\n",
       " 'carbon reactivation facilities california': 1,\n",
       " 'what is a variable factor': 1,\n",
       " 'side effects for diclofenac topical': 1,\n",
       " 'where is decatur, il': 1,\n",
       " \"what's the relationship between a codon and an anticodon\": 1,\n",
       " 'quid pro quo definition in english': 1,\n",
       " 'military caisson definition': 1,\n",
       " 'is tom robinson found guilty': 1,\n",
       " 'who is jack valentine': 1,\n",
       " 'who started the tesla car company': 1,\n",
       " 'who must a company contribute to for a sep': 1,\n",
       " 'who presides over a senate trial trial after a president is impeached': 1,\n",
       " 'fec independent expenditure definition': 1,\n",
       " 'current time in oakland ca': 1,\n",
       " 'what is the bovine growth hormone': 1,\n",
       " 'euless texas is in what county?': 1,\n",
       " 'who was the lone survivor': 1,\n",
       " 'thirty one gifts corporate phone number': 1,\n",
       " 'when the northern hemisphere experiences summer, the southern hemisphere experiences': 1,\n",
       " 'was michael jordan a cub scout': 1,\n",
       " 'hamp modification': 1,\n",
       " 'how many oscars has peter jackson wolllnlll?l': 1,\n",
       " 'luggage that can withstand airline handling': 1,\n",
       " 'where is browerville mn': 1,\n",
       " 'how to use a selfie stick': 1,\n",
       " 'how many nba championships did the suns win': 1,\n",
       " 'what is the dependent type': 1,\n",
       " 'average gas costs in kentucky': 1,\n",
       " 'what does annual deductible mean': 1,\n",
       " 'when does water boil at sea level': 1,\n",
       " 'how many calories per gram are in fats': 1,\n",
       " 'pomegranate seeds calories': 1,\n",
       " 'vestibular hypofunction definition': 1,\n",
       " 'is holland the netherlands': 1,\n",
       " 'how many lieutenant colonels in us army': 1,\n",
       " 'what century was the bronze age': 1,\n",
       " 'vitamin d deficiency and skin lesions': 1,\n",
       " 'what did imus ranch sell for': 1,\n",
       " 'who is dakota meyer': 1,\n",
       " 'what are upright pertrified trees': 1,\n",
       " 'is the aluminum compound in antiperspirants carcinogenic': 1,\n",
       " 'when did ice cube start rapping': 1,\n",
       " 'who ran mexico for decades as a dictator?': 1,\n",
       " 'when was the national park system established': 1,\n",
       " 'who is the current general manager of the arizona diamondbacks': 1,\n",
       " 'what is risk management? give example': 1,\n",
       " 'what is a blast beat': 1,\n",
       " 'what tree has acorns?': 1,\n",
       " 'what is a fume hood used for in chemistry': 1,\n",
       " 'moneygram toll free number': 1,\n",
       " 'what foods contain histamines?': 1,\n",
       " 'does insulin give you constipation': 1,\n",
       " 'how long do you keep flag at half mast': 1,\n",
       " 'how to cook string beans': 1,\n",
       " 'symptoms of bulging cervical disc': 1,\n",
       " 'what is a dapple dog': 1,\n",
       " 'vvi airport code': 1,\n",
       " 'what county is cabazon, ca in': 1,\n",
       " 'what in skin care is linked to thyroid problems': 1,\n",
       " 'how much does a green anaconda weigh': 1,\n",
       " 'what is the purpose of a conceptual model for advanced practice nursing': 1,\n",
       " 'what part of the throat gets sore': 1,\n",
       " 'what is the real color of saturn': 1,\n",
       " 'what is the oldest dog in the world?': 1,\n",
       " 'what is the generic name for zanaflex': 1,\n",
       " 'what is probability biology': 1,\n",
       " 'carnelian color meaning': 1,\n",
       " 'are life insurance policies protected information?': 1,\n",
       " 'what are the basic assumptions economists have about individuals?': 1,\n",
       " 'how long do you cook pork tenderloin in oven': 1,\n",
       " 'what is rocket league trading': 1,\n",
       " 'what is spoc llc': 1,\n",
       " 'what types of earthquake waves usually cause the most destruction?': 1,\n",
       " 'how long does tramadol stay in the urine': 1,\n",
       " 'whose number of terms is limited to two': 1,\n",
       " 'what are honeycomb panels used for?': 1,\n",
       " 'what is cameroon most important resource': 1,\n",
       " 'definition of meat myoglobin': 1,\n",
       " 'what causes extreme knee pain': 1,\n",
       " 'task definition': 1,\n",
       " 'what county is needles ca in': 1,\n",
       " 'what is pimple': 1,\n",
       " 'what plants have edible vegetable leaves?': 1,\n",
       " 'does invisalign straighten crooked teeth': 1,\n",
       " 'how far is lake como to lugano switzerland': 1,\n",
       " 'what county is perris ca in': 1,\n",
       " 'what is a good body cleanse to use': 1,\n",
       " 'what are the causes of burning and weakness in the legs': 1,\n",
       " 'is sadie a nickname': 1,\n",
       " 'what causes hypoxemia in pulmonary edema': 1,\n",
       " 'population in naples florida': 1,\n",
       " 'hughes federal routing number': 1,\n",
       " 'what testing phase ensures that the code meets customer requirements': 1,\n",
       " 'what does shoulder mdi mean': 1,\n",
       " \"what's the fees on amazon to sell and policy\": 1,\n",
       " 'where is sustain health practice scott ling': 1,\n",
       " 'what does obesity ,e': 1,\n",
       " 'denora anodes': 1,\n",
       " 'what is the function of rotator cuff tendon': 1,\n",
       " 'what college did charlie puth go to': 1,\n",
       " 'what is full moon of august called': 1,\n",
       " 'equivalent what does it means': 1,\n",
       " 'can router slow internet': 1,\n",
       " 'what does it mean your body in ketosis': 1,\n",
       " 'what study for mets to brain': 1,\n",
       " 'what is the synonym for prudent': 1,\n",
       " 'definition diagram': 1,\n",
       " 'what part of the nervous system does adderall affect': 1,\n",
       " 'cabin fever meaning': 1,\n",
       " 'what does bmp test for': 1,\n",
       " 'who is the author of soccerland': 1,\n",
       " 'what is trump': 1,\n",
       " 'what state is st. cloud in': 1,\n",
       " 'what is the best merv rating for filters?': 1,\n",
       " 'lakegirl address': 1,\n",
       " 'what is the salary range of a dentist': 1,\n",
       " 'distance between cities amarillo tx and mora new mexico': 1,\n",
       " 'largest oyster ever': 1,\n",
       " 'nonspecific lupus symptoms': 1,\n",
       " 'temple university student population': 1,\n",
       " 'who invented the tabata exercises': 1,\n",
       " 'what causes a prickly itchy feeling all over': 1,\n",
       " 'what is the meaning of cross breeding': 1,\n",
       " 'jujube candy flavors': 1,\n",
       " 'what cisco ios': 1,\n",
       " 'did andy gibb die': 1,\n",
       " 'confirmation number definition': 1,\n",
       " 'what oscars has denzel washington won': 1,\n",
       " 'how long does it take to cook dry beans': 1,\n",
       " 'where is baby beach hi': 1,\n",
       " 'what does the rellis campus name mean': 1,\n",
       " 'distance phoenix to nogales': 1,\n",
       " 'in what county is honea path sc': 1,\n",
       " 'how many vessels in an umbilical cord': 1,\n",
       " 'westborough local history librarian': 1,\n",
       " 'define the term codon': 1,\n",
       " \"what's the difference between c++ and java\": 1,\n",
       " 'which way should solar panels face in the winter': 1,\n",
       " 'aol email helpline number': 1,\n",
       " 'what are accounts payable?': 1,\n",
       " 'what is a short term study abroad': 1,\n",
       " 'at what temperature is rump roast done': 1,\n",
       " 'what is the survival rate of total gastrectomy': 1,\n",
       " 'define unadjusted trial balance': 1,\n",
       " 'what is carbon monoxide made of': 1,\n",
       " 'webmd symptoms of lupus': 1,\n",
       " 'what is a podiatry': 1,\n",
       " 'what county is junction city ar in': 1,\n",
       " 'difference between discrete and process manufacturing': 1,\n",
       " 'cto systems meaning': 1,\n",
       " 'what are the tax benefits of a heloc': 1,\n",
       " 'what is the training wage': 1,\n",
       " 'what county is hinton wv located?': 1,\n",
       " 'what does a nickel l': 1,\n",
       " 'who is the judge michael day district court of south dakota': 1,\n",
       " 'what is the most commonly used temperature scale in the scientific community': 1,\n",
       " 'weather in fort lauderdale today': 1,\n",
       " 'confident man definition': 1,\n",
       " 'what are the effects of sediment transport': 1,\n",
       " \"what's the difference between racquetball and squash\": 1,\n",
       " 'what do usa flag represent': 1,\n",
       " 'causes of petechial hemorrhage': 1,\n",
       " 'what structure are located in the abdominal  cavity?': 1,\n",
       " 'what is the meaning of chicken rice?': 1,\n",
       " 'does ichigo lose his powers': 1,\n",
       " 'definition of tumor': 1,\n",
       " 'what is inference example': 1,\n",
       " 'how to become a real estate agent in louisiana': 1,\n",
       " 'can tmj cause your jaw lock': 1,\n",
       " 'what fraction of the light falling on a piece of photographic film is typically wasted': 1,\n",
       " 'can Climbing Roses grow as bush roses?': 1,\n",
       " 'what is the empirical formula for phosphorus selenide': 1,\n",
       " 'tricare service number': 1,\n",
       " 'another name for reaper': 1,\n",
       " 'what natural processes affect climate change': 1,\n",
       " 'when was the first united states quarter minted?': 1,\n",
       " 'what disease do roof rats cause': 1,\n",
       " \"what is donald's trump's iq\": 1,\n",
       " 'average time for an appraisal': 1,\n",
       " 'is keppra medication a barbiturate?': 1,\n",
       " 'what is beverly hills md': 1,\n",
       " 'how much protein in a cup of turkey breast': 1,\n",
       " 'what is sealaska corporation': 1,\n",
       " 'what is a gis?': 1,\n",
       " 'triads definition': 1,\n",
       " 'what is the properties of breadfruit flour': 1,\n",
       " 'what does the moon on your nails mean': 1,\n",
       " 'is the hawb the tracking number': 1,\n",
       " 'are you overweight for your age': 1,\n",
       " 'what is the normal range for urine sodium': 1,\n",
       " 'the most common serious knee ligament injury involves the _____.': 1,\n",
       " 'what is meant by a blind copy in an email': 1,\n",
       " 'who is your favourite drink?': 1,\n",
       " 'insertion point definition': 1,\n",
       " 'where is the enzyme lipase': 1,\n",
       " 'what is a guaranteed general fund investment': 1,\n",
       " 'what iowa law determines inheritance exceptions?': 1,\n",
       " 'do physicians pay for insurance from their salaries?': 1,\n",
       " 'what is the acceptance rate at wellesley': 1,\n",
       " 'what agency can i report a scammer concerning my computer': 1,\n",
       " 'how long is a typical car loan?': 1,\n",
       " 'where is wild creek reservoir': 1,\n",
       " 'what does replace in sql mean': 1,\n",
       " 'central city definition': 1,\n",
       " 'what does lsa stand for in oil and gas': 1,\n",
       " 'how to present a project charter': 1,\n",
       " 'largest shipping day of year': 1,\n",
       " 'why are lymph nodes swelling in growing area': 1,\n",
       " 'define coherent': 1,\n",
       " 'largest state in usa new twenty': 1,\n",
       " 'what is counter': 1,\n",
       " 'what is rohs testing': 1,\n",
       " 'how many kids does jack osbourne have': 1,\n",
       " 'what county is van nuys, ca in': 1,\n",
       " 'what is headlock elite supplement': 1,\n",
       " 'is cortana smart???': 1,\n",
       " 'why did italy attack ethiopia in 1935': 1,\n",
       " 'how is the gem painite made': 1,\n",
       " 'justification  definition literature': 1,\n",
       " 'what does clindamycin gel treat': 1,\n",
       " 'what does it mean by the right to redress': 1,\n",
       " 'which herbs are best for drying': 1,\n",
       " 'aneca federal credit union routing number': 1,\n",
       " 'age requirements for name change': 1,\n",
       " 'when was grand theft auto four released': 1,\n",
       " 'where is bulli creek queensland australia': 1,\n",
       " 'which is positive and negative': 1,\n",
       " 'what is barron': 1,\n",
       " 'chateau meaning': 1,\n",
       " 'when does the umbilical cord form during pregnancy': 1,\n",
       " 'cost of cbs all access on roku': 1,\n",
       " 'what are prime costs': 1,\n",
       " 'what type of brain tumor did craig shergold have': 1,\n",
       " 'how far deep to plant beet early wonder': 1,\n",
       " 'who was involved in the locarno treaties': 1,\n",
       " 'what is depreciation of products': 1,\n",
       " 'average salary of the mn orchestra': 1,\n",
       " ' Androgen receptor define': 1,\n",
       " 'what does the pythagorean theorem do': 1,\n",
       " 'what foods are good for lupus': 1,\n",
       " 'average oxygen level': 1,\n",
       " 'what is flared gas': 1,\n",
       " 'how does genome editing work': 1,\n",
       " 'what audience can you reach boosting post facebook': 1,\n",
       " 'what are some meat companies': 1,\n",
       " 'black sea definition': 1,\n",
       " 'types of honeysuckle': 1,\n",
       " 'what is irs individual taxpayer identification number (itin)': 1,\n",
       " 'share of cost': 1,\n",
       " 'capital expenditure definition example': 1,\n",
       " 'an eating disorder is characterized by _____.': 1,\n",
       " 'what is a sabbatical policy for nonprofit': 1,\n",
       " 'what is the tn sales tax on groceries': 1,\n",
       " 'what does provisioning a modem mean': 1,\n",
       " 'when was john t scopes trial': 1,\n",
       " 'how far back do employment background checks': 1,\n",
       " 'meaning of by the fall of': 1,\n",
       " 'are cold sores and fever blisters the same': 1,\n",
       " 'can phosphourous be in nucleic acids': 1,\n",
       " 'what are the food sources for astaxanthin': 1,\n",
       " 'who foundation for internal classification of diseases': 1,\n",
       " 'what does assessment center means': 1,\n",
       " 'tsca definition malpractice claim': 1,\n",
       " 'what the hell is salad fingers': 1,\n",
       " 'which therapy is most likely to be effective in treating patients with eating disorders?': 1,\n",
       " 'the definition of sustainability is': 1,\n",
       " 'calories burned using elliptical': 1,\n",
       " 'definition of a pest': 1,\n",
       " 'highest paid public employee states': 1,\n",
       " 'does motion sensor device have camera': 1,\n",
       " 'shopko kennewick address': 1,\n",
       " 'in what city lived nelson mandela': 1,\n",
       " 'what medications can cause low blood sugar': 1,\n",
       " \"who are harrison ford's children\": 1,\n",
       " 'where is mayflower arkansas': 1,\n",
       " 'what does a frito lay packaging machine operator do?': 1,\n",
       " 'what were the brothers grimm names': 1,\n",
       " 'what is the the speed of coffee lake processor': 1,\n",
       " 'what make up the pistil in the female flower': 1,\n",
       " 'what are boot devices?': 1,\n",
       " 'where is the house of myrtlewood': 1,\n",
       " \"what are thomas jefferson's kids named\": 1,\n",
       " 'how is delaware sales tax calculated': 1,\n",
       " 'is aspirin good for muscle aches': 1,\n",
       " 'how long take tylenol codeine to get out system': 1,\n",
       " 'what is the nba salary cap set at': 1,\n",
       " 'what is green soap used for': 1,\n",
       " 'what age do moles appear': 1,\n",
       " 'difference between sprain and strain': 1,\n",
       " 'advanced weighing technology definition': 1,\n",
       " 'what cable connects pcs to printers': 1,\n",
       " 'price on toyota corolla finder': 1,\n",
       " 'espn streaming price': 1,\n",
       " 'who is scarlett in total drama': 1,\n",
       " 'can you get refunded on an amazon prime subscription': 1,\n",
       " 'was there a true bagger vance': 1,\n",
       " 'california wildfire largest': 1,\n",
       " 'is beer or wine more fattening': 1,\n",
       " 'what is freeroll in poker tournaments': 1,\n",
       " '________ disparity refers to the slightly different view of the world that each eye receives.cyclopeanbinocularmonoculartrichromatic': 1,\n",
       " 'how muxch do cdl endorsement tests cost': 1,\n",
       " 'what are assets when applying for food stamps': 1,\n",
       " 'what is inductive': 1,\n",
       " 'why did franklin d roosevelt launch a new programs during his first hundred days in office': 1,\n",
       " 'define user maintenance': 1,\n",
       " 'what is the opposite of benefit?': 1,\n",
       " 'who is dr burnett': 1,\n",
       " 'where does lice come from': 1,\n",
       " 'can plywood be painted': 1,\n",
       " 'benefits of comcast spectacor': 1,\n",
       " 'what cell process requires energy': 1,\n",
       " 'causes of swelling in the feet and ankles': 1,\n",
       " 'what is used to treat hiv/aids?': 1,\n",
       " 'what city is roseanne': 1,\n",
       " 'when to repot boxwood bonsai': 1,\n",
       " 'parasympathetic dominance definition': 1,\n",
       " 'what is abbreviated dialing': 1,\n",
       " 'how to unlock duty roulette': 1,\n",
       " 'foods and supplements to lower blood sugar': 1,\n",
       " \"as scarce as hen's teeth meaning\": 1,\n",
       " 'why do people use gypsum in soil': 1,\n",
       " 'can chewing gum cause ulcers': 1,\n",
       " 'benefits of coconut oil for gout': 1,\n",
       " 'camille grammer height and weight': 1,\n",
       " 'does prilosec cause weight loss': 1,\n",
       " 'house is settling and outside wall is cracked': 1,\n",
       " 'how many times the normal radiation': 1,\n",
       " 'what county is hixson, tn': 1,\n",
       " 'causes of hand, leg numbness': 1,\n",
       " 'windstream troubleshooting phone number': 1,\n",
       " 'what are botulinum toxins currently most commonly used for?': 1,\n",
       " 'calories crab meat': 1,\n",
       " 'what is the legal definition of deposit': 1,\n",
       " 'how are blood borne viruses spread': 1,\n",
       " 'what antihistamines will reduce redness and swelling': 1,\n",
       " 'which rotator cuff muscle originates on the subscapular fossa of the scapula and inserts on the lesser tubercle of the humerus?': 1,\n",
       " 'what are some operating systems for computers': 1,\n",
       " 'what methodology is used for siemens rxl': 1,\n",
       " 'iis post size limit': 1,\n",
       " 'how many square feet does a student need?': 1,\n",
       " 'what is the gpa requirement for usd': 1,\n",
       " 'what is the plural of kohlrabies': 1,\n",
       " 'what causes twitching of eyelid': 1,\n",
       " 'is term gpa and semester gpa the same thing': 1,\n",
       " \"what's the thirteenth letter of the english alphabet\": 1,\n",
       " 'what is a ds1 cable': 1,\n",
       " 'what causes feet swelling when riding in car': 1,\n",
       " 'where is the new sheet button in excel': 1,\n",
       " 'what county is holliston ma': 1,\n",
       " 'can inheritance monies be taken by the trustee in bankruptcy': 1,\n",
       " 'what is psychic remote viewing': 1,\n",
       " 'who created school for feeble minded': 1,\n",
       " 'what is maxim': 1,\n",
       " 'which contestant on the bachelor was missing': 1,\n",
       " 'benefits of noni soap': 1,\n",
       " 'what county is pennsboro wv in?': 1,\n",
       " 'pregnancy symptoms by day': 1,\n",
       " 'is thread.sleep implicit wait': 1,\n",
       " 'when was niagara falls created': 1,\n",
       " 'what is body hives': 1,\n",
       " 'is rensselaer polytechnic institute': 1,\n",
       " 'who does cameron boyce play in liv and maddie': 1,\n",
       " 'how many calories in a medium smoothie king smoothie': 1,\n",
       " 'what is the restaurant in seinfeld': 1,\n",
       " 'when and where did paella originate?': 1,\n",
       " 'what is the geography of china like': 1,\n",
       " 'what is an acl': 1,\n",
       " 'what causes subchorionic hemorrhage': 1,\n",
       " 'what rights cannot be taken away stated in the declaration of independence': 1,\n",
       " 'unemployment rate honolulu hawaii': 1,\n",
       " 'tobira stock price': 1,\n",
       " 'what is a wellness champion': 1,\n",
       " 'laws synonyms': 1,\n",
       " 'what is nabumetone used to treat': 1,\n",
       " 'java what is a short': 1,\n",
       " 'alabama central credit union routing number': 1,\n",
       " 'what are the four steps in the machine cycle': 1,\n",
       " 'what are the symptoms of preeclampsia and eclampsia': 1,\n",
       " \"parkinson's disease symptoms depression\": 1,\n",
       " 'at what age does the average person retire': 1,\n",
       " 'what is an rs in money': 1,\n",
       " 'what is stop order process': 1,\n",
       " 'how many episodes is there in naruto jump': 1,\n",
       " 'who wrote good will hunting': 1,\n",
       " 'what type of beef is used for beef & broccoli': 1,\n",
       " 'madison county tn population': 1,\n",
       " 'what did wells fargo do wrong': 1,\n",
       " 'why hemorrhagic disease tests due to vitamin k deficiency in infancy': 1,\n",
       " 'valve of hasner': 1,\n",
       " 'where does spermatogenesis occur?': 1,\n",
       " 'what is the length of a runway': 1,\n",
       " 'calories burrito supreme': 1,\n",
       " 'rna primase definition biology': 1,\n",
       " 'is the teachers salary better in a charter school': 1,\n",
       " 'hp envy size': 1,\n",
       " 'how much grow light does succulents need': 1,\n",
       " 'what is el metate': 1,\n",
       " 'how long does a us passport need to be valid to visit germany': 1,\n",
       " 'foods to raise my creatinine level': 1,\n",
       " 'webster definition of mourning': 1,\n",
       " 'when to aerate grass': 1,\n",
       " 'time to cut knock out roses': 1,\n",
       " 'how many calories in skyy vodka': 1,\n",
       " 'when is flipping out next season': 1,\n",
       " 'how many of grams a sugar should a person have in one day?': 1,\n",
       " 'forget paris cast': 1,\n",
       " 'who developed the first photograph': 1,\n",
       " 'how to change ip address on netgear router': 1,\n",
       " 'how much does the average professional nba player make': 1,\n",
       " 'how much is donald trump jr worth': 1}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_count_dict_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "816"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_count_dict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgroup_train = []\n",
    "dgroup_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker = []\n",
    "for idx, row in train_data.iterrows():\n",
    "    query = row['queries']\n",
    "    if query in checker:\n",
    "        continue\n",
    "    checker.append(query)\n",
    "    group_entry_count = query_count_dict_train[query]\n",
    "    dgroup_train.append(group_entry_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dgroup_train = np.array(dgroup_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgroup_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(dgroup_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>queries</th>\n",
       "      <th>passage</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>passage_cleaned</th>\n",
       "      <th>query_cleaned</th>\n",
       "      <th>co_similarity</th>\n",
       "      <th>bm25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>1101806</td>\n",
       "      <td>1046669</td>\n",
       "      <td>wow essential oil</td>\n",
       "      <td>Use approximately six drops of essential oil f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[use, approximately, six, drop, essential, oil...</td>\n",
       "      <td>[wow, essential, oil]</td>\n",
       "      <td>0.644733</td>\n",
       "      <td>24.310915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>1078446</td>\n",
       "      <td>1050857</td>\n",
       "      <td>wine cabinets definition</td>\n",
       "      <td>Terroir definition, the environmental conditio...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[terroir, definition, environmental, condition...</td>\n",
       "      <td>[wine, cabinets, definition]</td>\n",
       "      <td>0.550765</td>\n",
       "      <td>17.995400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1101868</td>\n",
       "      <td>1037249</td>\n",
       "      <td>willie weeks net worth</td>\n",
       "      <td>Kirk Frost net worth: $600 Thousand. Kirk Fros...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[kirk, frost, net, worth, thousand, kirk, fros...</td>\n",
       "      <td>[willie, weeks, net, worth]</td>\n",
       "      <td>0.661165</td>\n",
       "      <td>25.569472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>1101868</td>\n",
       "      <td>1059211</td>\n",
       "      <td>willie weeks net worth</td>\n",
       "      <td>Updated Constance McCashin Net Worth in 2017. ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[update, constance, mccashin, net, worth, wiki...</td>\n",
       "      <td>[willie, weeks, net, worth]</td>\n",
       "      <td>0.684801</td>\n",
       "      <td>25.771400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>1101868</td>\n",
       "      <td>1059337</td>\n",
       "      <td>willie weeks net worth</td>\n",
       "      <td>The Leonardo DiCaprio net worth total of $217 ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[leonardo, dicaprio, net, worth, total, millio...</td>\n",
       "      <td>[willie, weeks, net, worth]</td>\n",
       "      <td>0.812987</td>\n",
       "      <td>27.338163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>14947</td>\n",
       "      <td>1063503</td>\n",
       "      <td>airport code mont tremblant</td>\n",
       "      <td>Cities &gt; Norway &gt; Airports near Ølen. The clos...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[cities, norway, airports, near, ølen, closest...</td>\n",
       "      <td>[airport, code, mont, tremblant]</td>\n",
       "      <td>0.559477</td>\n",
       "      <td>20.494116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>10264</td>\n",
       "      <td>1040388</td>\n",
       "      <td>access parallels cost</td>\n",
       "      <td>Some are available every day of the year. At t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[available, every, day, year, beach, surf, wat...</td>\n",
       "      <td>[access, parallel, cost]</td>\n",
       "      <td>0.341840</td>\n",
       "      <td>8.133568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>1099488</td>\n",
       "      <td>1062356</td>\n",
       "      <td>a scar meaning</td>\n",
       "      <td>Last Week's Popular Questions for Celebrities ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[last, week, popular, question, celebrities, a...</td>\n",
       "      <td>[scar, mean]</td>\n",
       "      <td>0.646320</td>\n",
       "      <td>11.629403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>1099488</td>\n",
       "      <td>1062358</td>\n",
       "      <td>a scar meaning</td>\n",
       "      <td>Last Week's Popular Questions for Celebrities....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[last, week, popular, question, celebrities, a...</td>\n",
       "      <td>[scar, mean]</td>\n",
       "      <td>0.590652</td>\n",
       "      <td>10.590634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>4696</td>\n",
       "      <td>1015723</td>\n",
       "      <td>Is the Louisiana sales tax 4.75</td>\n",
       "      <td>The Livingston Sales Tax is collected by the m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[livingston, sales, tax, collect, merchant, qu...</td>\n",
       "      <td>[louisiana, sales, tax]</td>\n",
       "      <td>0.843126</td>\n",
       "      <td>27.049819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         qid      pid                          queries  \\\n",
       "719  1101806  1046669                wow essential oil   \n",
       "774  1078446  1050857         wine cabinets definition   \n",
       "567  1101868  1037249           willie weeks net worth   \n",
       "907  1101868  1059211           willie weeks net worth   \n",
       "915  1101868  1059337           willie weeks net worth   \n",
       "..       ...      ...                              ...   \n",
       "996    14947  1063503      airport code mont tremblant   \n",
       "627    10264  1040388            access parallels cost   \n",
       "968  1099488  1062356                   a scar meaning   \n",
       "969  1099488  1062358                   a scar meaning   \n",
       "237     4696  1015723  Is the Louisiana sales tax 4.75   \n",
       "\n",
       "                                               passage  relevancy  \\\n",
       "719  Use approximately six drops of essential oil f...        0.0   \n",
       "774  Terroir definition, the environmental conditio...        0.0   \n",
       "567  Kirk Frost net worth: $600 Thousand. Kirk Fros...        0.0   \n",
       "907  Updated Constance McCashin Net Worth in 2017. ...        0.0   \n",
       "915  The Leonardo DiCaprio net worth total of $217 ...        0.0   \n",
       "..                                                 ...        ...   \n",
       "996  Cities > Norway > Airports near Ølen. The clos...        0.0   \n",
       "627  Some are available every day of the year. At t...        0.0   \n",
       "968  Last Week's Popular Questions for Celebrities ...        0.0   \n",
       "969  Last Week's Popular Questions for Celebrities....        0.0   \n",
       "237  The Livingston Sales Tax is collected by the m...        0.0   \n",
       "\n",
       "                                       passage_cleaned  \\\n",
       "719  [use, approximately, six, drop, essential, oil...   \n",
       "774  [terroir, definition, environmental, condition...   \n",
       "567  [kirk, frost, net, worth, thousand, kirk, fros...   \n",
       "907  [update, constance, mccashin, net, worth, wiki...   \n",
       "915  [leonardo, dicaprio, net, worth, total, millio...   \n",
       "..                                                 ...   \n",
       "996  [cities, norway, airports, near, ølen, closest...   \n",
       "627  [available, every, day, year, beach, surf, wat...   \n",
       "968  [last, week, popular, question, celebrities, a...   \n",
       "969  [last, week, popular, question, celebrities, a...   \n",
       "237  [livingston, sales, tax, collect, merchant, qu...   \n",
       "\n",
       "                        query_cleaned  co_similarity       bm25  \n",
       "719             [wow, essential, oil]       0.644733  24.310915  \n",
       "774      [wine, cabinets, definition]       0.550765  17.995400  \n",
       "567       [willie, weeks, net, worth]       0.661165  25.569472  \n",
       "907       [willie, weeks, net, worth]       0.684801  25.771400  \n",
       "915       [willie, weeks, net, worth]       0.812987  27.338163  \n",
       "..                                ...            ...        ...  \n",
       "996  [airport, code, mont, tremblant]       0.559477  20.494116  \n",
       "627          [access, parallel, cost]       0.341840   8.133568  \n",
       "968                      [scar, mean]       0.646320  11.629403  \n",
       "969                      [scar, mean]       0.590652  10.590634  \n",
       "237           [louisiana, sales, tax]       0.843126  27.049819  \n",
       "\n",
       "[1000 rows x 9 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data = validation_data.sort_values(by=['queries'], ascending=False)\n",
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdaMart_x_val = np.zeros((validation_data.shape[0], num_of_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.64473255, 24.31091456],\n",
       "       [ 0.55076461, 17.99540048],\n",
       "       [ 0.66116479, 25.56947225],\n",
       "       ...,\n",
       "       [ 0.64632024, 11.62940342],\n",
       "       [ 0.59065184, 10.59063425],\n",
       "       [ 0.84312551, 27.049819  ]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdaMart_x_val = validation_data[['co_similarity', 'bm25']].values\n",
    "lambdaMart_x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdaMart_y_val = validation_data.relevancy.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_count_dict_val = validation_data['queries'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgroup_val = []\n",
    "dgroup_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker = []\n",
    "for idx, row in validation_data.iterrows():\n",
    "    query = row['queries']\n",
    "    if query in checker:\n",
    "        continue\n",
    "    checker.append(query)\n",
    "    group_entry_count = query_count_dict_val[query]\n",
    "    dgroup_val.append(group_entry_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dgroup_val = np.array(dgroup_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgroup_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LambdaMart Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_rank_params1 ={    \n",
    "#     'booster' : 'gbtree',\n",
    "#     'eta': 0.1,\n",
    "#     'gamma' : 1.0 ,\n",
    "#     'min_child_weight' : 0.1,\n",
    "#     'objective' : 'rank:pairwise',\n",
    "#     'eval_metric' : 'merror',\n",
    "#     'max_depth' : 6,\n",
    "#     'num_boost_round':10,\n",
    "#     'save_period' : 0 \n",
    "# }\n",
    "\n",
    "# params = {\n",
    "#     'bst:max_depth':2, \n",
    "#     'bst:eta':1, 'silent':1, \n",
    "#     'objective':'rank:pairwise',\n",
    "#     'nthread':4,\n",
    "#     'eval_metric':'ndcg'\n",
    "# }\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate training dataset\n",
    "# dtrain= lambdaMart_x_train\n",
    "# dtarget= lambdaMart_y_train\n",
    "# dgroup_train = dgroup_train\n",
    "# dtrain= lambdaMart_x_train[:2]\n",
    "# dtarget= lambdaMart_y_train[:2]\n",
    "# dgroup_train = dgroup_train[:2]\n",
    "# dtrain.shape\n",
    "# dtarget.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.65264477, 20.73309484],\n",
       "       [ 0.47845439, 16.04187895],\n",
       "       [ 0.6930888 , 14.63510436],\n",
       "       ...,\n",
       "       [ 0.52710561, 15.64887427],\n",
       "       [ 0.50842772, 10.67230032],\n",
       "       [ 0.78260784, 13.88766498]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdaMart_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgroup_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate Train data, very import here !\n",
    "\n",
    "train_dmatrix = DMatrix(lambdaMart_x_train, label = lambdaMart_y_train)\n",
    "valid_dmatrix = DMatrix(lambdaMart_x_val, label = lambdaMart_y_val)\n",
    "train_dmatrix.set_group(dgroup_train)\n",
    "valid_dmatrix.set_group(dgroup_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'objective': 'rank:ndcg', 'eta': 0.1, 'gamma': 1.0,\n",
    "          'min_child_weight': 0.1, 'max_depth': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-map:1\n",
      "[1]\tvalidation-map:1\n",
      "[2]\tvalidation-map:1\n",
      "[3]\tvalidation-map:1\n"
     ]
    }
   ],
   "source": [
    "lambdaRank_model = train(params, train_dmatrix, num_boost_round=4,\n",
    "                      evals=[(valid_dmatrix, 'validation')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = lambdaRank_model.predict(test_dmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate eval data -> validation data\n",
    "# dtrain_eval= lambdaMart_x_val   \n",
    "# xgbTrain_eval = DMatrix(lambdaMart_x_val, label = lambdaMart_y_val)\n",
    "\n",
    "# evallist  = [(xgbTrain,'train'),(xgbTrain_eval, 'eval')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-ndcg:1\teval-ndcg:1\n",
      "[1]\ttrain-ndcg:1\teval-ndcg:1\n",
      "[2]\ttrain-ndcg:1\teval-ndcg:1\n",
      "[3]\ttrain-ndcg:1\teval-ndcg:1\n",
      "[4]\ttrain-ndcg:1\teval-ndcg:1\n",
      "[5]\ttrain-ndcg:1\teval-ndcg:1\n",
      "[6]\ttrain-ndcg:1\teval-ndcg:1\n",
      "[7]\ttrain-ndcg:1\teval-ndcg:1\n",
      "[8]\ttrain-ndcg:1\teval-ndcg:1\n",
      "[9]\ttrain-ndcg:1\teval-ndcg:1\n",
      "[10]\ttrain-ndcg:1\teval-ndcg:1\n",
      "[11]\ttrain-ndcg:1\teval-ndcg:1\n",
      "[12]\ttrain-ndcg:1\teval-ndcg:1\n",
      "[13]\ttrain-ndcg:1\teval-ndcg:1\n",
      "[14]\ttrain-ndcg:1\teval-ndcg:1\n",
      "[15]\ttrain-ndcg:1\teval-ndcg:1\n",
      "[16]\ttrain-ndcg:1\teval-ndcg:1\n",
      "[17]\ttrain-ndcg:1\teval-ndcg:1\n",
      "[18]\ttrain-ndcg:1\teval-ndcg:1\n",
      "[19]\ttrain-ndcg:1\teval-ndcg:1\n"
     ]
    }
   ],
   "source": [
    "# # train model\n",
    "# # xgb_rank_params1加上 evals 这个参数会报错，还没找到原因\n",
    "# # rankModel = train(xgb_rank_params1,xgbTrain,num_boost_round=10)\n",
    "# rankModel = train(params,xgbTrain,num_boost_round=20,evals=evallist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test dataset\n",
    "# dtest= lambdaMart_x_val  \n",
    "# dtestgroup=dgroup_val\n",
    "# xgbTest = DMatrix(dtest)\n",
    "# xgbTest.set_group(dgroup_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "# print(rankModel.predict(xgbTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, activations, losses, Model, Input\n",
    "from tensorflow.nn import leaky_relu\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from tensorflow.keras.utils import plot_model, Progbar\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# model architecture\n",
    "class RankNet(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = [layers.Dense(16, activation=leaky_relu), layers.Dense(8, activation=leaky_relu)]\n",
    "        self.o = layers.Dense(1, activation='linear')\n",
    "        self.oi_minus_oj = layers.Subtract()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        xi, xj = inputs\n",
    "        densei = self.dense[0](xi)\n",
    "        densej = self.dense[0](xj)\n",
    "        for dense in self.dense[1:]:\n",
    "            densei = dense(densei)\n",
    "            densej = dense(densej)\n",
    "        oi = self.o(densei)\n",
    "        oj= self.o(densej)\n",
    "        oij = self.oi_minus_oj([oi, oj])\n",
    "        output = layers.Activation('sigmoid')(oij)\n",
    "        return output\n",
    "    \n",
    "    def build_graph(self):\n",
    "        x = [Input(shape=(10)), Input(shape=(10))]\n",
    "        return Model(inputs=x, outputs=self.call(x))\n",
    "\n",
    "# visualize model architecture\n",
    "# plot_model(RankNet().build_graph(), show_shapes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>queries</th>\n",
       "      <th>passage</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>passage_cleaned</th>\n",
       "      <th>query_cleaned</th>\n",
       "      <th>co_similarity</th>\n",
       "      <th>bm25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188714</td>\n",
       "      <td>1000052</td>\n",
       "      <td>foods and supplements to lower blood sugar</td>\n",
       "      <td>Watch portion sizes: ■ Even healthy foods will...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[watch, portion, size, even, healthy, foods, c...</td>\n",
       "      <td>[foods, supplement, lower, blood, sugar]</td>\n",
       "      <td>0.812210</td>\n",
       "      <td>28.648708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>995526</td>\n",
       "      <td>1000094</td>\n",
       "      <td>where is the federal penitentiary in ind</td>\n",
       "      <td>It takes THOUSANDS of Macy's associates to bri...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[take, thousands, macy, associate, bring, magi...</td>\n",
       "      <td>[federal, penitentiary, ind]</td>\n",
       "      <td>0.345645</td>\n",
       "      <td>8.308349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660957</td>\n",
       "      <td>1000115</td>\n",
       "      <td>what foods are good if you have gout?</td>\n",
       "      <td>The good news is that you will discover what g...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[good, news, discover, go, action, spur, narro...</td>\n",
       "      <td>[foods, good, gout]</td>\n",
       "      <td>0.640991</td>\n",
       "      <td>21.800407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>837202</td>\n",
       "      <td>1000252</td>\n",
       "      <td>what is the nutritional value of oatmeal</td>\n",
       "      <td>Oats make an easy, balanced breakfast. One cup...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[oats, make, easy, balance, breakfast, one, cu...</td>\n",
       "      <td>[nutritional, value, oatmeal]</td>\n",
       "      <td>0.670880</td>\n",
       "      <td>9.377437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130825</td>\n",
       "      <td>1000268</td>\n",
       "      <td>definition for daring</td>\n",
       "      <td>Such a requirement would have three desirable ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[requirement, would, three, desirable, consequ...</td>\n",
       "      <td>[definition, dare]</td>\n",
       "      <td>0.386194</td>\n",
       "      <td>5.959412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>400803</td>\n",
       "      <td>1016366</td>\n",
       "      <td>is a revocable trust a separate legal entity</td>\n",
       "      <td>The income and deductions of the trust are rep...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[income, deductions, trust, report, income, ta...</td>\n",
       "      <td>[revocable, trust, separate, legal, entity]</td>\n",
       "      <td>0.667951</td>\n",
       "      <td>21.828710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>400803</td>\n",
       "      <td>1016370</td>\n",
       "      <td>is a revocable trust a separate legal entity</td>\n",
       "      <td>A grantor trust is a living revocable trust in...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[grantor, trust, live, revocable, trust, grant...</td>\n",
       "      <td>[revocable, trust, separate, legal, entity]</td>\n",
       "      <td>0.699175</td>\n",
       "      <td>22.962073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>544319</td>\n",
       "      <td>1016449</td>\n",
       "      <td>weather in gig harbor, wa</td>\n",
       "      <td>The gig economy is the collection of markets t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[gig, economy, collection, market, match, prov...</td>\n",
       "      <td>[weather, gig, harbor, wa]</td>\n",
       "      <td>0.383561</td>\n",
       "      <td>11.130238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>617246</td>\n",
       "      <td>1016466</td>\n",
       "      <td>what decisions rules can determine upheld or d...</td>\n",
       "      <td>To claim a tax deduction for business mileage,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[claim, tax, deduction, business, mileage, sel...</td>\n",
       "      <td>[decisions, rule, determine, uphold, dismiss, ...</td>\n",
       "      <td>0.534107</td>\n",
       "      <td>17.704609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1097069</td>\n",
       "      <td>1016480</td>\n",
       "      <td>average gas costs in kentucky</td>\n",
       "      <td>Over the next 30 months, however, the average ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[next, months, however, average, cost, clients...</td>\n",
       "      <td>[average, gas, cost, kentucky]</td>\n",
       "      <td>0.577678</td>\n",
       "      <td>22.996400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         qid      pid                                            queries  \\\n",
       "0     188714  1000052         foods and supplements to lower blood sugar   \n",
       "1     995526  1000094           where is the federal penitentiary in ind   \n",
       "2     660957  1000115              what foods are good if you have gout?   \n",
       "3     837202  1000252           what is the nutritional value of oatmeal   \n",
       "4     130825  1000268                              definition for daring   \n",
       "..       ...      ...                                                ...   \n",
       "995   400803  1016366       is a revocable trust a separate legal entity   \n",
       "996   400803  1016370       is a revocable trust a separate legal entity   \n",
       "997   544319  1016449                          weather in gig harbor, wa   \n",
       "998   617246  1016466  what decisions rules can determine upheld or d...   \n",
       "999  1097069  1016480                      average gas costs in kentucky   \n",
       "\n",
       "                                               passage  relevancy  \\\n",
       "0    Watch portion sizes: ■ Even healthy foods will...        0.0   \n",
       "1    It takes THOUSANDS of Macy's associates to bri...        0.0   \n",
       "2    The good news is that you will discover what g...        0.0   \n",
       "3    Oats make an easy, balanced breakfast. One cup...        0.0   \n",
       "4    Such a requirement would have three desirable ...        0.0   \n",
       "..                                                 ...        ...   \n",
       "995  The income and deductions of the trust are rep...        0.0   \n",
       "996  A grantor trust is a living revocable trust in...        0.0   \n",
       "997  The gig economy is the collection of markets t...        0.0   \n",
       "998  To claim a tax deduction for business mileage,...        0.0   \n",
       "999  Over the next 30 months, however, the average ...        0.0   \n",
       "\n",
       "                                       passage_cleaned  \\\n",
       "0    [watch, portion, size, even, healthy, foods, c...   \n",
       "1    [take, thousands, macy, associate, bring, magi...   \n",
       "2    [good, news, discover, go, action, spur, narro...   \n",
       "3    [oats, make, easy, balance, breakfast, one, cu...   \n",
       "4    [requirement, would, three, desirable, consequ...   \n",
       "..                                                 ...   \n",
       "995  [income, deductions, trust, report, income, ta...   \n",
       "996  [grantor, trust, live, revocable, trust, grant...   \n",
       "997  [gig, economy, collection, market, match, prov...   \n",
       "998  [claim, tax, deduction, business, mileage, sel...   \n",
       "999  [next, months, however, average, cost, clients...   \n",
       "\n",
       "                                         query_cleaned  co_similarity  \\\n",
       "0             [foods, supplement, lower, blood, sugar]       0.812210   \n",
       "1                         [federal, penitentiary, ind]       0.345645   \n",
       "2                                  [foods, good, gout]       0.640991   \n",
       "3                        [nutritional, value, oatmeal]       0.670880   \n",
       "4                                   [definition, dare]       0.386194   \n",
       "..                                                 ...            ...   \n",
       "995        [revocable, trust, separate, legal, entity]       0.667951   \n",
       "996        [revocable, trust, separate, legal, entity]       0.699175   \n",
       "997                         [weather, gig, harbor, wa]       0.383561   \n",
       "998  [decisions, rule, determine, uphold, dismiss, ...       0.534107   \n",
       "999                     [average, gas, cost, kentucky]       0.577678   \n",
       "\n",
       "          bm25  \n",
       "0    28.648708  \n",
       "1     8.308349  \n",
       "2    21.800407  \n",
       "3     9.377437  \n",
       "4     5.959412  \n",
       "..         ...  \n",
       "995  21.828710  \n",
       "996  22.962073  \n",
       "997  11.130238  \n",
       "998  17.704609  \n",
       "999  22.996400  \n",
       "\n",
       "[1000 rows x 9 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>queries</th>\n",
       "      <th>passage</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>passage_cleaned</th>\n",
       "      <th>query_cleaned</th>\n",
       "      <th>co_similarity</th>\n",
       "      <th>bm25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1082792</td>\n",
       "      <td>1000084</td>\n",
       "      <td>what does the golgi apparatus do to the protei...</td>\n",
       "      <td>Start studying Bonding, Carbs, Proteins, Lipid...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[start, study, bond, carbs, proteins, lipids, ...</td>\n",
       "      <td>[golgi, apparatus, proteins, lipids, arrive]</td>\n",
       "      <td>0.609541</td>\n",
       "      <td>19.798340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>995825</td>\n",
       "      <td>1000492</td>\n",
       "      <td>where is the graphic card located in the cpu</td>\n",
       "      <td>For example, a “PC Expansion Card” maybe the j...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[example, pc, expansion, card, maybe, jargon, ...</td>\n",
       "      <td>[graphic, card, locate, cpu]</td>\n",
       "      <td>0.675188</td>\n",
       "      <td>19.714530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>995825</td>\n",
       "      <td>1000494</td>\n",
       "      <td>where is the graphic card located in the cpu</td>\n",
       "      <td>The Common Cards &amp; Buses. The most common type...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[common, card, bus, common, type, expansion, c...</td>\n",
       "      <td>[graphic, card, locate, cpu]</td>\n",
       "      <td>0.762009</td>\n",
       "      <td>20.945883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1091246</td>\n",
       "      <td>1000522</td>\n",
       "      <td>property premises meaning</td>\n",
       "      <td>The occurrence of since tells us that the firs...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[occurrence, since, tell, us, first, statement...</td>\n",
       "      <td>[property, premise, mean]</td>\n",
       "      <td>0.561729</td>\n",
       "      <td>12.263720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1047854</td>\n",
       "      <td>1000585</td>\n",
       "      <td>what is printing mechanism</td>\n",
       "      <td>Windows desktop applications Develop Desktop t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[windows, desktop, applications, develop, desk...</td>\n",
       "      <td>[print, mechanism]</td>\n",
       "      <td>0.612431</td>\n",
       "      <td>13.277623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1069028</td>\n",
       "      <td>1063432</td>\n",
       "      <td>what is a preliminary source</td>\n",
       "      <td>In September 22 1862, after the Union's victor...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[september, union, victory, antietam, lincoln,...</td>\n",
       "      <td>[preliminary, source]</td>\n",
       "      <td>0.296364</td>\n",
       "      <td>9.579258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>14947</td>\n",
       "      <td>1063503</td>\n",
       "      <td>airport code mont tremblant</td>\n",
       "      <td>Cities &gt; Norway &gt; Airports near Ølen. The clos...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[cities, norway, airports, near, ølen, closest...</td>\n",
       "      <td>[airport, code, mont, tremblant]</td>\n",
       "      <td>0.559477</td>\n",
       "      <td>20.494116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1036002</td>\n",
       "      <td>1063567</td>\n",
       "      <td>who is melvin booker</td>\n",
       "      <td>Double or Nothing, a song by B.o.B and Big Boi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[double, nothing, song, bob, big, boi, album, ...</td>\n",
       "      <td>[melvin, booker]</td>\n",
       "      <td>0.396677</td>\n",
       "      <td>6.983769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>731736</td>\n",
       "      <td>1063649</td>\n",
       "      <td>what is coastal erosion</td>\n",
       "      <td>1.1 DEFINING COASTAL AREAS. Coastal areas are ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[define, coastal, areas, coastal, areas, commo...</td>\n",
       "      <td>[coastal, erosion]</td>\n",
       "      <td>0.570154</td>\n",
       "      <td>12.559034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1056726</td>\n",
       "      <td>1063656</td>\n",
       "      <td>what is diagnosis of engineering</td>\n",
       "      <td>Structural engineers often specialize in parti...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[structural, engineer, often, specialize, part...</td>\n",
       "      <td>[diagnosis, engineer]</td>\n",
       "      <td>0.556620</td>\n",
       "      <td>13.463982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         qid      pid                                            queries  \\\n",
       "0    1082792  1000084  what does the golgi apparatus do to the protei...   \n",
       "1     995825  1000492       where is the graphic card located in the cpu   \n",
       "2     995825  1000494       where is the graphic card located in the cpu   \n",
       "3    1091246  1000522                          property premises meaning   \n",
       "4    1047854  1000585                         what is printing mechanism   \n",
       "..       ...      ...                                                ...   \n",
       "995  1069028  1063432                       what is a preliminary source   \n",
       "996    14947  1063503                        airport code mont tremblant   \n",
       "997  1036002  1063567                               who is melvin booker   \n",
       "998   731736  1063649                            what is coastal erosion   \n",
       "999  1056726  1063656                   what is diagnosis of engineering   \n",
       "\n",
       "                                               passage  relevancy  \\\n",
       "0    Start studying Bonding, Carbs, Proteins, Lipid...        0.0   \n",
       "1    For example, a “PC Expansion Card” maybe the j...        0.0   \n",
       "2    The Common Cards & Buses. The most common type...        0.0   \n",
       "3    The occurrence of since tells us that the firs...        0.0   \n",
       "4    Windows desktop applications Develop Desktop t...        0.0   \n",
       "..                                                 ...        ...   \n",
       "995  In September 22 1862, after the Union's victor...        0.0   \n",
       "996  Cities > Norway > Airports near Ølen. The clos...        0.0   \n",
       "997  Double or Nothing, a song by B.o.B and Big Boi...        0.0   \n",
       "998  1.1 DEFINING COASTAL AREAS. Coastal areas are ...        0.0   \n",
       "999  Structural engineers often specialize in parti...        0.0   \n",
       "\n",
       "                                       passage_cleaned  \\\n",
       "0    [start, study, bond, carbs, proteins, lipids, ...   \n",
       "1    [example, pc, expansion, card, maybe, jargon, ...   \n",
       "2    [common, card, bus, common, type, expansion, c...   \n",
       "3    [occurrence, since, tell, us, first, statement...   \n",
       "4    [windows, desktop, applications, develop, desk...   \n",
       "..                                                 ...   \n",
       "995  [september, union, victory, antietam, lincoln,...   \n",
       "996  [cities, norway, airports, near, ølen, closest...   \n",
       "997  [double, nothing, song, bob, big, boi, album, ...   \n",
       "998  [define, coastal, areas, coastal, areas, commo...   \n",
       "999  [structural, engineer, often, specialize, part...   \n",
       "\n",
       "                                    query_cleaned  co_similarity       bm25  \n",
       "0    [golgi, apparatus, proteins, lipids, arrive]       0.609541  19.798340  \n",
       "1                    [graphic, card, locate, cpu]       0.675188  19.714530  \n",
       "2                    [graphic, card, locate, cpu]       0.762009  20.945883  \n",
       "3                       [property, premise, mean]       0.561729  12.263720  \n",
       "4                              [print, mechanism]       0.612431  13.277623  \n",
       "..                                            ...            ...        ...  \n",
       "995                         [preliminary, source]       0.296364   9.579258  \n",
       "996              [airport, code, mont, tremblant]       0.559477  20.494116  \n",
       "997                              [melvin, booker]       0.396677   6.983769  \n",
       "998                            [coastal, erosion]       0.570154  12.559034  \n",
       "999                         [diagnosis, engineer]       0.556620  13.463982  \n",
       "\n",
       "[1000 rows x 9 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "qids = train_data.qid.values\n",
    "doc_features = train_data[['co_similarity', 'bm25']].values\n",
    "doc_scoures = train_data.relevancy.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(816,)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(qids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "# nb_query = 20\n",
    "# query = np.array([i+1 for i in range(nb_query) for x in range(int(np.ceil(np.abs(np.random.normal(0,scale=15))+2)))])\n",
    "# doc_features = np.random.random((len(query), 10))\n",
    "# doc_scores = np.random.randint(5, size=len(query)).astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# put data into pairs\n",
    "xi = []\n",
    "xj = []\n",
    "pij = []\n",
    "pair_id = []\n",
    "pair_query_id = []\n",
    "for qid in np.unique(qids):\n",
    "    query_idx = np.where(query == qid)[0]\n",
    "    for pair_idx in combinations(query_idx, 2):\n",
    "        pair_query_id.append(qid)\n",
    "        \n",
    "        pair_id.append(pair_idx)\n",
    "        i = pair_idx[0]\n",
    "        j = pair_idx[1]\n",
    "        xi.append(doc_features[i])\n",
    "        xj.append(doc_features[j])\n",
    "        \n",
    "        if doc_scores[i] == doc_scores[j]:\n",
    "            _pij = 0.5\n",
    "        elif doc_scores[i] > doc_scores[j]:\n",
    "            _pij = 1\n",
    "        else: \n",
    "            _pij = 0\n",
    "        pij.append(_pij)\n",
    "        \n",
    "xi = np.array(xi)\n",
    "xj = np.array(xj)\n",
    "pij = np.array(pij)\n",
    "pair_query_id = np.array(pair_query_id)\n",
    "\n",
    "xi_train, xi_test, xj_train, xj_test, pij_train, pij_test, pair_id_train, pair_id_test = train_test_split(\n",
    "    xi, xj, pij, pair_id, test_size=0.2, stratify=pair_query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 260 samples, validate on 65 samples\n",
      "Epoch 1/50\n",
      "260/260 [==============================] - 30s 117ms/sample - loss: 0.6769 - val_loss: 0.6502\n",
      "Epoch 2/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6725 - val_loss: 0.6478\n",
      "Epoch 3/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6716 - val_loss: 0.6452\n",
      "Epoch 4/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6706 - val_loss: 0.6406\n",
      "Epoch 5/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6651 - val_loss: 0.6372\n",
      "Epoch 6/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6633 - val_loss: 0.6311\n",
      "Epoch 7/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6558 - val_loss: 0.6195\n",
      "Epoch 8/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6541 - val_loss: 0.6093\n",
      "Epoch 9/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6449 - val_loss: 0.6013\n",
      "Epoch 10/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6397 - val_loss: 0.6058\n",
      "Epoch 11/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6444 - val_loss: 0.5852\n",
      "Epoch 12/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6318 - val_loss: 0.5793\n",
      "Epoch 13/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6319 - val_loss: 0.5707\n",
      "Epoch 14/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6243 - val_loss: 0.5756\n",
      "Epoch 15/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6224 - val_loss: 0.5644\n",
      "Epoch 16/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6225 - val_loss: 0.5490\n",
      "Epoch 17/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6137 - val_loss: 0.5544\n",
      "Epoch 18/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6157 - val_loss: 0.5401\n",
      "Epoch 19/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6081 - val_loss: 0.5342\n",
      "Epoch 20/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6048 - val_loss: 0.5432\n",
      "Epoch 21/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6121 - val_loss: 0.5466\n",
      "Epoch 22/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6077 - val_loss: 0.5451\n",
      "Epoch 23/50\n",
      "260/260 [==============================] - ETA: 0s - loss: 0.612 - 1s 2ms/sample - loss: 0.6135 - val_loss: 0.5269\n",
      "Epoch 24/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6111 - val_loss: 0.5287\n",
      "Epoch 25/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6101 - val_loss: 0.5433\n",
      "Epoch 26/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6107 - val_loss: 0.5365\n",
      "Epoch 27/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6123 - val_loss: 0.5291\n",
      "Epoch 28/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6080 - val_loss: 0.5297\n",
      "Epoch 29/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.6046 - val_loss: 0.5163\n",
      "Epoch 30/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5978 - val_loss: 0.5083\n",
      "Epoch 31/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5922 - val_loss: 0.5101\n",
      "Epoch 32/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5909 - val_loss: 0.5048\n",
      "Epoch 33/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5879 - val_loss: 0.5192\n",
      "Epoch 34/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5888 - val_loss: 0.5157\n",
      "Epoch 35/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5881 - val_loss: 0.4983\n",
      "Epoch 36/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5829 - val_loss: 0.5007\n",
      "Epoch 37/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5797 - val_loss: 0.5125\n",
      "Epoch 38/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5858 - val_loss: 0.5023\n",
      "Epoch 39/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5759 - val_loss: 0.4885\n",
      "Epoch 40/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5737 - val_loss: 0.4910\n",
      "Epoch 41/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5746 - val_loss: 0.4963\n",
      "Epoch 42/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5741 - val_loss: 0.4872\n",
      "Epoch 43/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5660 - val_loss: 0.5182\n",
      "Epoch 44/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5789 - val_loss: 0.4933\n",
      "Epoch 45/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5750 - val_loss: 0.4815\n",
      "Epoch 46/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5698 - val_loss: 0.4789\n",
      "Epoch 47/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5694 - val_loss: 0.4983\n",
      "Epoch 48/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5635 - val_loss: 0.4812\n",
      "Epoch 49/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5671 - val_loss: 0.4965\n",
      "Epoch 50/50\n",
      "260/260 [==============================] - 1s 2ms/sample - loss: 0.5645 - val_loss: 0.4834\n"
     ]
    }
   ],
   "source": [
    "# train model using compile and fit\n",
    "ranknet = RankNet()\n",
    "ranknet.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = ranknet.fit([xi_train, xj_train], pij_train, epochs=50, batch_size=1, validation_data=([xi_test, xj_test], pij_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
